<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Morven&#39;s Life</title>
    <link>https://morven.life/</link>
    <description>Recent content on Morven&#39;s Life</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Jan 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://morven.life/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Linux网络数据包的接收与发送过程</title>
      <link>https://morven.life/notes/networking-1-linux-virtual-device/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/networking-1-linux-virtual-device/</guid>
      <description>早在农历新年之前，就构思着将近半年重拾的网络基础整理成成一个系列，正好赶上武汉疫情在春节假期爆发，闲来无事，于是开始这一系列的笔记。
我的整体思路是：
 第一篇笔记会简单介绍Linux网络数据包接收和发送过程，但不涉及TCP/IP协议栈的细节知识，如果有需要了解这些基础知识的读者，我推荐去阅读阮一峰老师的互联网协议的系列文章。 接下来，我会总结常用的Linux虚拟设备，同时会结合Linux自带的新网络工具包iproute2来操作这些常用的网络设备。 有了前面的基础知识，我们再来了解几种常用的容器网络实现远离。 最后，我们深入探讨一下K8s平台中主流的网络实现。  其实，严格上来说，这种学习思路其实很“急功近利”，但是，对于不太了解网络基础知识和Linux内核原理的人来说，这反而是一种很有效的学习方式。但是，私以为学习过程不只应该由浅入深，更应该螺旋向前迭代，温故而知新，才能获益良多。
数据包的接收过程 废话不多说，我们先开始第一篇笔记的内容。
为了简化起见，我们以一个UDP数据包在物理网卡上处理流程来介绍Linux网络数据包的接收和发送过程，我会尽量忽略一些不相关的细节。
从网卡到内存 我们知道，每个网络设备（网卡）需要有驱动才能工作，驱动需要在内核启动时加载到内核中才能工作。事实上，从逻辑上看，驱动是负责衔接网络设备和内核网络栈的中间模块，每当网络设备接收到新的数据包时，就会触发中断，而对应的中断处理程序正是加载到内核中的驱动程序。
下面这张图详细的展示了数据包如何从网络设备进入内存，并被处于内核中的驱动程序和网络栈处理的：
 数据包进入物理网卡。如果目的地址不是该网络设备，且该来网络设备没有开启混杂模式，该包会被网络设备丢弃。 物理网卡将数据包通过DMA的方式写入到指定的内存地址，该地址由网卡驱动分配并初始化。 物理网卡通过硬件中断（IRQ）通知CPU，有新的数据包到达物理网卡需要处理。 CPU根据中断表，调用已经注册的中断函数，这个中断函数会调到驱动程序（NIC Driver）中相应的函数 驱动先禁用网卡的中断，表示驱动程序已经知道内存中有数据了，告诉物理网卡下次再收到数据包直接写内存就可以了，不要再通知CPU了，这样可以提高效率，避免CPU不停的被中断。 启动软中断继续处理数据包。这样的原因是硬中断处理程序执行的过程中不能被中断，所以如果它执行时间过长，会导致CPU没法响应其它硬件的中断，于是内核引入软中断，这样可以将硬中断处理函数中耗时的部分移到软中断处理函数里面来慢慢处理。  内核处理数据包 上一步中网络设备驱动程序会通过软触发内核网络模块中的软中断处理函数，内核处理数据包的流程如下图所示：
 对于第6步中驱动发出的软中断，内核中的ksoftirqd进程会调用网络模块的相应软中断所对应的处理函数，这里其实就是调用net_rx_action函数。 接下来net_rx_action调用网卡驱动里的poll函数来一个个地处理数据包。 而poll函数会让驱动会读取网卡写到内存中的数据包。事实上，内存中数据包的格式只有驱动知道。 驱动程序将内存中的数据包转换成内核网络模块能识别的skb(socket buffer)格式，然后调用napi_gro_receive函数 napi_gro_receive会处理GRO相关的内容，也就是将可以合并的数据包进行合并，这样就只需要调用一次协议栈。然后判断是否开启了RPS，如果开启了，将会调用enqueue_to_backlog。 enqueue_to_backlog函数会将数据包放入input_pkt_queue结构体中，然后返回。 &amp;gt; Note: 如果input_pkt_queue满了的话，该数据包将会被丢弃，这个queue的大小可以通过net.core.netdev_max_backlog来配置 接下来CPU会在软中断上下文中处理自己input_pkt_queue里的网络数据（调用__netif_receive_skb_core函数） 如果没开启RPS，napi_gro_receive会直接调用__netif_receive_skb_core函数。 紧接着CPU会根据是不是有AF_PACKET类型的socket（原始套接字），如果有的话，拷贝一份数据给它(tcpdump抓包就是抓的这里的包)。 将数据包交给内核协议栈处理。 当内存中的所有数据包被处理完成后（poll函数执行完成），重新启用网卡的硬中断，这样下次网卡再收到数据的时候就会通知CPU。  内核协议栈 内核网络协议栈此时接收到的数据包其实是三层(IP网络层)数据包，因此，数据包首先会进入到IP网络层层，然后进入传输层处理。
IP网络层  ip_rcv: ip_rcv函数是IP网络层处理模块的入口函数，该函数首先判断属否需要丢弃该数据包（目的mac地址不是当前网卡，并且网卡设置了混杂模式），如果需要进一步处理就然后调用注册在netfilter中的NF_INET_PRE_ROUTING这条链上的处理函数。 NF_INET_PRE_ROUTING: netfilter放在协议栈中的钩子函数，可以通过iptables来注入一些数据包处理函数，用来修改或者丢弃数据包，如果数据包没被丢弃，将继续往下走。 &amp;gt; NF_INET_PRE_ROUTING等netfilter链上的处理逻辑可以通iptables来设置，详情请移步: https://morven.life/notes/the_knowledge_of_iptables/ routing: 进行路由处理，如果是目的IP不是本地IP，且没有开启ip forward功能，那么数据包将被丢弃，如果开启了ip forward功能，那将进入ip_forward函数。 ip_forward: 该函数会先调用netfilter注册的NF_INET_FORWARD链上的相关函数，如果数据包没有被丢弃，那么将继续往后调用dst_output_sk函数。 dst_output_sk: 该函数会调用IP网络层的相应函数将该数据包发送出去，这一步将会在下一章节发送数据包中详细介绍。 ip_local_deliver: 如果上面路由处理发现发现目的IP是本地IP，那么将会调用ip_local_deliver函数，该函数先调用NF_INET_LOCAL_IN链上的相关函数，如果通过，数据包将会向下发送到UDP层。  传输层  udp_rcv: 该函数是UDP处理层模块的入口函数，它首先调用__udp4_lib_lookup_skb函数，根据目的IP和端口找对应的socket，如果没有找到相应的socket，那么该数据包将会被丢弃，否则继续。 sock_queue_rcv_skb: 该函数一是负责检查这个socket的receive buffer是不是满了，如果满了的话就丢弃该数据包；二是调用sk_filter看这个包是否是满足条件的包，如果当前socket上设置了filter，且该包不满足条件的话，这个数据包也将被丢弃。 __skb_queue_tail: 该函数将数据包放入socket接收队列的末尾。 sk_data_ready: 通知socket数据包已经准备好。 调用完sk_data_ready之后，一个数据包处理完成，等待应用层程序来读取。   Note: 上面所述的所有执行过程都在软中断的上下文中执行。</description>
    </item>
    
    <item>
      <title>我的「2019」</title>
      <link>https://morven.life/posts/the_year_of_2019/</link>
      <pubDate>Sat, 21 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/the_year_of_2019/</guid>
      <description>TL;DR 最近我发现自己对于思考和记录的欲望在慢慢消亡，这是件很令我沮丧的事情。“少愤怒而多思考”的人设正在崩塌，我在向着自己厌恶的方向，附和着，愤怒着，可一旦冷却翻篇之后，又恢复到若无其事的状态。归根结底，我没有以前那么坚定了，对「自我」的认同感逐渐淡化，开始被生活的惯性驱动，不去思考为什么，因为我发现随波逐流原来这么轻松。但始终，我还是没法和自己的内心和解，因为一旦我进入放纵之后的“贤者模式”，我开始恐慌，怀疑。与其被这种循环往复的矛盾折磨，不如投入到工作当中，至少不会有失落感。所以，这一年我基本放弃“抵抗”，避免思考为什么，开始享受这一年加速的节奏。
是告别也是开始 这一年注定是个说再见的一年，作为一个影迷，这个时代的我们也是幸运的，因为我们有着共同的记忆，漫威宇宙；有时候你会感慨十年真的很短，弹指一挥间，反过来想想，终局之战是集结，是重聚，但也是新时代的开始。这一年我们愤怒，声讨HBO，我们会怀念狼家的孩子流离失所的模样，我们期待龙妈身披铠甲力挽狂澜的终章，“问君能有几多愁，恰似六季过后无权游”。但是我们也会惊叹HBO用一种阴冷，克制却令人毛骨悚然的方式将切尔诺贝利的悲鸣呈现出来，它时刻提醒着对于自然与规律，我们始终应该保持敬畏之心。这个时代的我们注定是怀旧的，我们怀念Queen，怀念佛雷迪，遗憾我们没有亲身经历那段充满爱、痛苦和接纳的音乐之旅；我们想以一部《爱尔兰人》重温当年风华正茂的阿尔帕西诺、德尼罗与乔·佩西共同演绎的黑帮传奇；我们想象着上世纪六七十年代风起云涌的好莱坞，光怪陆离的电影中的电影往事；我们在菲尼克斯肆意癫狂的小丑表演中看DC如何在后黑暗骑士时代重新扳回一局；
当然，这一年华语电影也慢慢摆脱流量时代，我们看到了哪吒的横空出世，《流量地球》的荡气回肠，《少年的你》的真实无助；好的影视剧也层出不穷，《我们与恶的距离》让我们学会重新审视现实，《长安十二时辰》让我们领略盛世长安美景的同时也赞叹古人的理想抱负；甚至一些精彩的华语综艺也开始摆脱刻板印象，突出重围，《乐队的夏天》让我们享受音乐的同时开始尊重摇滚；《圆桌派》继续教我们如何做一个人畜无害的空巢老人；《奇葩说》继续满足着我们围观不怕事儿大的幻想。
 每一颗苹果都值得被偷吃。
 这一年还有很多没有提及的优秀作品，它们不只是我们茶余饭后的谈资，甚至是我们迷茫低落时的一颗启明星。很期待在新的一年我们有幸见证更多优秀影视剧的诞生。
一些小小的成就 回顾过去一年，工作上取得了一些小的进步，除了不断完善基础网络层技术栈，也更加深入地钻研了Istio以及Envoy。之前不断横向地扩展自己的知识领域，却没有用心向下深入探索，导致面对很多技术话题都可以侃侃而谈，可是遇到实际问题的时候无处下手。于是干脆从现实案例出发，在多耦合的复杂环境中练级打怪。
上半年处于持续的三线程并发工作状态，一方面完成日常的ICP Roadmap开发任务，另一方面帮助在客户在生产环境实施Istio，同时，还要在Istio社区推动开源产品的不断演进。
由于工作资源变动，年中有一段时间在Kubernetes-Sig社区参与开发Cluster-api，参与周期并不是很长，很多问题并没有深入研究，但也有所成就。
九月初开始深入调研Kubernetes CICD利器Prow，并将部分项目从Travis迁移到Prow，Prow的好处在于提供了各种插件完成Travis, CirleCI等CICD工具没有的命令式自动化工具，对于Kubernetes原生项目尤其友好。
这一年下来收获也不少，一方面帮助客户完成了将Istio投入生产，另一方面，从Istio1.1到1.4，保证了每三个月的大版本更新顺利更新，最重要的是，使得Istio1.0发布之初的性能问题得到显著改善，同时，Istio官方提供的Operator也随1.4的发布正式投入使用。这也是我个人投入时间比较多的项目之一。另外，也和同事一起完成了Cluster API for IBM Cloud Provider第一个版本的开源与发布。在十月份的时候，开始将Prow投入到生产环境，将公司部分开源项目从Travis切换到Prow。另外，大约在三月份的时候，收到了机械工业出版社的写本关于Istio实践书的邀请，当时我并没有答应，原因一方面是我觉得个人的经验无法承担这样一个角色，另一方面我对于完成这样一本书的意义持保留态度。
新的期待 不知道我是否还理智和清醒，我很享受这一年的加速节奏，甚至工作的惯性让我持续地处于颅内高潮的状态，以至于偶尔“悠闲时光”里我懒得去思考这些到底有没有意义。
我依稀记得年初我斗志昂扬的给自己定了flag:
 Make things happen.
 我不知道自己的计划是不是已经完成，但是我能确定的是在接下来的一年里面，怎么也得给战斗升个级，更加激进的计划才能逼迫憋出大招：
 取法乎上，得乎其中；取法乎中，得乎其下；取法呼下，无所得矣。
 总之，一句话，来年再战！</description>
    </item>
    
    <item>
      <title>六字回忆录</title>
      <link>https://morven.life/posts/memory_with_six_word/</link>
      <pubDate>Sat, 26 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/memory_with_six_word/</guid>
      <description>前几天晚上我发现了一个很有意思叫做sixwordmemoirs的网站，上面收集了很多六个英文单词组成的回忆录，特别类似于国内前几年风行一时的“三行情书”大赛。
我很好奇这样一个温暖而有趣的想法是怎么被提出来的。索性从床上爬起来，经过几番搜索，终于知道原来是早在2006年的时候美国的网络杂志SMITH Magazine发起了一个活动，要求读者用6个词来描述自己过去5年的生活。结果得到了出乎意料的热烈反响，美国各地的民众纷纷寄去了自己的&amp;rdquo;六字回忆录&amp;rdquo;，甚至在2009年的时候，这些回忆录被汇编一本书：《Six-Word Memoirs on Love and Heartbreak》。此书出版后，成了《纽约时报》畅销书，评价也很正面。费城一家报纸打趣地说：&amp;rdquo;Buy it, keep it in bathroom&amp;rdquo;（去买一本，放在马桶边）。
再后来，SMITH Magazin基于这个想法开发了sixwordmemoirs的网站，所有人都可以随时都可以投稿，还可以详细的描述回忆录背后的故事，展现在全世界读者的面前。
我搬运了一些比较有意思的几条：
 Cursed with cancer. Blessed by friends. - 一个9岁的白血病患者
I still make coffee for two. - 一位失去丈夫的女士
You eleven? You brave, ya&amp;rsquo; know. - 一位爷爷鼓励自己的孙女
 其实，更多的关于爱情故事：
 Eight letters; so hard to say.
It&amp;rsquo;s her smile. The real one.
You left footprints across my heart.
I blush. He smirks. We hug.
 其实在很久之前，我就在构思创建一个供所有人记录故事，分享心得的平台，但我内心深处的惰性决定了我还是作为产品的消费者，对于设计与运维抱着惯有的敬畏心态，所以，很多想法终究只是想法。这或许是很好的契机，开发一个留下故事，收获灵感的平台。</description>
    </item>
    
    <item>
      <title>Istio的前世今生</title>
      <link>https://morven.life/posts/the_history_of_istio/</link>
      <pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/the_history_of_istio/</guid>
      <description>其实要彻底了解Istio以及服务网格出现的背景，就得从计算机发展的早期说起。
下面这张图展示的的通信模型变种其实从计算机刚出现不久的上世纪50年代开始就得到广泛应用，那个时候，计算机很稀有，也很昂贵，人们手动管理计算机之间的连接，图中绿色的网络栈底层只负责传输电子信号和字节码：
随着计算机变得越来越普及，价格也没那么贵了，计算机之间的连接数量和通信的数据量出现了疯狂式的增长，人们越来越依赖网络系统，工程师们必须确保他们开发的服务能够满足用户的要求。于是，如何提升系统质量成为人们关注的焦点。机器需要知道如何找到其他节点，处理同一个通道上的并发连接，与非直接连接的机器发生通信，通过网络路由数据包，加密流量……除此之外，还需要流量控制机制，流量控制可以防止下游服务器给上游服务器发送过多的数据包。
于是，在一段时期内，开发人员需要在自己的代码里处理上述问题。在下面这张图的示例中，为了确保下游服务器不给其他上游服务造成过载，应用程序需要处理流量控制逻辑，于是网络中的流量控制逻辑和业务逻辑就混杂在一起：
幸运的是，上世纪60年代末，TCP/IP协议栈的出现解决了可靠传输和流量控制等问题，此后尽管网络逻辑代码依然存在，但已经从应用程序里抽离出来，成为操作系统网络栈的一部分，工程师只需要按照操作系统的调用接口进行编程就可以解决基础的网络传输问题：
进入21世纪，计算机越来越普及，也越来越便宜，相互连接的计算机节点越来越多，业界出现了各种网络系统，如分布式代理和面向服务架构(SOA)：
分布式为我们带来了更高层次的能力和好处，但却带来了新的挑战。这时候工程师的重心开始转移到应用程序的网络功能上面，这时候的服务之间的对话以“消息”为传输单元，当工程师们通过网络进行调用服务时，必须能为应用程序消息执行超时、重试、确认等操作。
于是，有工程师是开始尝试使用消息主干网（messaging backbone）集中式地来提供、控制应用程序网络功能，如服务发现、负载均衡、重试等等，甚至，比如协议调解、消息转换、消息路由、编排等功能，因为他们觉得如果可以将这些看似同一层面的内容加入到基础设施中，应用程序或许会更轻量、更精简、更敏捷等等。这些需求绝对是真实的，ESB(Enterprise Service Bus)演变并满足了这些需要。ESB在是2005年被提出的，它的概念特别类似于计算机硬件概念里的USB, USB作为电脑中的标准扩展接口，可以连接各种外部设备；而ESB则就把路由管理、协议转换、策略控制等通用应用程序网络功能加到现有的集中式消息总线里：
这看似行得通！
可是，在实施SOA架构的时候，工程师们发现这种架构有点儿用力过度，矫枉过正了。集中式的消息总线往往会成为架构的瓶颈，用它来进行流量控制、路由、策略执行等并不像我们想象那么容易。加上组织结构过于复杂，使用专有格式（XML等），需要业务逻辑需要实现路由转换和编排等功能，各个服务之间耦合度很高，在敏捷运动的时代背景下，ESB架构已经无法跟上时代的节奏了。
在接下来的几年内，REST革命和API优先的思潮孕育了微服务架构应用，而以docker为代表的容器技术和以Kubernetes为代表的容器编排技术的出现促进了微服务架构的落地。事实上，微服务时代可以以Kubernetes的出现节点划分为“前微服务时代”和“后微服务时代”：
“前微服务时代”基本上是微服务作为用例推动容器技术的发展，而到“后微服务时代”，特别是成为标准的Kubernetes其实在驱动和重新定义微服务的最佳实践，容器和Kubernetes为微服务架构的落地提供了绝佳的客观条件。微服务架构有很多好处，比如：
 快速分配计算资源 快速部署升级迭代 易于分配的存储 易于访问的边界等等  但是作为较复杂的分布式系统，微服务架构给运维带来了新的挑战。当工程师开始接尝试微服务架构，必须考虑如何进行微服务治理。狭义的微服务治理，关注的是微服务组件之间的连接与通讯，例如服务注册发现、东西向路由流控、负载均衡、熔断降级、遥测追踪等。
历史总是惊人的相似，第一批采用微服务架构的企业遵循的是与第一代网络计算机系统类似的策略，也就是说，解决网络通信问题的任务又落在了业务工程师的肩上。
这个时候出现了看到诸如Netflix OSS堆栈、Twitter Finagle以及赫赫有名的Spring Cloud这样的框架和类库帮助业务工程师快速开发应用程序级别的网路功能，只需要写少量代码，就可以把服务发现，负载均衡，路由管理，遥测收集，监控告警等这些功能实现：
但是如果仔细想一下的话，就会发现这样编写微服务程序的问题很明显。
这些类库或者框架是特定语言编写的，并且混合在业务逻辑中（或在整个基础设施上层分散的业务逻辑中）。姑且不说类库和框架的学习成本和门槛，我们知道微服务架构问世的一个承诺就是不同的微服务可以采用不同的编程语言来编写，可是当你开始编写代码的时候会发现有些语言还没有提供对应的类库。这是一个尴尬的局面！这个问题非常尖锐，为了解决这个问题，大公司通常选择就是统一编程语言来编写微服务代码另外的问题是，升级怎么办？框架不可能一开始就完美无缺，所有功能都齐备，没有任何BUG。升级一般都是逐个版本递进升级，一旦出现客户端和服务器端版本不一致，就要小心维护兼容性。实际上，每做出一次变更都需要进行集成、测试，还要重新部署所有的服务——尽管服务本身并没有发生变化。
与网络协议栈一样，工程师们急切地希望能够将分布式服务所需要的一些特性放到底层的平台中。这就像工程师基于HTTP协议开发非常复杂的应用，无需关心底层TCP如何控制数据包。在开发微服务时也是类似的，业务工程师们聚焦在业务逻辑上，不需要浪费时间去编写服务基础设施代码或管理系统用到的软件库和框架。把这种想法囊括到之前架构中，就是下边这幅图所示的样子：
不过，在网络协议栈中加入这样的一个层是不实际的。貌似可以尝试一下代理的方案！事实上，确实有有一些先驱者，尝试过使用代理的方案，例如nginx，haproxy，proxygen等代理。也就是说，一个服务不会直接与上游服务发生连接，所有的流量都会流经代理，代理会拦截服务之间的请求并转发到上游服务。可是，那时候代理的功能非常简陋，很多工程师尝试之后觉得没有办法实现服务的客户端所有的需求。
在这样的诉求下，第一代的Sidecar出现了，Sidecar扮演的角色和代理很像，但是功能就齐全很多，基本上原来微服务框架在客户端实现的功能都会有对应的实现：
但是第一代的sidecar有一个重要的限制，它们是专门为特定基础设施组件而设计的，导致无法通用。例如，Airbnb的Nerve和Synapse，它们工作的基础是服务一定是注册到ZooKeeper上的，而Netflix的Prana要求一定要使用Netflix自己的Eureka注册服务&amp;hellip;
随着微服务架构日渐流行，新一波的sidecar出现了，可以用在不同基础设施组件上，我们把他们叫做通用型的sidecar。其中Linkerd是业界第一个通用型sidecar，它是基于Twitter微服务平台而开发的，实际上也正是它创造了Service Mesh，即服务网格的概念。2016年1月15日，Linkerd 0.0.7版本发布，随后加入CNCF，1.0版本于2017年4月份发布；随后的通用型sidecar就是大名鼎鼎来自于Lyft的envoy，Lyft在2016年9月发布envoy的1.0版本之后。2017年9月envoy加入CNCF；最后一个比较新的sidecar来自于我们熟悉的NGINX，叫做Nginmesh，2017年9月发布了第一个版本。
有了通用型sidecar，每个微服务都会有一个sidecar代理与之配对，服务间通信都是通过sidecar代理进行的。正如我们在下这幅图上看到的那样，sidecar代理之间的连接形成了一种网格网络：
这就是服务网格概念的来源，下面是服务网格概念地官方定义，它不再把sidecar代理看成单独的组件，并强调了这些sidecar代理所形成的网络的重要性。
 A service mesh is a dedicated infrastructure layer for making service-to-service communication safe, fast, and reliable.</description>
    </item>
    
    <item>
      <title>一份好吃的Istio入门餐</title>
      <link>https://morven.life/posts/getting_started_istio/</link>
      <pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/getting_started_istio/</guid>
      <description>前两天Istio正式发布1.2版本，至此，Istio社区再次回归每3个月一个大版本+每月一个小版本更新的传统。从Istio1.0发布以来，社区在对改进Istio的一些非功能性的需求（例如性能与扩展性）方面的努力是大家有目共睹的。我觉得是时候写一篇通俗易懂的Istio入门文章，让更多的人去体验一下Istio在云时代所带来的各种益处。
为什么需要Istio 说到Istio，就不得不提到另外一个名词：Service Mesh，中文名为服务网格。
相信很多人对于传统的单体应用以及它所面临的问题很熟悉，一种显而易见的方案是将其分解为多个微服务。这确实能简化微服务的开发，但是不得不说，也带来非常多的挑战，比如对于成百上千的微服务的通信、监控以及安全性的管理并不是一件简单的事。目前为止，针对这些问题确实有一些解决方案（比如Spring Cloud），但基本都是通过类似于类库或者定制化脚本的方式将为服务串联在一起，附带很高的维护升级成本。
Service Mesh的出现正是为了解决这一问题。它是位于底层网络与上层服务之间的一层基础设施，提供了一种透明的、与编程语言无关的方式，使网络配置、安全配置以及遥测等操作能够灵活而简便地实现自动化。从本质上说，它解耦了服务的开发与运维工作。如果你是一名开发者，那么部署升级服务的时候不需要担心这些操作会对整个分布式系统带来哪些运维层面的影响；与之对应，如果你是运维人员，那么也可以放心大胆的服务之间的运维结构进行变更，而不需要修改服务的源代码。
而Istio真正地将Service Mesh的概念发扬光大。它创新性地将控制平面（Control Plane）与数据平面（Data Plane）解耦，通过独立的控制平面可以对Mesh获得全局性的可观测性（Observability）和可控制性（Controllability），从而让Service Mesh有机会上升到平台的高度，这对于对于希望提供面向微服务架构基础设施的厂商，或是企业内部需要赋能业务团队的平台部门都具有相当大的吸引力。
为什么会需要这样的设计呢？
我们先来看一个单独的一个微服务，正常情况下不管单个微服务部署在一个物理机上，亦或一个容器里，甚至一个k8s的pod里面，它的基本数据流向不外乎所有的Inbound流量作为实际业务逻辑的输入，经过微服务处理的输出作为Outbound流量：
随着应用的衍进与扩展，微服务的数量快速增长，整个分布式系统变得“失控”，没有一种通用可行的方案来监控、跟踪这些独立、自治、松耦合的微服务组件的通信；对于服务发现和负载均衡，虽说像k8s这样的平台提供了通过kube-proxy下发iptables规则来实现的基本的服务发现和转发能力，不能满足高并发应用下的高级的服务特性；至于更加复杂的熔断、限流、降级等需求，看起来不通过应用侵入式编程几乎不可能实现。
但是，真的是这样吗？我们来换一个思路，想象一下，如果我们在部署微服务的同时部署一个sidecar，这个sidecar充当代理的功能，它会拦截所有流向微服务业务逻辑的流量，做一些预处理（解包然后抽取、添加或者修改特定字段再封包）之后在将流量转发给微服务，微服务处理的输出也要经过sidecar拦截做一些后处理（解包然后抽取、删除或者修改特定字段再封包），最后将流量分发给外部：
随着微服务数量的增长，整个分布式应用看起来类似于这样，可以看到所有的sidecar接管了微服务之间通信的流量：
这样的架构也有问题，数量少的情况下，sidecar的配置简单，我们可以简单应对，但是sidecar的数量会随着微服务的数量不断增长，sidecar需要又一个能集中管理控制模块根据整个分布式系统的架构变动动态下发sidecar配置，这里我们把随着分布式应用一起部署的sidecar成为数据平面（Data Plane），而集中是的管理模块成为控制平面（Control Plane）。现在整个架构看起来会像是这样：
Istio架构设计 让我们再次简化一下上面的架构设计，这次我们将关注点放在数据平面里两个部署sidecar的微服务中，它们之间通信的所有流量都要被sidecar拦截，经过预处理之后转发给对应的应用逻辑：
这基本就是Istio的设计原型，只不过Istio默认使用Envoy作为sidecar代理（Istio利用了Envoy内建的大量特性，例如服务发现与负载均衡、流量拆分、故障注入、熔断器以及分阶段发布等功能），而在控制层面Istio也分为四个主要模块：
 Pilot: 为Envoy sidecar提供服务发现功能，为智能路由（例如A/B测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能；它将控制流量行为的高级路由规则转换为特定于Envoy的配置，并在运行时将它们传播到sidecar；
 Mixer: 负责在服务网格上执行访问控制和使用策略，并从Envoy代理和其他服务收集遥测数据；代理提取请求级属性，发送到Mixer进行评估；
 Citadel: 通过内置身份和凭证管理赋能强大的服务间和最终用户身份验证；可用于升级服务网格中未加密的流量，并为运维人员提供基于服务标识而不是网络控制的强制执行策略的能力；
 Galley: 用来验证用户编写的Istio API配置；未来的版本中Galley将接管Istio获取配置、 处理和分配组件的顶级责任，从而将其他的Istio组件与从底层平台（例如Kubernetes）获取用户配置的细节中隔离开来；
  最终，Istio的架构设计如下图所示：
Istio快速上手 部署Istio控制平面
 Note: Istio可以用来运行在多种不同的环境里面，包括Kubernetes、Docker、Consul甚至BareMental（物理机上），文本以Kubernetes环境为例来演示Istio的多种特性。
 在部署使用Istio之前，我们需要准备一个Kubernetes的环境，如果只是试用Istio的话，推荐试用Minikube或者Kind来快速搭建Kubernetes环境。
有了Kubernetes环境并且配置好kubectl之后，我们就可以正式开始了。
 下载并解压最新的Istio发布版本（截至发布本文时的最新Istio版为为1.2.0）
 root@mcdev1:~# curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.2.0 sh - % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 2140 100 2140 0 0 1787 0 0:00:01 0:00:01 --:--:-- 1787 Downloading istio-1.</description>
    </item>
    
    <item>
      <title>MacBookPro开启HiDPI</title>
      <link>https://morven.life/posts/enable_hidpi_for_external_monitor/</link>
      <pubDate>Fri, 12 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/enable_hidpi_for_external_monitor/</guid>
      <description>上周旧笔记本（2015-Mid MacBookPro）由于自己的疏忽导致背包内水杯漏水而浸液，基本无法使用。因为是主力机，所以无奈只能硬着头皮换了最新款的MacBookPro（2018-Mid）。不换不知道，原本以为很快的数据和配置迁移消耗了我一整晚的时间。可能是因为近些年来苹果品控的下降，对于最新MacBookPro没多少好感，拿到新本后令人发狂的蝶式键盘加上鸡肋的TouchBar，让新旧本之间的过渡期再次延长，于是决定还是继续使用外接键盘和鼠标，并将自己之前的显示器作为主显示器。
将外接显示器连接上之后，很快就会发现整体显示模糊，即使作为4K的显示屏也不能达到期望的Retina效果。其实原因也很简单，没有开启HiDPI。
HiDPI 何为HiDPI
我们知道，高分辨率意味着更小的字体和图标，而HiDPI可以用软件的方式实现单位面积内的高密度像素。通过开启HiDPI渲染，可以在保证分辨率不变的情况下，使得字体和图标变大。所以，总结一下就是：
高PPI(硬件) + HiDPI渲染(软件) = 更细腻的显示效果(Retina)  如何开启HiDPI
关于如何开启HiDPI，Google搜索之后会有很多方案，但是因为系统的不断升级，有的不够全面，有的过于繁琐。在此针对我目前的笔记本（MacBookPro 2018-Mid）给出一个相对简洁的方案。主要包含三个步骤：
 关闭SIP 终端命令 开启SIP  接下来一一详细介绍。
备份 实际的操作的过程会更改部分系统文件，因此在操作前要确保对文件进行备份：
 打开终端并进入到/System/Library/Displays/Contents/Resources 拷贝Overrides文件夹到其他目录一份  关闭SIP  Note: 关闭SIP有风险，确保所有操作完整之后再次打开SIP，否则对系统文件的保护将不存在
  打开终端并输入csrutil status，如果结果为enabled则表明SIP为开启状态 关机之后再按电源键后长按command + R直至出现苹果LOGO 在Utils-&amp;gt;Terminal打开终端，并输入csrutil disable，之后关掉终端 重启并正常开机 打开终端并输入csrutil status，确保结果是disable  运行设置脚本  Note: 为了简化配置过程，我们使用自动脚本来开启HiDPI，如果想手动来配置，请访问：https://comsysto.github.io/Display-Override-PropertyList-File-Parser-and-Generator-with-HiDPI-Support-For-Scaled-Resolutions/
  输入curl -o ~/enable-HiDPI.sh https://raw.githubusercontent.com/syscl/Enable-HiDPI-OSX/master/enable-HiDPI.sh 输入chmod +x ~/enable-HiDPI.sh 输入~/enable-HiDPI.sh 输入你想设置的分辨率比如1920x1080   Note: 设置分辨率的时候请务必使用x(乘号)，不能使用*(星号)
 开启SIP  关机之后再次按电源键开机，并长按Command + R直至出现苹果LOGO 在Utils-&amp;gt;Terminal打开终端，并输入csrutil enable之后关掉终端 重启并正常开机 打开终端并输入csrutil status，确保结果是enable  使用RDM切换分辨率 RDM是一个用来切换分辨率的开源软件，使用方法也很简单，其中带⚡️标志的才是开启了HiDPI的分辨率。</description>
    </item>
    
    <item>
      <title>创建最小Docker镜像</title>
      <link>https://morven.life/posts/build_the_smallest_possible_docker_image/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/build_the_smallest_possible_docker_image/</guid>
      <description>如果你熟悉docker，你可能知道docker镜像存储使用Union FS分层存储技术。在构建一个docker镜像时，会一层一层构建，前一层是后一层的基础，每一层构建完成之后就不会再改变。正是因为这一点，我们在构建docker镜像的时候，要特别小心，每一层尽量只包含需要的东西，构建应用额外的东西尽量在构建结束的时候删除。举例来说，比如你在构建一个go写的简单web应用程序的时候，原则上只需要一个go编译出来的binary，没有必要保留构建的工具以及环境。
docker官方提供了一个特殊的空镜像scratch,使用这个镜像意味着我们不需要任何的已有镜像为基础，直接将我们自定义的指令作为镜像的第一层。
FROM scratch ...  实际上，我们可以创建自己的scratch镜像：
tar cv --files-from /dev/null | docker import - scratch  那么，问题来了，我们可以使用scratch镜像为基础制作哪些镜像呢？答案是所有不需要任何依赖库的可执行文件都可以放到以scratch为基础镜像来制作。具体来说，对于linux下静态编译的程序来说，并不需要操作系统提供的运行时支持，所有需要的一切都已经在可执行文件中包含了，比如使用go语言开发的很多应用会使用直接FROM scratch的方式制作镜像，这样最终的镜像体积非常小。
下面是一个简单的go语言开发的web程序代码：
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;net/http&amp;quot; ) func main() { http.HandleFunc(&amp;quot;/&amp;quot;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, &amp;quot;Hello, you&#39;ve requested: %s\n&amp;quot;, r.URL.Path) }) http.ListenAndServe(&amp;quot;:80&amp;quot;, nil) }  我们可以使用go build来编译此程序，并以scratch为基础制作docker镜像，dockerfile如下：
FROM scratch ADD helloworld / CMD [&amp;quot;/helloworld&amp;quot;]  接下来开始编译并构建docker镜像：
mc@mcmbp:~/gocode/src/hello# go build -o helloworld mc@mcmbp:~/gocode/src/hello# docker build -t helloworld .</description>
    </item>
    
    <item>
      <title>重拾少年时期的「信仰」</title>
      <link>https://morven.life/posts/the_summary_of_2018/</link>
      <pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/the_summary_of_2018/</guid>
      <description>不必太纠结于当下，也不必太忧虑未来，当你经历过一些事情的时候，眼前的风景已经和从前不一样了。
 窗外的霓虹灯让我有点儿走神，在下高铁回家的出租车上，我被出租车司机套路了年龄。本来以为他会说看起来这么年轻之类的客套话，但是他接下来说的话让我有点儿猝不及防！
 快三十岁的人了，该娶媳妇儿了！
 我一时不知道该怎么回复，陷入了沉思。我到底还年轻吗？这个问题也许在半年前我会毫不犹豫的回答，我当然年轻，我还没有闯出什么名堂，怎么可能变老呢？但是现在呢？我有点儿心虚！出租车在一路飘红的二环边上缓缓挪动，走走停停，恍恍惚惚中，我的大脑开始闪现那个我难以接受的事实：我真的老了。
一直以来，作为最小的，我一直拥有简单的信仰，就是去做一个优秀的人，让所有人夸赞；我有自己的“偶像”，追赶成为他的步伐甚至成了我少年时期的生活主旋律。虽然及其功利的信仰不值一提，真是肤浅至极，但是日子过得很纯粹，为因为小小的成就而高兴好几天，也会为偶然的失利懊恼而睡不着觉。总之就是时刻想证明自己，所有的这一切感觉都是理所当然，心中时刻提醒自己不要辜负每个关心自己的人，无暇思考为什么！
然而，成长就是一个摸石头过河的过程，尤其对于我这个出身普通家庭的人来说。一路磕磕绊绊，我开始质问为什么大人们都在做自己认为不对的事情而心照不宣，到底是该回归初心还是随波水流？俨然，少年时的单纯开始出现裂痕，天平开始向另外一端倾斜，甚至开始影响自己的日常生活，在人生的重要十字路口开始犹犹豫豫，不知所措！这个时候我多么希望有看穿一切的长者给我指导？然而，这场源自于内心的混乱，终究只能在内心寻找答案。有时候连续好几个月，我在想如果不离开自己这种舒适的环境，如果不能给自己规划未来的出路，我就无法成为自己欣赏的人。然而自己终究没有那种魄力，有时候这样的纠结让我几度抑郁，同时也时刻提醒自己，简简单单无心无肺不也挺好！
我其实很清楚这种执念会将自己带完何处？固执，腐败，糜烂！我要放下哪些愚蠢的想法，重新认识自己，不去思考为什么？踏踏实实做好每一件事情，享受小成就带给自己的快感。虽然我又一次以为我找到了事物的真相，但是在这个片刻，我似乎看大了少年时代的自己，我是心满意足的。
所以最终2018年的年底，我要结束了这种纠结的状态，把自己重新放到“野外”，用一种新的视角去看自己，看世界。去按照自己希望的方式，改造自己，做一次升级。
因此，在2019年，我要更高效的提升自己的能力，做更多具备稀缺性和自我认同的事情，重拾少年时期的「信仰」！</description>
    </item>
    
    <item>
      <title>Docker知识点拾遗</title>
      <link>https://morven.life/notes/the_knowledge_of_docker/</link>
      <pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_knowledge_of_docker/</guid>
      <description>Docker是一个划时代的产品，它彻底地释放了计算机虚拟化的威力，极大的提高了应用的部署、测试和分发。虽然我们几乎每天都使用docker，但还是有一些特别容易被忽略却很重要的docker知识，今天，我们就集中起来聊一聊。
Docker与传统虚拟机的区别 经常有人说“docker是一种性能非常好的虚拟机”，这种说法是错误的。Docker相比于传统虚拟机的技术来说更为轻便，具体表现在docker不是在宿主机上虚拟出一套硬件后再运行一个完整的操作系统，然后再在其上运行所需的应用进程，而是docker容器里面的进程直接运行在宿主的内核（Docker会做文件系统、网络互联到进程隔离等等），容器内没有自己的内核，而且也没有进行硬件虚拟。这样一来docker会相对于传统虚拟机来说“体积更轻、跑的更快、同宿主机下可创建的个数更多”。
Docker不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用systemd去启动后台服务，容器内没有后台服务的概念。举个例子，常有人在dockerfile里面这样写：
CMD service nginx start  然后发现容器执行后就立即退出了，甚至在容器内去使用systemctl命令结果却发现根本执行不了。没有区分docker容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。对于docker容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。而使用CMD指令service nginx start，则是以后台守护进程形式启动nginx服务。CMD service nginx start最终会被docker引擎转化为CMD [ &amp;quot;sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;service nginx start&amp;quot;]，因此主进程实际上是sh。那么当service nginx start命令结束后，sh也就结束了，sh作为主进程退出了，自然就会令容器退出。
正确的做法是直接执行nginx可执行文件，并且要求以前台形式运行：
CMD [&amp;quot;nginx&amp;quot;, &amp;quot;-g&amp;quot;, &amp;quot;daemon off;&amp;quot;]  慎用docker commit 知道使用docker commit可以在基础镜像层的基础上定制新的镜像。我们知道镜像是多层存储，每一层是在前一层的基础上进行的修改；而容器同样也是多层存储，是在以镜像为基础层，在其基础上加一层作为容器运行时的存储层。
举个例子，我们使用docker commit的定制一个nginx镜像：
$ docker run --name mynginx -d -p 80:80 nginx  上面的命令帮我们启动一个nginx的容器，接着我们就可以使用http://localhost来访问这个容器提供的web服务了，如果没啥意外的话，我们会看到 接着我们访问如下输出：
$ curl http://localhost &amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt; &amp;lt;style&amp;gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } &amp;lt;/style&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;h1&amp;gt;Welcome to nginx!</description>
    </item>
    
    <item>
      <title>5分钟系列 -「Go Template」</title>
      <link>https://morven.life/notes/the_go_template/</link>
      <pubDate>Fri, 24 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_go_template/</guid>
      <description>随着近几年Restful架构的盛行，前后端分离大行其道，模板渲染也由后端转移到了前端，后端只需要提供资源数据即可，这样导致类似于JSP和PHP的传统服务端模板脚本语言几乎问人问津了。但是在Go语言中，模板渲染技术不只局限于服务端标记语言（如HTML页面）的渲染，事实上，GO语言经常使用模版语言来处理譬如插入特定数据的文本转化等，虽然没有正则表达式那么灵活，但是渲染效率远优于正则表达式，而且使用起来也更简单。对于某些云计算的场景十分友好。今天，我们就来详细聊一聊Go Template的技术细节。
运行机制 模板的渲染技术本质上都是一样的，一句话来说明就是字串模板和结构化数据的结合，再详细地讲就是将定义好的模板应用于结构化的数据，使用注解语法引用数据结构中的元素（例如Struct中的特定feild，Map中的key）并显示它们的值。模板在执行过程中遍历数据结构并且设置当前光标（&amp;quot;.&amp;quot;表示当前的作用域）标识当前位置的元素。
类似于Python的jinja，Node的jade等模版引擎，Go语言模板引擎的运行机制也是类似：
 创建模板对象 解析模板字串 加载数据渲染模板  Go模板核心包 Go提供了两个标准库用来处理模板渲染text/template和html/template，它们的接口几乎一摸一样，但处理的模板数据不同。其中text/template用来处理普通文本的模板渲染，而html/template专门用来渲染格式化html字符串。
下面的例子我们使用text/template来处理普通文本模板的渲染：
package main import ( &amp;quot;os&amp;quot; &amp;quot;text/template&amp;quot; ) type Student struct { ID uint Name string } func main() { stu := Student{0, &amp;quot;jason&amp;quot;} tmpl, err := template.New(&amp;quot;test&amp;quot;).Parse(&amp;quot;The name for student {{.ID}} is {{.Name}}&amp;quot;) if err != nil { panic(err) } err = tmpl.Execute(os.Stdout, stu) if err != nil { panic(err) } }  上述代码第4行引入text/template来处理普通文本模板渲染，第14行定义一个模板对象test来解析变量&amp;quot;The name for student {{.</description>
    </item>
    
    <item>
      <title>5分钟系列 -「Go Context」</title>
      <link>https://morven.life/notes/the_go_context/</link>
      <pubDate>Tue, 12 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_go_context/</guid>
      <description>从go1.7开始，正式将context(golang.org/x/net/context)，即“上下文”包引入官方标准库。事实上，我们经常见到它，不论是在一般的服务器代码还是在复杂的并发处理程序中，它都起到很重要的作用。今天，我们就来深入研究一下它的实现以及最佳实践。
官方文档对于context包的解释是：
 Package context defines the Context type, which carries deadlines, cancelation signals, and other request-scoped values across API boundaries and between processes.
 简单来说，context包是专门用来简化处理针对单个请求的多个goroutine与请求截止时间、取消信号以及请求域的数据等相关操作。一个简单的例子是在典型的go服务器程序中，每个网络请求都需要创建单独的goroutine进行处理，这些goroutine有可能涉及多个API的调用，进而可能会开启其他的goroutine；由于这些goroutine都是在处理同一个网络请求，所以它们往往需要访问一些共享的资源，比如用户认证token、请求截止时间等；而且如果请求超时或者被取消后，所有的goroutine都应该马上退出并且释放相关的资源。使用context，即“上下文”，可以让go开发者方便地实现这些多个goroutine之间的交互操作，跟踪并控制这些goroutine，并传递request相关的数据、取消goroutine的signal或截止日期等。
Context结构 context包中核心的数据结构是一种嵌套的结构或者说是单向继承的结构。基于最初的context（根context），开发者可以根据使用场景的不同定义自己的方法和数据来继承根context；正是context这种分层的组织结构，允许开发者在每一层context都定义一些不同的特性，这种层级式的组织也使得context易于扩展，职责清晰。
context包中的最基础的数据结构是一个Context接口：
type Context interface { // Done returns a channel that is closed when this Context is canceled // or times out. Done() &amp;lt;-chan struct{} // Err indicates why this context was canceled, after the Done channel // is closed. Err() error // Deadline returns the time when this Context will be canceled, if any.</description>
    </item>
    
    <item>
      <title>5分钟系列 -「Go Routine &amp; Channel」</title>
      <link>https://morven.life/notes/the_go_channel/</link>
      <pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_go_channel/</guid>
      <description>说到Go语言，不得不提一下Go语言的并发编程。Go从语言层面增加了对并发编程的良好支持，不像Python、Java等其他语言使用Thread库来新建线程，同时使用线程安全队列库来共享数据。Go语言对于并发编程的支持依赖于Go语言的两个基础概念：Go Routine和Go Channel。
 Note: 也许我们还对并发(Concurrency)和并行(Parallelism)傻傻分不清楚，在这里再次强调两者的不同点：
Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once.
也就是说，并发是在同一时间处理多件事情，往往是通过编程的手段，目的是将CPU的利用率提到最高；而并行是在同一时间做多件事情，需要多核CPU的支持。
 Go Routine Go Routine是Go语言并行编程的核心概念之一，有人将它称作为“协程”，是比Thread更轻量级的并发单元，最小Go Routine只需极少的栈内存(大约是4~5KB)，这样十几个Go Routine的规模可能体现在底层就是五六个线程的大小，最高同时运行成千上万个并发任务；同时，Go语言内部实现了Go Routine之间的内存共享使得它比Thread更高效，更易用，我们不必再使用类似于晦涩难用的线程安全的队列库来同步数据。
创建Go Routine 要创建一个Go Routine，我们只需要在函数调⽤语句前添加go关键字，Go语言的调度器会自动将其安排到合适的系统线程上执行。实际上，我们在并发编程的过程中经常将一个大的任务分成好几块可以并行的小任务，为每一个小任务创建一个Go Routine。当一个程序启动时，其主函数即在一个单独的Go Routine中运行，我们叫它main routine，然后在主函数中使用go关键字来创建其他的Go Routine：
func subTask() { i := 0 for { i++ fmt.Printf(&amp;quot;new goroutine: i = %d\n&amp;quot;, i) time.Sleep(1 * time.Second) } } func main() { go subTask() // Create go rountine to execute sub task i := 0 // main goroutine for { i++ fmt.</description>
    </item>
    
    <item>
      <title>5分钟系列 -「Go Interface &amp; Composition」</title>
      <link>https://morven.life/notes/the_go_interface_and_composition/</link>
      <pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_go_interface_and_composition/</guid>
      <description>Go语言接口与鸭子类型的关系 什么是“鸭子类型”？
 If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck.
 以上引用自维基百科的解释描述了什么是“鸭子类型”，所谓鸭子类型，是动态编程语言的一种对象推断策略，它更关注对象能如何被使用，而不是对象的类型本身。
传统的静态语言，如Java, C++等，它们必须显示声明实现了某个接口，之后，才能用在任何需要这个接口的地方，否则编译也不会通过，这也是静态语言比动态语言更安全的原因。但是Go语言作为一门“现代”静态语言，使用的是动态编程语言的对象推断策略，它更关注对象能如何被使用，而不是对象的类型本身。也就是说，Go语言引入了动态语言的便利，同时又会进行静态语言的类型检查，因此，它采用了折中的做法：不要求类型显示地声明实现了某个接口，只要实现了相关的方法即可，编译器就能检测到。
举个例子，下面的代码片段先定义一个接口，和使用此接口作为参数的函数：
type IGreeting interface { greeting() } func sayHello(i IGreeting) { i.greeting() }  接下来再定义两个结构体：
type A struct {} func (a A) greeting() { fmt.Println(&amp;quot;Hi, I am A!&amp;quot;) } type B struct {} func (b B) greeting() { fmt.Println(&amp;quot;Hi, I am B!</description>
    </item>
    
    <item>
      <title>又是冬至日</title>
      <link>https://morven.life/posts/thoughts_in_winter_solstice/</link>
      <pubDate>Fri, 22 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/thoughts_in_winter_solstice/</guid>
      <description>冬至已到，也该向2017告别了。
最近时常酝酿着写写2017年，但到真正提笔的时候却无从下手。2017年，我一直很努力，却总是停留在原地。该离开的终究没有留下来，该坚持的还是半途而废。
2017发生了很多事，心情一直处于低谷，时常在深邃的夜晚开始怀疑自己当初的选择。理想、信念和情感&amp;hellip;所有种种陪伴了自己青春岁月的精神支撑，在这冰冷冷的，麻木的，毫无人性的现实世界里显得沧桑而无力。可能是自己过于“玻璃心”，也经常被家人评价“不成熟”的标签。
不是我无法承受现实世界的洗礼，我只是想按照自己的方式来生活。仔细想想，这些年虽然变化不少，但自己始终是个理想主义斗士。
以前的我有自己的原则，却没有太多的阅历，此外，行动力也不太够，但是仍然对这个世界充满向往。 而现在的我，却因为短期内见过太多社会的黑暗面，变得无法坚持自己的信念而变得愤世嫉俗，因为我内心的良知让我无法和这个有美好也有丑陋的世界和解。
我在无数个夜晚想象着自己回到过去，用纯粹的对世界的热情感染早已麻木的自己，提醒自己世界上还有正义，原则还有信念等一众美好的东西。
我也想拥有一台时光机器，回到过去。用自己的经历和行动力，告诫尚处于十字路口的自己坚定自己正确的选择。而不是在理想的分崩离析中不断回味那些外表逐渐模糊但内在却深入骨髓而不能磨灭的印记。
不管2017年对我来说是多么tough的一年，毫无疑问，它也将会成为我永远都铭记和感恩的一年。
希望在即将到来的2018年里，不要再掉进自己思想的漩涡里，重新拿回了生活的主导权，不忘初心，坚持梦想。
  人生几回仿往事，山形依旧枕寒流。 ——刘禹锡《西塞山怀志》   </description>
    </item>
    
    <item>
      <title>十年，火影</title>
      <link>https://morven.life/posts/ten-years-for-naruto/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/ten-years-for-naruto/</guid>
      <description>去年听到火影忍者漫画完结的时候，心里是极其复杂的。急于见证万年吊车尾主人公鸣人的结局，却更多地是对于十年青春陪伴的不舍。
第一次见到火影是一个黄头发中二少年踩着树杈飞来飞去，身后跟着几个小屁孩嘴里大叫着佐助。当时觉得这种需要按着牛顿棺材板才能看的动漫实在无聊，但是正值高中紧张复习期间，偶尔用MP4从同学那里拷几集看来缓减一下压力。
对火影路转粉应该是在“复制忍者”旗木卡卡西与原雾隐村“忍刀七人众”之一桃地再不斩天桥决战。当时和几个小伙伴周末躲在教室偷用多媒体反复播放那几集，俨然被各种各样的结印手势以及高潮迭起的热血剧情吸引得神魂颠倒。一方是年仅12岁时就成为上忍木叶天才旗木卡卡西，加上可以复制了上千种忍术的开挂写轮眼；另一方是雾隐村“忍刀七人众”之一，无声杀人术技巧之高超的“鬼人”桃地再不斩。两人实力相近，却有着不同的目标，经过几番焦灼战斗，最后在漫天的飞雪中，当再不斩死在白的身边时，流着泪说道：“如果可以的话，真想和你去同一个地方。”，而与此同时，一颗雪花的结晶则因为白刚刚死去时残留的体温而在白的眼角融化流下，仿佛印证了再不斩那句“白，是你在哭泣么？”，再配上背景音乐——钢琴版“Sadness and Sorrow”，​最终，这成为火影里最初也是最让人感动以及震撼的画面。
其实，火影的战斗场景非常丰富，除了高速飞行和冲刺、拳脚过招、立体镜头、火焰爆发等，还会有很多战斗的细节与伏笔。超燃的热血剧情加上富有逻辑性的战斗场景设计完全让高中时期的我们进入了节奏。随着剧情的展开，久而久之，开始对“万年吊车尾”鸣人的未来越来越感兴趣。漩涡鸣人，见证了白心甘情愿为再不斩牺牲，感同身受地与我爱罗诉说着相同的过去，将佐助视为最重要的羁绊，为了对小樱的承诺，拼尽全力追回佐助，独自练习着更强的忍术，为自己的父母深深地感到骄傲，更想用爱感化九尾，他所经历的一切，都为他贯彻自己的忍道。正是印证了鼬所说的“不是当上火影的人才能得到认可，只有得到大家的认可才能当上火影”。
其实不止主人公，火影里面每个人物都是栩栩如生。 实力又神秘，帅气又低调，漫不经心又可靠，冷酷中还有温暖的天才忍者旗木卡卡西。 高冷而睿智，隐秘而伟大，残忍却温柔，怀揣光明于黑暗中独自行走的最完美忍者宇智波鼬。 猥琐好色而真性情，荒诞不羁却坚韧刚强，洒脱随和而正气凛然，推动整个火影进程的自来也老师。 &amp;hellip;
我想火影完结对我来说不是件过于悲伤的事，尽管那天听到消息的我真的有种不可填补的缺失感，但是关于鸣人的故事暂时告一段落，但新的故事还在延续，更何况我们还有高梨康治大师级的音乐。每个人都有自己的人生，无论振作拼搏，还是颓废懈怠，时间终会带走一切，我们能留下的，只是种子，就像火影的种子早已埋在心底，何时发芽？也许明天，也许……就是现在。</description>
    </item>
    
    <item>
      <title>毕业这两年</title>
      <link>https://morven.life/posts/the-two-years/</link>
      <pubDate>Tue, 20 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/the-two-years/</guid>
      <description>步入6月，一场青春的盛宴，如期而至，这场盛宴的主题，是毕业。我虽早已是这场盛宴之外的人，但看到微博上在校同学们的种种分享，仿佛是在提醒自己：WTF, 你都毕业两年了！
是啊，我毕业都两年了！
想到这个，心中难免感慨万千，但是要谈谈这万千感慨，却又不知从何说起，真有一种情深而何以往之的感觉！
毕业这两年，让我渐渐感到陌生的，是自己。我似乎已经越来越不清楚自己是谁，越来越不知道该往何方。我时而追忆大学时代的那个自己，以寻求一点自我的慰藉。当慰藉醒来，我梦让发现，和过去的自己相比，我此时整个的生命轨迹，正在下陷。
大学时的那个我，总得而言，是一个乐于奉献、敢于担当的人。从大一入学之处欢呼雀跃，奔走于各个社团之间，到研究生逃离实验室，和几个“疯狂的哥们儿”整天呆在破烂不堪的写字楼里创业，那时候感觉自己还能“输得起”，所以做什么事情都不惧困难，在别人看来那是“不务正业”，自己却乐此不彼。
但是从毕业到现在的这两年的经历，显然让我感觉到自己正处于人生的低谷。当然，并不是我现在的处境有多么的不堪，而是精神上“斗志泯灭”。回头看看那个曾经有着纯高理想追求，并定会为之奋斗的自己，如今正处于一种“望风披靡”，追求“息事宁人”的状态。
毕业这两年，我一直就职于同一家公司，两年前的今天，我期待着可以顺风顺水地“大干一番”，毕竟公司肯定期待纯粹的技术出生，坚持技术路线的新鲜血液。的确，我投入了极大的热情，也获得了良好的发展。但是由于某种原因，公司一直处于”不稳定期“，自己也努力适应公司公司的转变策略。这样的好处是很明显的，经过几次的变化，虽然职位再三调整，但自己已经适应公司工作环境，也接触了目前各个产品线上的研发同事，合作也非常愉快，显然，公司需要这样能跟着公司战略转变的员工。然而，经过几轮的变化，自己心态早已从之前的“主动学习”，“积极探索”转变到“随波追流”，追求“完成自己分内工作就行”。我不知道这种变化是一种普遍现象，还是我自己开始濒临堕落边缘。
对工作没有了开始时的热情，问题到底处在哪里？整个而言，我现在仍然处于一个迷茫期，前路漫漫，不知所以。
虽然很多的事还没有想清楚，但并不意味着活得糊涂。我这两年虽然让自己的一些有点沉寂了，但是很多东西，我依然在坚持。坚持对善的向往，坚持对独立人格的向往，坚持读书学习思考，时刻保持着对人生观与价值观的自信。暂时得不到太多，只能好好坚持一些自认为可贵的东西，唯有坚持，才有改变的可能。</description>
    </item>
    
    <item>
      <title>从零开始认识iptables</title>
      <link>https://morven.life/notes/the_knowledge_of_iptables/</link>
      <pubDate>Sat, 20 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_knowledge_of_iptables/</guid>
      <description>在使用Linux的过程中，很多人和我一样经常接触iptables，但却只知道它是用来设置Linux防火墙的工具，不知道它具体是怎么工作的。今天，我们就从零开始认识一下Linux下iptables的具体工作原理。
iptables是Linux上常用的防火墙软件netfilter项目的一部分，所以要讲清楚iptables，我们先理一理什么是防火墙？
什么是防火墙 简单来说，防火墙是一种网络隔离工具，部署于主机或者网络的边缘，目标是对于进出主机或者本地网络的网络报文根据事先定义好的规则做匹配检测，规则匹配成功则对相应的网络报文做定义好的处理（允许，拒绝，转发，丢弃等）。防火墙根据其管理的范围来分可以将其划分为主机防火墙和网络防火墙；根据其工作机制来区分又可分为包过滤型防火墙（netfilter）和代理服务器（Proxy）。我们接下来在这篇笔记中主要说说**包过滤型防火墙（netfilter）。
 Note: 也有人将tcp_warrpers也划分为防火墙的一种，它是根据服务程序软件的名称来处理网络数据包的工具。
 包过滤型防火墙的工作原理 包过滤型防火墙主要依赖于Linux内核软件netfilter，它是一个Linux内核“安全框架”，而iptables是内核软件netfilter的配置工具，工作于用户空间。iptables/netfilter组合就是Linux平台下的过滤型防火墙，并且这个防火墙软件是免费的，可以用来替代商业防火墙软件，来完成网络数据包的过滤，修改，重定向以及网络地址转换（nat）等功能。
 Note: 在有些Linux发行版上，我们可以使用systemctl start iptables来启动iptables服务，但需要指出的是，iptables 并不是也不依赖于守护进程，它只是利用Linux内核提供的功能。
 Linux网络管理员通过配置iptables规则以及对应的网路数据包处理逻辑，当网络数据包符合这样的规则时，就做执行预先定义好的相应的处理逻辑。可以简单的总结为：
IF network_pkg match rule; THEN handler FI  其中规则可以包括匹配数据报文的源地址，目的地址，传输层协议（TCP/UDP/ICMP/..）以及应用层协议（HTTP/FTP/SMTP/..）等，处理逻辑就是根据规则所定义的方法来处理这些数据包，如放行（accept），拒绝（reject），丢弃（drop）等。
而netfilter是工作于内核空间当中的一系列网络（TCP/IP）协议栈的钩子（hook），为内核模块在网络协议栈中的不同位置注册回调函数（callback）。也就是说，在数据包经过网络协议栈的不同位置时做相应的由iptables配置好的处理逻辑。 netfilter中的五个钩子（这里也称为五个关卡）PRE_ROUTING，INPUT，FORWARD，OUTPUT，POST_ROUTING，网络数据包的流向图如下图所示：
 当主机/网络服务器网卡收到一个数据包之后进入内核空间的TCP/IP协议栈进行层层解封装 刚刚进入网络层的数据包通过PRE_ROUTING关卡时，要进行一次路由选择，当目标地址为本机地址时，数据进入INPUT，非本地的目标地址进入FORWARD（需要本机内核支持IP_FORWARD），所以目标地址转换通常在这个关卡进行 INPUT：经过路由之后送往本地的数据包经过此关卡，所以过滤INPUT包在此点关卡进行 FORWARD：经过路由选择之后要转发的数据包经过此关卡，所以网络防火墙通常在此关卡配置 OUTPUT：由本地用户空间应用进程产生的数据包过此关卡，所以OUTPUT包过滤在此关卡进行 POST_ROUTING：刚刚通过FORWARD和OUTPUT关卡的数据包要通过一次路由选择由哪个接口送往网络中，经过路由之后的数据包要通过POST_ROUTING此关卡，源地址转换通常在此点进行  上面提到的这些处于网络（TCP/IP）协议栈的“关卡”，在iptables的术语里叫做“链（chain）”，内置的链包括上面提到的5个：
 PreRouting Forward Input Output PostRouting  一般的场景里面，数据包的流向基本是：
 到本主机某进程的报文：PreRouting -&amp;gt; Input -&amp;gt; Process -&amp;gt; Output -&amp;gt; PostRouting 由本主机转发的报文：PreRouting -&amp;gt; Forward -&amp;gt; PostRouting  iptables的四表五链 iptables默认有五条链（chain），分别对应上面提到的五个关卡，PRE_ROUTING，INPUT，FORWARD，OUTPUT，POST_ROUTING，这五个关卡分别由netfilter的五个钩子函数来触发。但是，为什么叫做“链”呢？ 我们知道，iptables/netfilter防火墙对经过的数据包进行“规则”匹配，然后执行相应的“处理”。当报文经过某一个关卡时，这个关卡上的“规则”不止一条，很多条规则会按照顺序逐条匹配，将在此关卡的所有规则组织称“链”就很适合，对于经过的数据包按照顺序逐条匹配“规则”。
另外一个问题是，每一条“链”上的一串规则里面有些功能是相似的，比如，A类规则都是对IP或者端口进行过滤，B类规则都是修改报文，我们考虑能否将这些功能相似的规则放到一起，这样管理iptables规则会更方便。iptables把具有相同功能的规则集合叫做“表”，并且定一个四种表：
 filter：负责过滤功能；与之对应的内核模块是iptables_filter nat：Network Address Translation，网络地址转换功能，典型的比如SNAT，DNAT；与之对应的内核模块是iptables_nat mangle：解包报文，修改并封包；与之对应的内核模块是iptables_mangle raw：关闭nat表上启用的连接追踪机制；与之对应的内核模块是iptables_raw  这样，Linux网络管理员所定义的iptables“规则”都存在于这四张表中。</description>
    </item>
    
    <item>
      <title>编写健壮的Shell脚本</title>
      <link>https://morven.life/notes/writing-robust-shell-scripts/</link>
      <pubDate>Mon, 06 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/writing-robust-shell-scripts/</guid>
      <description>写Shell脚本应该已经成为程序员必须掌握的技能了。因为Shell脚本简单易上手的特性，在日常工作中，我们经常使用Shell脚本来自动化应用的部署测试，环境的搭建与清理等等。殊不知，Shell脚本也会有各种坑，经常导致Shell脚本因为各种原因不能正常执行成功。实际上，编写健壮可靠的Shell脚本也是有一定的技巧的，今天我们就来一一说明。
set -euxo pipefail 在执行Shell脚本的时候，通常都会创建一个新的Shell，比如，当我们执行：
bash script.sh  Bash会创建一个新的Shell来执行script.sh，同时也默认给定了这个执行环境的各种参数。set命令可以用来修改Shell环境的运行参数，不带任何参数的set命令，会显示所有的环境变量和Shell函数。对于所有可以定制的运行参数，请查看官方手册，我们重点介绍其中最常用的四个。
set -x 默认情况下，Shell脚本执行后只显示运行结果，不会展示结果是哪一行代码的输出，如果多个命令连续执行，它们的运行结果就会连续输出，导致很难分清一串结果是什么命令产生的。
set -x用来在运行结果之前，先输出执行的那一行命令，行首以+表示是命令而非命令输出，同时，每个命令的参数也会展开，我们可以清晰地看到每个命令的运行实参，这对于Shell的debug来说非常友好。
#!/bin/bash set -x v=5 echo $v echo &amp;quot;hello&amp;quot; # output: # + v=5 # + echo 5 # 5 # + echo hello # hello  实际上，set -x还有另一种写法set -o xtrace。
set -u Shell脚本不像其他高级语言，如Python, Ruby等，Shell脚本默认不提供安全机制，举个简单的例子，Ruby脚本尝试去读取一个没有初始化的变量的内容的时候会报错，而Shell脚本默认不会有任何提示，只是简单地忽略。
#!/bin/bash echo $v echo &amp;quot;hello&amp;quot; # output: # # hello  可以看到，echo $v输出了一个空行，Bash完全忽略了不存在的$v继续执行后面的命令echo &amp;quot;hello&amp;quot;。这其实并不是开发者想要的行为，对于不存在的变量，脚本应该报错且停止执行来防止错误的叠加。set -u就用来改变这种默认忽略未定义变量行为，脚本在头部加上它，遇到不存在的变量就会报错，并停止执行。
#!/bin/bash set -u echo $a echo bar # output: # .</description>
    </item>
    
    <item>
      <title>后会无期，2016</title>
      <link>https://morven.life/posts/farewell-2016/</link>
      <pubDate>Sun, 18 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/farewell-2016/</guid>
      <description> 2016年，很长又很短。 大学毕业，拼命折腾。 概括起来，三言两语便是全部，往细里谈，三天三夜也不够。 但我还是要写，有些情感，只能用文字表达，有些思考，在笔尖之下才会刻骨铭心。
生活 从参加工作到现在，差不多一年多了。这期间的大部分时间我都一个人住着。每天早上睡到自然醒，洗脸刷牙，心情好楼下买个早点，然后悠哉悠哉地走到公司。下班不用急着回家，独自呆在电脑前听听歌，看看书。肚子饿的时候去公司周围吃个晚饭，然后沿着公园慢慢哟哟地走回家，天气好的话去公园旁边的球场打打球。回到家后，洗完澡看看书，刷会儿知乎，偶尔来个葛优瘫追追美剧。平时要是有人约个饭局的话就跟几个关系不错的哥们儿一起吃个饭，然后各回各家。
我这人不太合群，不喜欢热闹。除了碍于情面上的应酬，大多时候我都喜欢一个人呆着。我比较享受这种独自生活的状态。看书，听歌，看电影，玩手机，或者沏一杯茶然后发呆愣神的呆着，我都很喜欢。下半年开始，星巴克成为我出没的第三场所-公司、家之外最好的选择。经常在星巴克看到聋哑人的聚会，围成一桌用手语沟通，还看到过织毛衣的女人，都觉得蛮温暖的。我有时会点杯喝的打开笔电戴着耳机一坐一下午甚至一整天。我始终都觉得，只有当一个人的时候，才能听到自己内心的声音，才有一个好的状态去想明白一些事情。外界纷扰反而会容易让人失去判断和自我意识。
工作 一直在思考，自己到底适合做什么样的开发岗位。从大学本科阶段的被动接受计算机知识的阶段，到研究生期间主动涉猎计算机各方面知识，从最初的汇编语言与C，到面向对象语言，再到函数式编程，从小型单片机与操作系统到大型分布式系统缓存／并发的设计与实现。我一直没有停止学习，总想着扩展自己的技术栈，打造枝繁叶茂的技能树，却没有沉下心去专攻某项技能。总关注于横向领域的扩充，却忽略技能纵向深度的延伸。
从这方面来讲，2016年确实是等待以及转身的重要节点。因为经历多次工作角色的转变，终于找到最适合自己的工作方向。说实话，之前没有想过自己会从事前端开发，毕竟发展速度太快，每天都有新的轮子出现，难免不会迷失在其中。但是真正接过这个角色之后，就不要想那么多，我才27岁又不是72岁，既然入了这一行，就要有这一行的“匠人”精神。说实话，我一直也没认为IT这一行多么了不起，我们从事这一行只不过是个普通的“手艺人”，经历多年打磨自己的大脑，只有足够优秀才能制作出好的作品。但也正是因为如此，我们可以尽情发挥自己的创意。
从2016年4月开始，开始弥补自己前段领域的空白：从HTML到Jade，从CSS到Less／Sass，从Javascript到Typescript，从jQuery到Angular，从Grunt到Webpack&amp;hellip;虽然自己之前也略有涉猎前端知识，但对完整的前端技术栈知识略知皮毛。从最初的简单地写写页面到现在轻松构建整个项目框架，偶尔写个‘轮子’提供产品的个性化需求解决方案。回过头来看，这一年一路走来，发现自己的兴趣所在，沉下心去思考，研究，虽然踩了很多坑，但也收获颇丰。现在大部分人对于前段的认识还停留在‘刀耕火种’的阶段，其实这几年前端领域的快速发展导致前端工程化已经成为不可逆转的趋势。
梦想  我们走再远，终归难违初心所在。
 曾经的自己大部分时间都处于的奋斗的状态，放弃了生活的品质。如今独自生活后才发现，人一定要活在自己的气场里，才能做什么都舒心。埋头做事没什么不可，但久而久之生活里会充斥着现实的土腥味，偶尔跳出来做做自己喜欢的事，会让本就不好过的日子幸福很多。
最基本的作为自己必须培养一些爱好，必须做到一觉醒来清楚至少今天还能干什么。是去球场挥汗如雨？去找个知心老友唠嗑？还是制定一天内该吸收的知识？这些都行，自己必须做到积攒这种微小的期待和快乐，这样才不会被遥不可及的梦和无法掌控的爱给拖垮。
作为产品的创造者，总是梦想着有朝一日能有自己的优秀作品问世，自己设计，自己实现，自己运维。不期待能有成千上万的用户，不奢求能带来万贯的财富。所以，一直保持着一种虚心学习的态度，对自己、对产品。这是一个修炼的过程，若有懈怠或者失去警醒，就会距离创造伟大的产品，愈来愈远。
对2017年的展望 总的来说，2016年对我来说，还算满意。没有忘记自己的初心，也在不断获取想得到的东西，这比迷失自我的成功更难得吧。希望2017年继续保持这种状态，勿骄勿燥。
 多接触移动端开发领域，不管是iOS还是Android 系统学习UI／UX方面的知识 参加些前端以及Design方面的线下聚会 多读些历史人文方面的书籍 保持健康的生活习惯 带着相机出去走走  </description>
    </item>
    
    <item>
      <title>Webpack使用小结</title>
      <link>https://morven.life/notes/the_webpack_summary/</link>
      <pubDate>Sun, 20 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_webpack_summary/</guid>
      <description>分而治之是软件工程领域的重要思想，对于复杂度日益增加的前端也同样适用。一般前端团队选择合适的框架之后就要开始考虑开发维护的效率问题。而模块化是目前前端领域比较流行的分而治之手段。 Javascript模块化已经有很多规范和工具，例如CommonJS/AMD/requireJS/CMD/ES6 Module，在上篇文章中有详细的介绍。CSS模块化基本要依靠Less, Sass以及Stylus等于处理器的import/minxin特性实现。而HTML以及HTML模版和其他资源比如图片的模块化怎么去处理呢？ 这也正是webpack要解决的问题之一。严格来说，webpack是一个模块打包工具（module bundler），它既不像requireJS和seaJS这样的模块加载器，也不像grunt和gulp这样优化前端开发流程的构建工具，像是两类工具的集合却又远不止如此。
Webpack是一个模块打包工具，它将JS、CSS、HTML以及图片等都视为模块资源，这些模块资源必然存在某种依赖关系，webpack就是通过静态分析各种模块文件之间的依赖关系，通过不同种类的Loader将所有模块打包成起来。
webpack VS Gulp 严格来说，Gulp与webpack并没有可比性。Gulp应该和Grunt属于同一类工具，能够优化前端工作流程，比如压缩合并JS、CSS ，预编译Typescript、Sass等。也就是说，我们可以根据需要配置插件，就可以将之前需要手动完成的任务自动化。webpack作为模块打包工具，可以和browserify相提并论。两者都是预编译模块化解决方案。相比requireJS、seaJS这类‘在线’模块化方案更加智能。因为是‘预编译’，不需要在浏览器中加载解释器。另外，你可以直接在本地编写JS，不管是 AMD / CMD / ES6 风格的模块化，都编译成浏览器认识的JS。
总之，Gulp只是一个流程构建工具，而webpack、browserify等是模块化解决方案。Gulp也可以配置seaJS、requireJS甚至webpack的插件。
避免多个配置文件 刚开始接触webpack的时候，不管是去浏览GitHub上面star数较多的webpack项目，还是搜索stack overflow上面赞成数较多的回答，发现很多人提倡在一个项目中针对开发和产品发布提供不同的配置文件，比如webpack.dev.config.js和webpack.prod.config.js。看起来很清晰，也可以让新手迅速上手老项目，但仔细研究就会发现，不通环境的配置文件大部分配置项基本相同。这与工程领域一直提倡的DRY（Don&amp;rsquo;t Repeat Yourself）原则相悖，于是就产生了另外一种做法，先生成一个common的webpack.common.config.js，然后再针对不同的环境去继承（其实就是require）common的配置文件。但是不管怎样，其实都是生成多个不同的配置文件。如果换个角度想想，这些配置文件虽然不同，但都遵循着node的逻辑，所以完全可以只维护一个配置文件，然后针对不同的环境传入不同的参数。如果你使用npm，则完全可以在package.json文件中这样写：
&amp;quot;scripts&amp;quot;: { &amp;quot;devs&amp;quot;: &amp;quot;cross-env DEV=1 webpack-dev-server --hot --inline&amp;quot;, &amp;quot;build&amp;quot;: &amp;quot;cross-env PROD=1 rm -rf ./build &amp;amp;&amp;amp; webpack --p&amp;quot; }  其中cross-env是个跨平台的环境变量设置工具，可以允许Unix风格环境变量设置通用在window平台。 这样只维护一个webpack.config.js配置文件，然后在配置文件中处理自定义的参数。怎么处理自定义参数呢？这里我们使用webpack自带插件definePlugin提供魔力变量（magic globals）来处理：
plugins: [ new webpack.DefinePlugin ({ __DEV__: JSON.stringify(JSON.parse(process.env.DEV || &#39;false&#39;)), __PROD__: JSON.stringify(JSON.parse(process.env.PROD || &#39;false&#39;)) }) ]  然后在配置文件的其他地方就可以根据设定的环境变量更有针对性地配置不同插件等。甚至在业务逻辑中也可以这样针对不同环境做针对性地调试，比如在开发环境下可以AJAX可以调试本地mock数据，然后在发布的时候，可以正常访问服务端数据。
if (__DEV__) { // code for dev //.</description>
    </item>
    
    <item>
      <title>Javascript模块化开发</title>
      <link>https://morven.life/notes/developing-modular-javascript/</link>
      <pubDate>Sun, 16 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/developing-modular-javascript/</guid>
      <description>随着互联网时代的到来，前端技术更新速度越来越快。起初只要在script标签中嵌入几行代码就能实现一些基本的用户交互，到现在随着Ajax，jQuery，MVC以及MVVM的发展，Javascript代码量变得日益庞大复杂。 网页越来越像桌面程序，需要一个团队分工协作、进度管理、单元测试等等&amp;hellip;&amp;hellip;开发者不得不使用软件工程的方法，管理网页的业务逻辑。 Javascript模块化开发，已经成为一个迫切的需求。理想情况下，开发者只需要实现核心的业务逻辑，其他都可以加载别人已经写好的模块。 但是，Javascript不是一种模块化编程语言，它不支持&amp;rdquo;类&amp;rdquo;（class），更遑论&amp;rdquo;模块&amp;rdquo;（module）了。直到前不久ES6正式定稿，Javascript才开始正式支持&amp;rdquo;类&amp;rdquo;和&amp;rdquo;模块&amp;rdquo;，但还需要很长时间才能完全投入实用。
什么是模块化 模块是任何大型应用程序架构中不可缺少的一部分，一个模块就是实现特定功能的代码区块或者文件。模块可以使我们清晰地分离和组织项目中的代码单元。在项目开发中，通过移除依赖，松耦合可以使应用程序的可维护性更强。有了模块，开发者就可以更方便地使用别人的代码，想要什么功能，就加载什么模块。模块开发需要遵循一定的规范，否则就会混乱不堪。
Javascript社区做了很多努力，在现有的运行环境中，实现&amp;rdquo;模块&amp;rdquo;的效果。本文总结了当前＂Javascript模块化编程＂的最佳实践，说明如何投入实用。
Javascript模块化基本写法 在第一部分，将讨论基于传统Javascript语法的模块化写法。
原始写法 模块就是实现特定功能的一组方法。 只要把不同的函数（以及记录状态的变量）简单地放在一起，就算是一个模块。
function func1(){ //... } function func2(){ //... }  上面的函数func1()和func2()，组成一个模块。使用的时候，直接调用就行了。 这种做法的缺点很明显：&amp;rdquo;污染&amp;rdquo;了全局变量，无法保证不与其他模块发生变量名冲突，而且模块成员之间看不出直接关系。
对象写法 为了解决上面的缺点，可以把模块写成一个对象，所有的模块成员都放到这个对象里面。
var moduleA = new Object({ _count : 0, func1 : function (){ //... }, func2 : function (){ //... } });  上面的函数func1()和func2(），都封装在moduleA对象里。使用的时候，就是调用这个对象的属性。
moduleA.func1();  但是，这样的写法会暴露所有模块成员，内部状态可以被外部改写。比如，外部代码可以直接改变内部计数器的值。
moduleA._count = 3;  立即执行函数写法 使用&amp;rdquo;立即执行函数&amp;rdquo;（Immediately-Invoked Function Expression，IIFE），可以达到不暴露私有成员的目的。
var moduleA = (function(){ var _count = 0; var func1 = function(){ //.</description>
    </item>
    
    <item>
      <title>聊聊Pied Pier的压缩算法</title>
      <link>https://morven.life/posts/the_data_compression_in_pied_pier/</link>
      <pubDate>Sat, 18 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/the_data_compression_in_pied_pier/</guid>
      <description>最近终于追完了HBO的自制喜剧《硅谷》的第三季。《硅谷》算是一部非常小众的美剧了，主要讲述湾区几个IT男创业的故事，剧情并没有过多围绕他们怎么写代码，而是把关注点聚焦在创业想法的诞生以及初期公司的成立以及与风投斡旋的过程中的戏剧冲突上，让&amp;rdquo;内行人&amp;rdquo;啼笑皆非。每季的后几集都有点儿燃烧，原本“改变世界”之类现实中会被嘲讽的话，却是最能触动内心的！
《硅谷》之所以与众不同，还因为剧中的很多理论都是很值得推敲的。我们今天就来聊一聊《硅谷》S2E08中提出的Pied Pier基础核心算法的“middle-out”数据压缩算法。为了便于理解，我们先来了解一下数据压缩算法的基本原理、“信息熵”以及霍夫曼编码。
数据压缩的原理 数据压缩原理很简单，概括起来就是找到那些重复出现的数据，然后用用更短的符号替代，从而达到缩短数据大小的目的。
例如，我有一段文本&amp;rdquo;ABCDABCDABCDABCDABCDABCD&amp;rdquo;，显然我们使用&amp;rdquo;6ABCD&amp;rdquo;也能替代原来的数据，因为可以根据&amp;rdquo;7ABCD&amp;rdquo;推算出原文本&amp;rdquo;ABCDABCDABCDABCDABCDABCD&amp;rdquo;，数据从原来的“28”byte变成了“5”byte，数据压缩比为“5/24”约等于“20.8”。事实上，只要保证对应关系，可以用任意字符代替那些重复出现的字符串。这让我想到了现在移动互联网时代广泛使用的Emoji，我们可以使用一个简单的Emoji表情来表达原来需要多个字表达的意思。
本质上，所谓&amp;rdquo;压缩&amp;rdquo;就是找出文件数据内容的概率分布，将那些出现概率高的部分代替成更短的形式。所以，内容越是重复的文件，就可以压缩地越小。比如，&amp;rdquo;ABCDABCDABCDABCDABCDABCD&amp;rdquo;可以压缩成&amp;rdquo;6ABCD&amp;rdquo;。与之对应地，如果数据的内容毫无重复，就很难压缩。极端情况就是，遇到那些均匀分布的随机字符串，往往连一个字符都压缩不了。比如，任意排列的10个阿拉伯数字（5271839406），就是无法压缩的；再比如，无理数（比如π）也很难压缩。
总结一下，压缩就是消除冗余的过程，用更精简的形式表达相同的复杂内容。可以想象，压缩过一次以后，文件中的重复字符串将大幅减少。好的压缩算法，可以将冗余降到最低，以至于再也没有办法进一步压缩。所以，压缩已经压缩过的文件（递归压缩），通常是没有意义的。
数据压缩的极限 我们可以从数学上用反证法证明数据压缩是有极限的，也就是不可能无限压缩一份数据而保证内容不丢失。
假定任何文件都可以压缩到N个二进制位以内，那么最多有2N种不同的压缩结果。这就是说，如果有2N+1个文件，必然至少有两个文件会产生同样的压缩结果。这就意味着这两个文件不可能无损地还原。因此，得到证明，并非所有文件都可以压缩到N个二进制位以下。
N是一个基于压缩的数据确定的数字，我们很自然地想知道，这个N到底是多少？
按照我们前面的关于数据压缩的原理，我们知道数据压缩可以分解成两个步骤。
 得到数据内容的概率分布，哪些部分出现的次数多，哪些部分出现的次数少 对数据内容进行编码，用较短的符号替代那些重复出现的部分  对于一封确定的数据文件来说，它的概率分布是确定的，不同的压缩算法主要是因为第二部编码方式的不同，最优的压缩算法，当然是最短的符号表示作为替代原数据内容。
我们使用数学归纳法来来演算一下N的值：
 最简单的情况，我们要压缩的数据只有一部分；这一部分只有两个值，那么一个二进制数就可以表示；这一部分只有三个值，那么就需要两个二进制数来表示；这一部分有n个不同的值，那么就需要&amp;rdquo;log2(n)&amp;ldquo;个二进制位来表示； 假设在数据文件各个字符均匀出现的情况下，一个字符在某一部分中出现的概率是p，也就是说这一部分可能会出现1/p种不同的情况，那么，这一部分就需要至少&amp;rdquo;log2(1/p)&amp;ldquo;个二进制位来表示； 推广开来，如果文件有n个部分组成，每个部分的内容在文件中的出现概率分别为p1、p2、&amp;hellip;pn，那么替代符号占据的二进制最少为下面这个式子：   log2(1/p1) + log2(1/p2) + ... + log2(1/pn)
= ∑ log2(1/pn)
 这就是数据压缩的极限。
信息熵 有了前面得到的数据压缩极限的公式，很容易知道，对于n相等的两个文件，概率p决定了这个式子的大小。p越大，表明文件内容越有规律，压缩后的体积就越小；p越小，表明文件内容越随机，压缩后的体积就越大。
我们将之前的数据压缩的极限公式除以数据的组成部分，既可以得到平均每个符号所占用的二进制位，这样我们就可以很方便的比较不同大小的文件的压缩极限：
 ∑ log2(1/pn)/n
= log2(1/p1)/n + log2(1/p2)/n + ... + log2(1/pn)/n
 更进一步，我们可以得到每个字符所占用的二进制位的数学期望：
 p1*log2(1/p1) + p2*log2(1/p2) + ... + pn*log2(1/pn)
= ∑ pn*log2(1/pn)
= E( log2(1/p) )
 结果是每个字符所占用的二进制位的数学期望等于概率倒数的对数的数学期望。</description>
    </item>
    
    <item>
      <title>JSON Web Token</title>
      <link>https://morven.life/notes/the_jwt_quick_start/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_jwt_quick_start/</guid>
      <description>近几年，前后端分离大行其道。在典型的前后端分离的应用架构中，后端主要作为Model层，为前端提供数据访问API。前后端之间的通信需要在不可信（Zero Trust）的异构网络之间进行，为了保证数据安全可靠地在客户端与服务端之间传输，实现服务端的客户端认证就显得非常重要。而HTTP协议本身是无状态的，实现服务端的客户端认证的基础是记录客户端和服务端的对话状态。
我们最熟悉的服务端认证客户端的方式就是基于Session/Cookie的状态记录方式，服务端在第一次请求时声生成对应的Session发送给客户端保存在Cookie中，同时Session信息还会保存在服务器端，然后客户端之后对于服务器端的每次请求都需要带上Cookie，服务器端取出相应Session并与保存的Session信息进行对比，以实现身份的认证。
这种模式最大的问题是，没有分布式架构，无法支持横向扩展。如果使用一个服务器，该模式完全没有问题。但是，如果它是服务器群集或面向服务的跨域体系结构的话，则需要一个统一的session数据库库来保存会话数据实现共享，这样负载均衡下的每个服务器才可以正确的验证用户身份。
举例来说，某企业同时有两个不同的网站A和网站B提供服务，如何做到用户只需要登录其中一个网站，然后它就会自动登录到另一个网站？
一种解决方案是使用听过持久化Session的基础设施（如Redis），写入Session数据到持久层。收到新的客户端请求后，服务端从从持久层查找对应的Session信息。这种方案的优点在于架构清晰，而缺点是架构修改比较费劲，整个服务的验证逻辑层都需要重写，工作量相对较大。而且由于依赖于持久层的数据库或者问题系统，会有单点风险，如果持久层失败，整个认证体系都会挂掉。
儿JWT另辟蹊径，基于Token（令牌）认证客户端，也就是说只需要在每次客户端的请求的HTTP头部附上 对应的Token，由服务器端去检查Token的签名来确保Token没有被篡改，这样通过客户端保存数据，而服务器根本不保存会话数据，每个请求都被发送回服务器端认证。
什么是JWT(JSON Web Token) 根据官方的定义，JWT是一套开放的标准（RFC 7519），它定义了一套简洁（compact）且安全（URL-safe）的方案，可以在客户端和服务器之间传输JSON格式的Token信息。
JWT工作原理 JWT服务端认证的基本原理是在服务器身份验证之后，将生成一个JSON对象并将其发送回用户，如下所示。
{ &amp;quot;username&amp;quot;: &amp;quot;morvencao&amp;quot;, &amp;quot;role&amp;quot;: &amp;quot;Admin&amp;quot;, &amp;quot;expire&amp;quot;: &amp;quot;2017-02-08 12:45:43&amp;quot; }  之后，当客户端与服务器端通信时，客户端需要在请求中发回这个JSON对象。服务器仅依赖于这个JSON对象的内容来认证客户端。为了防止中间人（man-in-middle）篡改数据，服务器将在生成JSON对象时添加签名。但服务器不保存任何会话数据，即服务器变为无状态，使其更容易扩展。
JWT的数据结构 一个典型的JWT的数据结构看起来如下图所示：
JWT对象为一个很长的字符串，字符之间通过&amp;rdquo;.&amp;ldquo;分隔符分为三个子串，各字串之间也没有换行符。每一个子串表示了一个功能块，总共有以下三个部分：
 JWT头  JWT头部分是一个描述JWT元数据的JSON对象，通常如下所示：
{ &amp;quot;alg&amp;quot;: &amp;quot;HS256&amp;quot;, &amp;quot;typ&amp;quot;: &amp;quot;JWT&amp;quot; }  在上面的代码片段中，alg属性表示签名使用的算法，默认为HMAC SHA256（HS256）；typ属性表示令牌的类型，JWT令牌统一写为JWT；最后，使用Base64URL算法将上述JSON对象转换为字符串保存。
 有效载荷  有效载荷部分，是JWT的主体内容部分，也是一个JSON对象，包含需要传递的数据。JWT指定七个默认字段供选择：
iss：发行人 exp：到期时间 sub：主题 aud：用户 nbf：在此之前不可用 iat：发布时间 jti：JWT ID用于标识该JWT  除以上默认字段外，我们还可以自定义私有字段，如下例所示：
{ &amp;quot;sub&amp;quot;: &amp;quot;xxxxxxxxx&amp;quot;, &amp;quot;username&amp;quot;: &amp;quot;morvencao&amp;quot;, &amp;quot;role&amp;quot;: &amp;quot;Admin&amp;quot;, &amp;quot;expire&amp;quot;: &amp;quot;2017-02-08 12:45:43&amp;quot; }   Note: 默认情况下JWT是未加密的，任何人都可以解读其内容，因此不要构建隐私信息字段，存放保密信息，以防止信息泄露。</description>
    </item>
    
    <item>
      <title>写在2015年年末</title>
      <link>https://morven.life/posts/post_at_the_end_of_2015/</link>
      <pubDate>Tue, 29 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/post_at_the_end_of_2015/</guid>
      <description> 2015年马上就要过去了。一年一年是如此相似，一年一年却有如此不同。每到年底总想写点什么，算是给自己的一个总结、一个回忆、一个自我述说的快乐。 2015年像很多年份一样是平常的，按部就班地数着日子过去了，虽然忙碌，但收获颇多。2015年又是不平常的，因为2015年是我正式告别学校踏入&amp;rdquo;江湖&amp;rdquo;的第一年。2015年对我来说，关键字有驾证，毕业以及入职。
驾证 之前总是一拖再拖，这次真是没法再推了。于是拉了宿舍一哥们儿入坑，组团报名。银子花出去之后便有了动力。从科目一理论到科目二场地考，再到科目三大路考，每周一到周四5点多爬起来赶班车去驾校练车。中午赶回来吃过饭休息之后再去实验室赶论文。还好，全部都一把过，算是对自己的慰藉吧。那几个月下来，认识了很多学车的同学，大多是即将毕业离开南京的学生，而且还认识了个南艺学珠宝设计的妹子（坏笑&amp;hellip;）。那段日子也确实是痛并快乐着，导致拿到驾照之后得了学车后遗症，每天早上天还没亮就睡不着了。学车本不是什么难事儿，只要安排得到，拿到驾照还是挺容易的，也不用为了考试顺利而铤而走险，贿赂教练或考官。
毕业 又毕业了，这一次是真的毕业了。研究生毕业答辩明显比本科毕业答辩阶段严格得多，所以投入了更多的时间在论文上。庆幸自己有个好导师，选题也不错，所以答辩比较顺利。毕业典礼没有本科那么正式，辅导员只是选了一些同学去参加，毕业合影部分同学的脸也是p上去的。临近毕业那段时间各种手续要办，又是档案又是户口&amp;hellip;不过总能抽出时间和室友开黑。直到离校当晚，拉着行李箱准备走出鼓楼校区，走在广州路熙熙攘攘的人群中才知道，这次不是回家，这次真要离开了，下次再来不知道是什么时候了。
入职 毕业后回家休息不到一个月正式入职。入职前一周收到HR发给我的入职材料列表，发现缺了‘无犯罪记录’证明，当时懵了，因为这材料需要回学校保卫科去办。还好，联系到了在南京的同学和辅导员，虽然麻烦，但还是搞定了。接下来就是正式入职，因为之前也有在IBM 上海Office实习的经历，对IBM的总体情况和公司文化比较了解，所以和同事融入较快。但是计划赶不上变化，我的hiring manager所在的部门研发计划有变，导致不需要太多的dev，所以就被transfer到其他的team，不过这几个team所做的产品相关性非常高，知识技术上稍有不同。IBM西安office主要做的产品是高性能计算相关，主要的核心产品要两款，其他的产品都是作为其add-on。技术的选择上java多一些，在core方面会有C&amp;amp;C++。令我不太满意的地方是，工作中对业务的熟悉程度远大于对技术的熟练上，这可能是大商业公司的通病。
对2016的期待 又到了许愿时间了。要做也要做有梦想的咸鱼～～
 希望能去自己感兴趣的team，做自己感兴趣的事儿。 希望抽出更多的时间看书，学习以及写博客。 希望能多在工作中和外籍同事交流，锻炼自己的口语。 希望坚持体育锻炼，一周跑两次步，周末打一次球。 希望感情能够稳定下来。  </description>
    </item>
    
    <item>
      <title>Dota中的Ban&amp;Pick机制与陪审团</title>
      <link>https://morven.life/posts/ban-pick_in_dota_and_jury/</link>
      <pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/ban-pick_in_dota_and_jury/</guid>
      <description>Dota(Defense of the Ancients)最初只是由《魔兽争霸3：冰封王座》的一个RPG地图衍变而来，是一款支持多人即时对战的战略游戏。最早的Dota地图则在混乱之治时代就出现了，一位叫做Euls的玩家制作了第一张Dota地图-Roc Dota。随后，经过多个玩家进一步完善，以及IceFrog的多次修正和更新，游戏最终定格为两个阵营，玩家需要操作英雄，通过摧毁对方遗迹建筑来取得最终的胜利。这种多人在线的竞技模式后来被称为“Dota类游戏”，对之后产生的多个竞技类游戏产生了深远的影响。
Ban&amp;amp;Pick机制 如何在比赛中选出一套让比赛的双方都能接受的英雄阵容，同时又要体现出竞技游戏的对抗性和平衡性。一般来说，游戏的对抗性表现为游戏元素的多元化。Dota这款游戏共有102名英雄，有两个阵营各选五名英雄分辨占据近卫和天灾两方，分三路进行对抗。当然五名英雄的职责各不相同，Carry位通常在地图的优势路发育，同时要有清晰的大局观，在比赛的后期发挥主力作用。Solo位，一般在中路对抗，该位置要求选手在中前期有很强的带节奏能力。Ganker，也称为抗压位，游走于各路，配合队友完成击杀对方英雄。剩余的两个位置称为辅助位，主要配合团队控制视野，帮助队友打出优势，从而在战略上压制对方。
假定Dota的英雄完全按照其所处的位置排列，（实战中完全不这样，经常原本12号位的英雄随着版本更迭称为辅助，反之亦然）如此每个位置的英雄也就约20名，如果我们再放弃掉一些不常用的英雄的话，实际让选手能够顺畅选择的英雄其实不过尔尔。这时，如何让场上选手得以公平竞技打出激情，就必须通过一套规则，选出一套让比赛双方均能够接受的阵容，从而增加比赛的观赏性。那么，选手们应当如何选取自己心仪的英雄呢？
Dota的设计者“IceFrog”最终采用了一套&amp;rdquo;Ban&amp;amp;Pick&amp;rdquo;机制，这套机制来源于千年之前一个在法国的英国人的发明。
陪审制 1135年，英国国王亨利一世去世，根据一份协定，英国国王的继承者将由自幼生长在法国的青年亨利二世于1154年担任。亨利二世是一位非常有野心的国王，开创了一个时代，人称“金雀花王朝”。可刚上任的他作为一个“外国人”，只要想在一个地区内树立权威，那么司法权则应当是极为重要的手段之一。在当时，高贵的法国贵族又如何听得懂英格兰的那些土话呢？如果听不懂别人说什么，作为一国之君，又如何来审理民间纠纷以确立自己的声望？亨利二世又能以怎样的方式，设计出一套完美的制度呢？
最开始的时候，每当出现土地方面的诉讼时，会由法院发给争议双方一纸令状(令状或由司法大臣发布)，命令争议双方找来十二名和本案无关的人士在上帝面前发誓之后，做出对案件事实方面的裁决。而亨利二世则带人组成巡回法院，当他巡回到该地之后，对案件做出裁决。毕竟，由自己亲手提拔的大臣是懂法语的，自己审完案子下发，再由手下将其翻译成英语传达，是再好不过的事了。
但是，那十二个与本案无关的人，是怎么选出来的？
陪审员的选择一开始是随机选，而且选的比较多。然后双方会有机会了解备选的人选，并且通过辩论来决定留下谁。当然，双方都会尽量留下对自己有利的陪审员（Pick），主要是根据职业，性别，年龄来判断是否会同情被告。还有几个机会可以不必给出理由而否定某一个陪审员（Ban）。最后确定一个名单。名单上的人就得去了……因为这个是公民义务的一部分，除非有特殊原因，否则必须去，要是故意不去那就要被起诉。同时，还会有几个备用人选，也必须去，以防有人生病或者出事来不了。 还有一个是选择陪审员的时候会尽量避免选择某些职业，比如律师，法学院的学生，老师，教授等等。主要是排除类似“权威”或者习惯教育指导别人的人，以防以一人之力左右整个陪审团的意见。
对Dota来说，情况也正是如此，“IceFrog”必须采取一套制度，让选手们首选去掉那些对自己特别不利的英雄，从而不会让先选英雄的一方率先抢走版本当中最热门也是最厉害的英雄，以降低游戏的对抗程度。说白了，对Dota来说，选手们竞技环境越是公平，游戏的对抗程度越高，游戏的活力也就越持久。
不过这套制度并非一经制定就一成不变，而是会根据玩家不断的反馈与游戏平衡的调整随之改进。Dota的Ban&amp;amp;Pick规则方式的更迭几乎和Dota的游戏版本更迭频率一致。从Dota6.28X版本开始到现在的Dota6.83，每一代版本非但在游戏本身的平衡性上做出了重大调整，在Ban&amp;amp;Pick规则上，&amp;rdquo;IceFrog&amp;rdquo;也从未掉以轻心。不论是Ban&amp;amp;Pick时间还是顺序，甚至于到底Ban掉多少名英雄，也是历经多个版本才最终确立。</description>
    </item>
    
    <item>
      <title>SSH协议以及端口转发</title>
      <link>https://morven.life/notes/the_knowledge_of_ssh/</link>
      <pubDate>Thu, 19 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_knowledge_of_ssh/</guid>
      <description>SSH估计是每台Linux机器的标配了。日程工作中程序员在本机写完Code之后，很少在本机上直接部署测试，经常需要通过SSH协议登录到实验室的Linux主机上验证。实际上SSH具备多种功能，不仅仅是远程登录这么简单，今天我们就详细探讨一下SSH协议以及它的高级功能-端口转发。
SSH的原理 SSH是一种网络协议，用于计算机之间的加密登录，也就是说这种登陆是安全的。SSH协议之所以安全，是因为它基于非对称加密。基本的过程可以描述为：
 客户端通过SSH user@remote-host发起登录远程主机的请求 远程主机收到请求之后，将自己的公钥发给客户端 客户端使用这个公钥加密登录密码之后发给远程主机 远程主机使用自己的私钥，解密登陆请求，获得登录密码，如果正确，则建立SSH通道，之后所有客户端和远程主机的数据传输都要加密发送  看似很完美是吗？其实有个问题，如果有人中途拦截了登录请求，将自己伪造的公钥发送给客户端，客户端其实并不能识别这个公钥的可靠性，因为SSH协议并不像HTTPS协议那样，公钥是包含在证书中心CA来颁发的证书之中。所以有攻击者（中间人攻击-Man-in-the-middle-attack）拦截客户端到远程主机的登陆请求，伪造公钥来获取远程主机的登录密码，SSH协议的安全机制就荡然无存了。
说实话，SSH协议本身确实无法阻止这种攻击形式，最终还是依靠终端用户自身来识别并规避风险。
比如，我们在第一次登录某一台远程主机的时候，会得到如下提示：
$ ssh user@remote-host The authenticity of host &#39;10.30.110.230 (10.30.110.230)&#39; can&#39;t be established. ECDSA key fingerprint is SHA256:RIXlybA1rgf4mbnWvLuOMAxGRQQFgfnM2+YbYLT7aQA. Are you sure you want to continue connecting (yes/no)?  这个提示的意思是说无法确定远程主机的真实性，只能得到它的指纹(fingerprint)，需要你确认是否信任这个返回的公钥。这里所说的&amp;rdquo;指纹&amp;rdquo;是RSA公钥的MD5哈希结果。我们知道为了保证RSA私钥的安全性，一般RSA公钥设置基本都不小于1024位，很难直接让终端用户去确认完整的RSA公钥，于是通过MD5哈希函数将之转化为128位的指纹，就很容易识别了。实际上，有很多网络应用程序都是用RSA公钥指纹来让终端用户识别公钥的可靠性。
具体怎么决定是依赖于终端用户了，所以推荐的做法是将远程主机的公钥保存在合适的地方方便核对。这是，如果用户决定接受这个返回的公钥，系统会继续提示：
Warning: Permanently added &#39;10.30.110.230&#39; (RSA) to the list of known hosts.  紧接着就是用户输入密码来登录远程主机。同时远程主机的公钥会被保存在$HOME/.ssh/known_hosts这个文件中，下次登录的时候就不需要用户再次核对指纹了。每个SSH用户都有自己的known_hosts文件，此外系统也有一个这样的文件，通常是/etc/ssh/ssh_known_hosts，保存一些对所有用户都可信赖的远程主机的公钥。
除了使用密码登录，SSH协议还支持公钥登录。这里的公钥不再是远程主机的公钥，而是客户端的RSA公钥。原理很简单：
 用户将客户端的RSA公钥保存在远程主机上 客户端向远程主机发起登录请求 远程主机会向客户端发送一段随机字符串 客户端用自己的RSA私钥加密后再发送给远程主机 远程主机使用保存的的RSA公钥进行解密，如果解密成功则代表客户端是可信的，不需要密码就能确认身份  在客户端生成RSA公私钥对，一般使用ssh-keygen。举例来说，我们要生成2048位RSA密钥对：
$ ssh-keygen -b 2048 -t rsa -f foo_rsa Generating public/private rsa key pair.</description>
    </item>
    
    <item>
      <title>Linux的启动流程</title>
      <link>https://morven.life/notes/the_knowledge_of_linux/</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_knowledge_of_linux/</guid>
      <description>和Window等其他操作系统一样，linux的启动也分为两个阶段：引导（boot）和启动（startup）。引导阶段开始于打开电源开关，接下来板载程序BIOS的开始POST上电自检主过程，结束于内核初始化完成。启动阶段接管剩余的工作，直到操作系统初始化完成进入可操作状态，并能够执行用户的功能性任务。
我们不花过多篇幅讲解引导阶段的硬件板载程序加载运行的过程。事实上，由于在板载程序上很多行为基本上固定的，程序员很难介入，所以接下来主要讲讲主板的引导程序如何加载内核程序以及控制权交给linux操作系统之后的各个服务的启动流程。
GRUB引导加载程序 所谓引导程序，一个用于计算机寻找操作系统内核并加载其到内存的智能程序，通常位于硬盘的第一个扇区，并由BIOS载入内存执行。为什么需要引导程序，而不是直接由BIOS加载操作系统？原因是BOIS只会自动加载硬盘的第一个扇区的512字节的内容，而操作系统的大小远远大于这个值，所以才会先加载引导程序，然后通过引导程序加载程序加载操作系统到内存中。
目前，各个Linux发行版主流的引导程序是GRUB(GRand Unified BootLoader)。GRUB的作用有以下几个： - 加载操作系统的内核 - 拥有一个可以让用户选择到底启动哪个系统的的菜单界面 - 可以调用其他的启动引导程序，来实现多系统引导
GRUB1现在已经逐步被弃用，在大多数现代发行版上它已经被GRUB2所替换。GRUB2通过/boot/grub2/grub.cfg进行配置，最终GRUB定位和加载linux内核程序到内存中，并转移控制权到内核程序。
内核程序 内核程序的相关文件位于/boot目录下，这些内核文件均带有前缀vmlinuz。内核文件都是以一种自解压的压缩格式存储以节省空间。在选定的内核加载到内存中并开始执行后，在其进行任何工作之前，内核文件首先必须从压缩格式解压自身。
# ll /boot/ total 152404 drwxr-xr-x 4 root root 4096 Nov 29 15:34 ./ drwxr-xr-x 22 root root 335 Jan 16 12:22 ../ -rw-r--r-- 1 root root 190587 Aug 10 2018 config-3.2.0-3-amd64 -rw-r--r-- 1 root root 190611 Oct 2 12:22 config-3.2.0-4-amd64 drwxr-xr-x 5 root root 4096 Nov 29 15:33 grub/ -rw-r--r-- 1 root root 39413114 Nov 29 15:33 initrd.</description>
    </item>
    
    <item>
      <title>回首「2014」</title>
      <link>https://morven.life/posts/backward_glance_of_2014/</link>
      <pubDate>Fri, 26 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/backward_glance_of_2014/</guid>
      <description> 一年将尽，一年伊始，兜兜转转，2014年就这样走近年末，不禁让人措手不及。似乎是一样的时光、一样的节奏、一样的人物，变换着不同的场景、不同的面孔、不同的心情，重重叠叠过去了一年。过去的一年，虽然忙碌，但也收获不少。如果一定要说一些年度关键词，那就是学分与课程、百度实习以及校园招聘。
学分与课程 从去年的9月份到今年的五月初，忙碌的两个学期，选择修完了研究生的大部分课程，这样五月份以后就可以外出实习。总的来看，研究生的课程是本科课程的扩展与深入。比如《高级数据库》这门课，本科也开，所讲内容大部分是数据库基本知识，关系型数据库设计以及SQL语句优化。研究生阶段则更深入，更倾向于数据库读写性能分析，各种关系型数据库横向对比，动手实践特殊类型的数据库设计，当然也包括对非关系型数据库的探讨。其他课程与此类似，包括高级网络，分布式设计与分析，数据挖掘与知识发现等等，基本上覆盖了计算机相关专业的主要课程。也利用研一空闲时间，复习了数据结构和算法，刷LeetCode，填本科阶段挖下的坑。到五月份主要课程都已修完，貌似还多修了两门课。研究生的成绩普遍较高，平均分达到了90分左右（现在才意识到了要好好学习，哈哈～）。研究生阶段还有一门研讨课，不过我选的研讨课自己不是特别感兴趣，所以也没有花太多的时间在上面，水水而已。
百度实习 关于今年的实习，本来想专门写一篇文章总结一下，后来由于找工作忙碌就搁置了（其实都是借口），这里就简单总结一下。今年5月份拿到百度网页搜索部的实习Offer，之前听说百度实习基本能留下正式工作，但是当时选择去百度实习的原因不是想拿百度的正式Offer，而是想去体验一下国内顶级的互联网公司，同时也增加自己的项目经验，为下半年的找工作打下基础。 先说说面试吧，百度的实习面试分为两轮技术面，HR都会跟你预约面试时间，这一点还是非常赞的。第一轮面试的大部分是基础数据结构与算法，当然也问了一些关于C++的问题，比较轻松。第二轮面试就比较开放了，当时我的面试官就是后来我实习的mentor，问题不是特别多，第一个是操作系统内存管理的，接下来，问了我对于Mysql的认识，包括mysql读写极限的多少，以及底层一些读写机制等等，最后就是一道算法题，自己回答的不是很好。不过还好，大约一周以后，就收到了Offer。 5月初正式入职，在百度网页搜索部，我所在的team在上海Office有4个DEV，主要负责抓取工作，包括百度搜索的抓取以及其他各个产品线的抓取。百度为了规范公司各个产品线的抓取，避免不必要的封禁和抓取混乱的现状，特意将抓取平台化，命名为CSPUB，公司内部各个产品线可以在CSPUB上注册，编辑抓取目标然后发起抓取。主要的开发语言是C++和PHP，自己平时也用Python写一些脚本检测线上机器的运行状况。百度确实是个年轻有活力的公司，内部员工干活都很积极，对新技术热情很高，部门内部经常会有一些技术分享，可以学到不少在学校根本不会接触到的架构知识。我的mentor是个对工作效率追求极致的人，一度使我感觉跟不上节奏，后来习惯了之后才发现自己受益颇多。到了8月份，由于开始找工作，自己也没有留上海的想法，遂辞职，返校找工作。
校园招聘 今年最重要的事情应该就是找工作了，可就是这最重要的事儿，我却刚开始就错过了一次绝佳的机会－阿里校招。不过阿里今年的校招也是让人不能理解，9月份之前就开始笔试，而且是线上笔试。我笔试当天晚上才得知消息，为时已晚。不过我对于自己在哪里工作找已有想法，因为女朋友的缘故，我会优先选择回西安工作。所以，当周围的同学都在准备BAT面试的时候，我在关注西安为数不多的IT公司校招情况。机缘巧合，之前发给IBM西安office HR的简历被筛选了，HR安排我去公司笔试和面试。接下来的一周我在西安参加了笔试一次面试三次，虽然HR说十一之后才会有消息，但当时感觉应该十拿九稳了。不出意料，回到南京之后，面试Manager通过电话给了口头offer，薪水比想象的要高，再加上之前有在IBM的实习经历，应聘职位也和自己的方向比较相关，再次考虑到在西安IT行业不景气的现状，所以也基本定下来了。当然，这时候还没到十月份，校招才刚开始，所以也准备了其他公司的笔试面试，包括百度，大众点评以及一些小公司。其中百度和大众点评都拿到Offer。在西安呆的一周内错过了腾讯，美团的校招。进入11月份基本周围同学都拿到Offer，校招也应该结束了。 总的来说，校招没必要很纠结，没必要各个公司招聘都参加，那要只会心力交瘁。想清楚自己适合什么样的公司，决定去哪个城市发展，这样自己目标就明确多了，准备起来也有条不紊，自然结果也不会很差。
新一年的愿望 希望将愿望写出来，能够更加有动力去实现。
 希望抓住最后的校园时光，感谢所有的老师，和最好的哥们儿好好告别。 希望公司入职能去好的team，找到nice的mentor，做自己想做的事情。 希望能抽出更多的时间看书，不管是技术方面的书籍还是人文历史方面的。 希望能学点设计方面的知识，尤其是UI，UX方面的。 希望在工作中锻炼自己的口语，再不要吃老本。 希望多多参与开源的项目，保持博客更新的频率。 希望能多陪陪家人出去旅游&amp;hellip;  </description>
    </item>
    
    <item>
      <title>创建私有CA以及颁发SSL证书</title>
      <link>https://morven.life/notes/create_pki_with_openssl/</link>
      <pubDate>Mon, 24 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/create_pki_with_openssl/</guid>
      <description>在上一篇文章深入理解PKI系统的与数字证书中介绍了PKI系统的基本组成以及CA认证中心的主要作用，以及X.509证书基本标准。今天，我们继续应用已经学习的理论知识构建一套自己的PKI/CA数字证书信任体系。
SSL证书生成工具 有以下两种常见的工具来生成RSA公私密钥对:
 Note: 有些情形只需要公私密钥对就够了，不需要数字证书，比如私有的SSH服务。但是对于一些要求身份认证的情形，则需要对公钥进行数字签名形成数字证书。
  ssh-keygen openssl genrsa  实际上ssh-keygen底层也是使用OpenSSL提供的库来生成密钥。
ssh-keygen 举例来说，我们要生成2048位RSA密钥对：
$ ssh-keygen -b 2048 -t rsa -f foo_rsa Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in foo_rsa. Your public key has been saved in foo_rsa.pub. The key fingerprint is: b8:c4:5f:2a:94:fd:b9:56:9d:5b:fd:96:02:5a:7e:b7 user@oasis The key&#39;s randomart image is: +--[ RSA 2048]----+ | | | | | | | .</description>
    </item>
    
    <item>
      <title>深入理解PKI系统的与CA中心</title>
      <link>https://morven.life/notes/the_pki_and_digital_certificate/</link>
      <pubDate>Wed, 12 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_pki_and_digital_certificate/</guid>
      <description>在上一篇文章数字签名与数字证书中介绍了数字签名以及数字证书的一些基础知识以及数字证书的作用，但是并没有提到数字证书的管理，比如数字证书的申请，数字证书的文件格式等知识。这篇文章将继续探讨数字证书的管理知识。
说到数字证书的管理，不得不提到一个词，PKI(Publick Key Infrastructure)，即公钥基础设施，是一种遵循既定标准的密钥管理平台，它能够为所有网络应用提供加密和数字签名等密码服务及所必需的密钥和证书管理体系。简单来说，可以理解为利用之前提到过的供私钥非对称加密技术为应用提供加密和数字签名等密码服务以及与之相关的密钥和证书管理体系。
 PKI既不是一个协议，也不是一个软件，它是一个标准，在这个标准之下发展出的为了实现安全基础服务目的的技术统称为PKI
 PKI的组成 PKI作为一个实施标准，有一系列的组件组成：
 CA(Certificate Authority)中心(证书签发)：是PKI的”核心”，即数字证书的申请及签发机关，CA必须具备权威性的特征，它负责管理PKI结构下的所有用户(包括各种应用程序)的证书，把用户的公钥和用户的其他信息捆绑在一起，在网上验证用户的身份，CA还要负责用户证书的黑名单登记和黑名单发布。
 X.500目录服务器(证书保存)：X.500目录服务器用于&amp;rdquo;发布&amp;rdquo;用户的证书和黑名单信息，用户可通过标准的LDAP协议查询自己或其他人的证书和下载黑名单信息。
 基于SSL(Secure socket layer)的安全web服务器：Secure Socket Layer(SSL)协议最初由Netscape企业发展，现已成为网络用来鉴别网站和网页浏览者身份，以及在浏览器使用者及网页服务器之间进行加密通讯的全球化标准。
 Web(安全通信平台)：Web有Web Client端和Web Server端两部分，分别安装在客户端和服务器端，通过具有高强度密码算法的SSL协议保证客户端和服务器端数据的机密性、完整性、身份验证。
 自开发安全应用系统：自开发安全应用系统是指各行业自开发的各种具体应用系统，例如银行、证券的应用系统等。完整的PKI包括:
1) 认证政策的制定 2) 认证规则 3) 运作制度的制定 4) 所涉及的各方法律关系内容 5) 技术的实现等
  CA 认证中心CA(Certificate Authority)，是一个负责发放和管理数字证书的权威机构，它作为电子商务交易中受信任的第三方，承担公钥体系中公钥的合法性检验的责任。CA为每个使用公开密钥的用户发放一个数字证书，以实现公钥的分发并证明其合法性。作为PKI的核心部分，CA实现了PKI中一些很重要的功能:
 接收验证最终用户数字证书的申请 确定是否接受最终用户数字证书的申请-证书的审批 向申请者颁发、拒绝颁发数字证书-证书的发放 接收、处理最终用户的数字证书更新请求-证书的更新 接收最终用户数字证书的查询、撤销 产生和发布证书废止列表(CRL) 数字证书的归档 密钥归档 历史数据归档  X.509标准 整个PKI体系中有很多格式标准，PKI的标准规定了PKI的设计、实施和运营，规定了PKI各种角色的&amp;rdquo;游戏规则&amp;rdquo;。如果两个PKI应用程序之间要想进行交互，只有相互理解对方的数据含义，交互才能正常进行，标准的作用就是提供了数据语法和语义的共同约定。PKI中最重要的标准，它定义了公钥证书的基本结构。
X.509是定义了公钥证书结构的基本标准，是目前非常通用的证书格式。X.509标准在PKI中起到了举足轻重的作用，PKI由小变大，由原来网络封闭环境到分布式开放环境，X.509起了很大作用，可以说X.509标准是PKI的雏形。PKI是在X.509标准基础上发展起来的。理论上只要为一个网络应用程序创建的证书符合ITU-T X.509国际标准，就可以用于任何其他符合X.509标准的网络应用。对于符合X.509标准的数字证书，必须保证公钥及其所有者(Subject)的姓名是一致的，同时，认证者(Issuer)总是CA或由CA指定的人。X.509数字证书是一些标准字段的集合，这些字段包含有关用户或设备及其相应公钥的信息。X.509标准定义了证书中应该包含哪些信息，并描述了这些信息是如何编码的(即数据格式)，所有的X.509证书包含以下数据：
 版本号(Version)：指出该证书使用了哪种版本的X.509标准（版本1、版本2或是版本3），版本号会影响证书中的一些特定信息，目前的版本为3 序列号(Serial Number)： 标识证书的唯一整数，由证书颁发者分配的本证书的唯一标识符 签名算法标识符： 用于签证书的算法标识，由对象标识符加上相关的参数组成，用于说明本证书所用的数字签名算法。例如，SHA-1和RSA的对象标识符就用来说明该数字签名是利用RSA对SHA-1杂凑加密 认证机构的数字签名：这是使用证书发布者私钥生成的签名，以确保这个证书在发放之后没有被撰改过 认证机构： 证书颁发者的可识别名（DN），是签发该证书的实体唯一的CA的X.500名字。使用该证书意味着信任签发证书的实体。(注意：在某些情况下，比如根或顶级CA证书，发布者自己签发证书) 有效期限(Validity)： 证书起始日期和时间以及终止日期和时间；指明证书在这两个时间内有效 主题信息(Subject)：证书持有人唯一的标识符(或称DN-distinguished name)这个名字在 Internet上应该是唯一的 公钥信息(Public-Key)： 包括证书持有人的公钥、算法(指明密钥属于哪种密码系统)的标识符和其他相关的密钥参数 颁发者唯一标识符(Issuer)：标识符—证书颁发者的唯一标识符，仅在版本2和版本3中有要求，属于可选项  除了以上字段之外，X.</description>
    </item>
    
    <item>
      <title>数字签名与数字证书</title>
      <link>https://morven.life/notes/the_digital_signature_and_digital_certificate/</link>
      <pubDate>Mon, 06 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_digital_signature_and_digital_certificate/</guid>
      <description>在之前的密码学笔记主要是介绍密码学的基础知识，包括两种加密算法的原理，引入在非对称加密算法中数字证书（Digital Certificate）的概念。这篇笔记将继续探讨什么是数字证书，不过在了解它之前，先得知道什么是数字签名（Digital Signature）。
关于数字签名和数字证书的概念，有一篇非常经典的文章，本文的大部分内容来自于那篇文章，喜欢读英文原版的请移步至：http://www.youdzone.com/signature.html
Bob生成了自己的（公钥，私钥）对，将私钥自己保存，并将公钥分发给了他的朋友们：Pat，Susan，Daug
Susan要给Bob写一封保密的信件，写完后用Bob的公钥加密，就可以达到保密的效果。Bob收到信件之后用自己的私钥来解密，就可以看到信件的内容。这里假设Bob的私钥没有泄露（私钥是十分敏感的信息，一定要注意保管，泄露私钥那么文章里很多假设都不成立），即使信件被别人截获，信件内容也无法解密，也就是说这封信的内容不会有第三个人知道。
Bob给Susan回信，决定采用&amp;rdquo;数字签名&amp;rdquo;：他写完后先用Hash函数，生成信件的摘要（Digest），然后，在使用自己的私钥，对这个摘要进行加密，生成&amp;rdquo;数字签名&amp;rdquo;（Digital Signature）。
Bob将这个数字签名，附在信件里面，一起发给Susan。
Susan收到信件之后，对信件本身使用相同的Hash函数，得到当前信件内容的摘要，同时，取下数字签名，用Bob的公钥解密，得到原始信件的摘要，如果两者相同就说明信件的内容没有被修改过。由此证明，这封信确实是Bob发出的。
但是，更复杂的情况出现了。Daug想欺骗Susan，他伪装成Bob制作了一对（公钥，私钥）对，并将公钥分发给Susan，Susan此时实际保存的是Daug的公钥，但是还以为这是Bob的公钥。因此，Daug就可以冒充Bob，用自己的私钥做成&amp;rdquo;数字签名&amp;rdquo;写信给Susan，而Susan用假的鲍勃公钥进行解密。一切看起来完美无缺？
Susan觉得有些不对劲，因为她并不确定这个公钥是不是真正属于Bob的。于是她想到了一个办法，要求Bob去找&amp;rdquo;证书中心&amp;rdquo;（Certificate Authority，简称CA），为公钥做认证。证书中心用证书中心的私钥，对Bob的公钥和一些相关信息一起加密，生成&amp;rdquo;数字证书&amp;rdquo;（Digital Certificate）。
一旦Bob拿到数字证书以后，就可以放心写信给任何人了。只需要在信件内容的后面附上数字签名的同时，再附上数字证书就行了。
Susan收到信件之后，首先使用用CA的公钥（一般都是公开的）解开数字证书，就可以拿到Bob真实可信的公钥了，然后就能用此公钥解密数字签名进一步验证信件内容是否被篡改过。
由此可见，&amp;rdquo;数字证书&amp;rdquo;就是解决身份认证的问题，就如同现实中我们每一个人都要拥有一张证明个人身份的身份证或驾驶执照一样，以表明我们的身份或某种资格。数字证书是由权威公正的第三方机构即Certificate Authority(CA)中心签发的，确保信息的机密性和防抵赖性。对于一个大型的应用环境，认证中心往往采用一种多层次的分级结构，各级的认证中心类似于各级行政机关，上级认证中心负责签发和管理下级认证中心的证书，最下一级的认证中心直接面向最终用户。</description>
    </item>
    
    <item>
      <title>密码学基础</title>
      <link>https://morven.life/notes/the_basic_of_cryptology/</link>
      <pubDate>Tue, 16 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_basic_of_cryptology/</guid>
      <description>密码学（Cryptography） 密码学是研究编制密码和破译密码的技术科学。研究密码变化的客观规律，应用于编制密码以保守通信秘密的，称为编码学；应用于破译密码以获取通信情报的，称为破译学，总称密码学。 密码是通信双方按约定的法则进行信息特殊变换的一种重要保密手段。依照这些法则，变明文为密文，称为加密变换；变密文为明文，称为脱密变换。密码在早期仅对文字或数码进行加、脱密变换，随着通信技术的发展，对语音、图像、数据等都可实施加、脱密变换。
密码算法 什么是密码算法（Cryptography Algorithm），通常是指加、解密过程所使用的信息变换规则，是用于信息加密和解密的数学函数。对明文进行加密时所采用的规则称作加密算法，而对密文进行解密时所采用的规则称作解密算法。加密算法和解密算法的操作通常都是在一组密钥的控制下进行的。
什么是密钥？密钥（Secret Key）是密码算法中的一个可变参数，通常是一组满足一定条件的随机序列。用于加密算法的叫做加密密钥，用于解密算法的叫做解密密钥，加密密钥和解密密钥可能相同，也可能不相同。
加密算法根据根据密钥的不同分为两类，对称加密算法(Symmetric-key Algorithm)和非对称加密算法(Asymmetric Key Encryption Algorithm)。
对称加密 首先，让我们先从一个情景开始讲起。
比如张三学习比李四好，李四就想在考试的时候让张三“帮助”一下自己，当然，他们俩不可能像我们平常对话一样说，第一题选A，第二题选B等等，为什么？因为监考老师明白他俩在谈论什么，也就是说这种沟通交流方式属于“明文”，所以李四就想：“我需要发明一种，只有我和张三明白的交流方式”，那李四做了什么呢？恩，李四去找张三说：“当我连续咳嗽三声的时候你看我，然后如果我摸了下左耳朵，说明你可以开始给我传答案了，如果没反应，那说明我真的是在咳嗽&amp;hellip;”， 然后，怎么传答案呢？很简单，“你摸左耳朵代表A, 摸右耳朵代表B，左手放下代表C，右手放下代表D”，好了，这就是他们的“算法(规则)”，将信息的一种形式(A,B,C,D)，这里我们称为“明文”，转换成了另一种形式(摸左耳朵，摸右耳朵，放左手，放右手)，这里称为“密文”，经过这种转换，很显然监考老师不会明白这些“密文”，这样，张三和李四就通过“密文”的形式实现了信息的交换。
对称加密算法也叫单钥加密（Private Key Cryptography），加密和解密过程都用同一套密钥。历史上，人类传统的加密方法都是前一种，比如二战期间德军用的Enigma电报密码，莫尔斯电码也可以看作是一种私钥加密方法。
结合前面的例子对应一下，密钥就是“将(A,B,C,D)转换成(摸左耳朵，摸右耳朵，放左手，放右手)”这么一个规则。
 实务上，这组密钥成为在两个或多个成员间的共同秘密，以便维持专属的通讯联系。
 这句话很好理解了吧，密钥是张三和李四间共同的秘密！只有他俩事先知道。 所以，为什么叫对称加密呢，你可以这么理解，一方通过密钥将信息加密后，把密文传给另一方，另一方通过这个相同的密钥将密文解密，转换成可以理解的明文。他们之间的关系如下：
明文 &amp;lt;-&amp;gt; 密钥 &amp;lt;-&amp;gt; 密文  目前常见的对称加密算法有：
DES、3DES、AES、Blowfish、IDEA、RC5、RC6。  非对称加密 非对称加密算法也称为双钥加密（Public Key Cryptography），加密和解密过程用的是两套密钥。非对称加密是一种比对称加密更加优秀的加密算法。对称加密的密钥只有一把，所以密钥的保存变得很重要。一旦密钥泄漏，密码也就被破解。 在非对称加密的情况下，密钥有两把，一把是公开的公钥，还有一把是不公开的私钥。 对称加密的原理如下：
 公钥和私钥是一一对应的关系，有一把公钥就必然有一把与之对应的、独一无二的私钥，反之亦成立。 所有的（公钥, 私钥）对都是不同的。 用公钥可以解开私钥加密的信息，反之亦成立。 同时生成公钥和私钥应该相对比较容易，但是从公钥推算出私钥，应该是很困难或者是不可能的。  在对称加密体系中，公钥用来加密信息，私钥用来数字签名。 比如，李四想给张三发送密文。于是李四开始给张三发消息：
李四： “hi哥们，我想给你发个密文，把你的公钥给我发过来。” 张三： “没问题的，这是我的公钥： d#8yHE8eU#hb*!neb，用这个公钥加密你的信息后给我发过来吧” 李四： “这是我想对你说的话： *&amp;amp;#@uehuu(**#eehu&amp;amp;$##bfeu&amp;amp;&amp;amp;”  为什么公开问公钥？非对称解密算法的强大之处就在这里！公钥可以随意分发，所以即使第三方截取了，也只是知道该公钥而已，但是要是想解密使用该公钥加密的密文！只有一个人可以办得到！就是张三！ 为什么？李四使用张三的公钥加密的信息，只有张三的公钥所对应的私钥，这里就是“张三私钥”，该私钥才可以解密！所以，没有张三私钥的第三方即时截取了这些密文，也破解不了！或者更严格的说在有限时间内比如说几千年内是暴力破解不出的！
非对称加密算法，首先要有一对key，一个被称为私钥（Private Key），一个成为公钥（Public Key），然后可以把公钥分发给想给你传密文的用户，然后用户使用该公钥加密过得密文，只有使用私钥才能解密，也就是说，只要保存好你的私钥，就能确保别人想给你发的密文不被破解。正因为，这种加密是单向的，所以被称为非对称加密算法。
这种加密算法应用非常广泛，SSH, HTTPS, TLS，电子证书，电子签名，电子身份证等等。
因为任何人都可以生成自己的（公钥，私钥）对，所以为了防止有人散布伪造的公钥骗取信任，就需要一个可靠的第三方机构来生成经过认证的（公钥，私钥）对。这就是数字证书的作用了，接下来的文章将会继续探讨什么是数字签名以及数字证书。</description>
    </item>
    
    <item>
      <title>字符编码的前世今生</title>
      <link>https://morven.life/notes/the_character_encoding/</link>
      <pubDate>Sun, 12 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/notes/the_character_encoding/</guid>
      <description>字符编码问题看似无关紧要，常常被忽略，但是对字符编码知识没有一个完整系统的认识，实际编码过程中会让我们吃尽苦头。今天，我们就来看一看字符编码的前世今生。
ASCII-一切的起源 字符编码主要是解决如何使用计算机的方式来表达特定的字符，但是有计算机基础理论知识的人都知道，计算机内部所有的数据都是基于二进制，每个二进制位（bit）有0和1两种状态，我们可以组合多个二进位来表达更大的数值，例如八个二进制位就可以组合出256种状态，这被称为一个字节（byte）。这就是说，我们可以一个字节来映射用来表示256种不同的状态，每一个状态对应一个符号，就是256个符号，从00000000到11111111，这样就建立了最初的计算机数值到自然语言字符最基本的映射关系。上个世纪60年代，美国国家标准协会ANSI制定了一个标准，规定了常用字符的集合以及每个字符对应的编号，这就是ASCII字符集（Character Set），也称ASCII码。ASCII码规定了英语字符与二进制位之间的对应关系。
ASCII码一共规定了128个字符的编码（包括32个不能打印出来的控制符号），比如空格SPACE是32（二进制表示为00100000），大写的字母A是65（二进制表示为01000001）。这128个符号只需要占用了一个字节的后面7位，最前面的一位统一规定为0。
按照ASCII字符集编码和解码就是简单的查表过程。例如将字符序列编码为二进制流写入存储设备，只需要在ASCII字符集中依次找到字符对应的字节，然后直接将该字节写入存储设备即可，解码二进制流就是相反的过程。
各种OEM编码的衍生 当计算机发展起来的时候，人们逐渐发现，ASCII字符集里的128个字符不能满足他们的需求。在英语国家，128个字符编码足矣，但是对于非英语国家，人们无法在ASCII字符集中找到他们的基本字符。比如，在法语中，字母上方有注音符号，它就无法用ASCII码表示。于是有些人就在想，ASCII字符只是使用了一个字节的前128个变换，后面的128位完全可以利用起来，于是一些一些欧洲国家就决定，利用字节中闲置的最高位编入新的符号。比如，法语中的é的编码为130（二进制10000010）。这样一来，这些欧洲国家使用的编码体系，可以表示最多256个符号。
但是，这里又出现了新的问题。不同的国家有不同的字母，因此，哪怕它们都使用256个符号的编码方式，代表的字母却不一样。比如，130在法语编码中代表了é，在希伯来语编码中却代表了字母Gimel (ג)，在俄语编码中又会代表另一个符号。不同的OEM字符集导致人们无法跨机器传播交流各种信息。例如甲发了一封简历résumés给乙，结果乙看到的却是rגsumגs，因为é字符在甲机器上的OEM字符集中对应的字节是0×82，而在乙的机器上，由于使用的OEM字符集不同，对0×82字节解码后得到的字符却是ג。
但是尽管出现了不同的OEM编码，所有这些编码方式中，0--127表示的符号是一样的，不一样的只是128--255的这一段代表的字符。
至于亚洲国家的文字，使用的符号就更多了，汉字就多达10万左右。一个字节只能表示256种符号，肯定是不够的，就必须使用多个字节表达一个符号。比如，简体中文常见的编码方式是GB2312，使用两个字节表示一个汉字，所以理论上最多可以表示256 x 256 = 65536个符号。
 Note: 中文编码的问题很复杂，这篇笔记就深入讨论，但需要指出的是虽然都是用多个字节表示一个符号，但是GB类的汉字编码与后文的Unicode和UTF-8是毫无关系的。
 从ANSI标准到国家标准再到ISO标准 不同ASCII衍生字符集的出现，让文档交流变得非常困难，因此各种组织都陆续进行了标准化流程。例如美国ANSI组织制定了ANSI标准字符编码，ISO组织制定的各种ISO标准字符编码，还有各国也会制定一些国家标准字符集，例如中国的GBK，GB2312和GB18030。
每台计算机的操作系统都会预装这些标准的字符集还有平台专用的字符集，这样只要使用标准字符集编写文档就可以达到很高的通用性。例如用GB2312字符集编写的文档，在中国大陆内的任何机器上都能正确显示。当然，也可以在一台计算机上阅读多个不同国家语言的文档，但是前提是计算机必须得安装该文档使用的字符集。
Unocode的出现 虽然通过使用不同字符集，我们可以在一台计算机上查阅不同语言的文档，但是仍然无法解决一个问题：在一份文档中显示所有字符。那时的人们就在想，如果有一种编码，能够映射世界上所有的语言符号，每一个符号都给予一个无二义的编码，那么乱码问题就会消失。这就是Unicode字符集，就像它的名字都表示的，这是一种所有符号的编码。
Unicode字符集包括了目前人类使用的所有字符，当然是一个很大的集合，现在的规模可以容纳100多万个符号。每个符号的编码都不一样，比如，U+0639表示阿拉伯字母Ain，U+0041表示英语的大写字母A，U+4E25表示汉字严。Unicode字符集将所有字符按照使用上的频繁度划分为17个层面（Plane），每个层面上有2^16=65536个字符空间。具体的符号对应表，可以查询unicode.org，或者专门的汉字对应表。
Unicode的问题 有了Unicode之后，人们常会问一个问题？
 Unicode是需要两个字节存储吗？
 其实Unicode只是定义了一个庞大的、全球通用的字符集，并为每个字符规定了唯一确定的二进制代码，却没有规定这个二进制代码应该如何存储。
例如，汉字严的Unicode是十六进制数#4E25，转换成二进制数足足有15位（100111000100101），也就是说，这个符号的表示至少需要2个字节。表示其他更大的符号，可能需要3个字节或者4个字节，甚至更多。
这样的话就会有两个严重的问题，第一个问题是，如何才能区别Unicode和ASCII？计算机无法知道三个字节是表示一个字符，还是分别表示三个字符。第二个问题是，英文字母只用一个字节表示就够了，但是基于Unicode的规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。
它们造成的结果是：
 出现了Unicode的多种存储方式，也就是说有许多种不同的二进制格式，可以用来表示Unicode。 Unicode在很长一段时间内无法推广，直到互联网的出现。  UTF-8 随着互联网的普及，人们强烈要求出现一种统一的Unicode的编码方式。UTF-8就是在互联网上使用最广的一种Unicode的编码实现方式。其他实现方式还包括UTF-16（字符用两个字节或四个字节表示）和UTF-32（字符用四个字节表示），不过在互联网上基本不用。
由于UCS-2/UTF-16对于ASCII字符使用两个字节进行编码，存储和处理效率相对低下，并且由于ASCII字符经过UTF-16编码后得到的两个字节，高字节始终是0×00，很多C语言的函数都将此字节视为字符串末尾从而导致无法正确解析文本。因此UTF-16刚推出的时候遭到很多西方国家的抵触，大大影响了Unicode的推行。后来聪明的人们发明了UTF-8编码，解决了这个问题。
 Note：一定要记住，UTF-8是Unicode的编码实现方式之一。
 UTF-8最大的一个特点，就是它是一种“变长”的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。
UTF-8的编码规则简单实用：
 对于单字节的符号，字节的第一位设为0，后面7位为这个符号的Unicode码。因此对于英文字母，UTF-8编码和ASCII编码是完全相同的 对于n字节的符号（n &amp;gt; 1），第一个字节的前n位都设为1，第n+1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的Unicode码  下表总结了编码规则，字母x表示可用编码的位：
   Unicode符号范围 UTF-8编码方式     十六进制表示 二进制表示   0000 0000-0000 007F 0xxxxxxx   0000 0080-0000 07FF 110xxxxx 10xxxxxx   0000 0800-0000 FFFF 1110xxxx 10xxxxxx 10xxxxxx   0001 0000-0010 FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx    基于上表，解读UTF-8编码就非常简单。如果一个字节的第一位是0，则这个字节单独就是一个字符；如果第一位是1，则连续有多少个1，就表示当前字符占用多少个字节。</description>
    </item>
    
    <item>
      <title>IBM实习总结</title>
      <link>https://morven.life/posts/internship-at-ibm/</link>
      <pubDate>Mon, 01 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/internship-at-ibm/</guid>
      <description>今天上午和新来的实习生交接了自己的工作，中午约同事们一起吃了午饭，之后很快办完了离职手续，自己为期6个多月的IBM实习也画上了句号，不管是不是完美，对于我自己来说，大学四年的第一份实习无疑对我价值颇高。这篇文章我主要说说我在IBM实习的经历以及感受。
实习时间 我出去实习的时间比较晚，根据学院规定暑假就可以开实习，时间不能少于6个月，所以最好应该在4，5月份找实习，而我在4，5月份却忙于备考GRE和Toefl，自然也错过了找实习的黄金时间。暑假结束后，我才开始计划找实习。开始一心想着去互联网公司，也正因为一直在等待这样的机会而浪费了不少时间。直达9月份，刷小百合看到一个不错的实习机会，也就是接下来6个多月我所在的IBM JTC部门。当时心动的主要原因是这个实习职位所在的Team是做JVM的，也是自己的兴趣方向所在，所以果断投了简历。
面试 IBM的面试分为技术面和英文面，可能当时急缺实习生，所以面试安排得很紧凑。自己也比较幸运，因为Team里有已经毕业的学长Ray，多少会有加分：）。因为是JVM小组，所以技术面都是关于Java的，比如Java多线程，IO，容器类以及反射机制等，没有问算法题。现在回想起来，这些知识确实在每天的实习工作中都会有所接触。接下来是英文面，主要面试口语。因为是招聘实习生，所以也没有太高的要求，基本的听说读写熟练就没有问题。最后面试官问我有没有问题想问他的，我进一步问了关于实习职位的工作内容，当时的面试官，也就是后来我的mentor Sanhong，人非常nice，blabla&amp;hellip;讲了一大堆，虽然当时也不是很懂，但真心觉得很NB。过了一个周左右，收到面试通过的邮件，几天后搭乘了去上海的动车，开始了我的IBM实习。
工作环境 IBM的工作环境很不错，整个办公室是个很大的开放式的环境，整个部门，从部门老大，到Manager，到实习生都在这里工作。我去的时候领到了一台旧电脑，是的，实习生用的都是旧电脑，不多这也完全不影响开发，因为基本的开发测试都是在云端，通过SecureCRT SSH登录到云端Linux。也正是从这开始彻底喜欢上了Unix/Linux哲学： &amp;gt; simple and beautiful &amp;ndash;Wikipedia
对于习惯了IDE的我一开始会有些不适应，不过后来发现在Terminal下工作效率丝毫不逊于IDE。
工作内容 IBM比较注重基础性软件研发，特别在中国成立CDL(China Development Lab)，我所在的部门JTC(Java Technology Center)正式属于CDL，而我所在的小组从事的是JVM的开发。IBM的J9 JVM与Oracle 的Hotspot VM齐名，是两大主流的JVM之一，为IBM许多Java产品提供支持，比如WebSphere，以及一些开源的产品如Apache Harmony。现在我们team的工作是与加拿大以及印度的同事合作，基于J9VM开发Multitennancy JVM，通过在单一的多租户 JVM 中运行多个应用程序，云系统可以加快应用程序的启动时间，并减少其内存占用。这将作为IBM Java8的一个新特性。因为是实习生，所以我的工作大多是于解决Bug，性能调优以及测试相关。我的mentor Sanhong是个技术大牛，人也非常nice，我很庆幸能遇到这样的导师。mentor对我的帮助不仅是技术上的提高，更多的是工作方式的改进，这些东西在学校的绝对学习不到的。
IBM软件过程管理 特别要提到的是IBM的软件过程管理方式，IBM使用敏捷软件开发方式，更具体点儿是Scrum，每两周一次Sprint迭代，每天都会下午选个时间Daily Scrum Meeting，控制在15分钟左右，每个人都必须发言，也包括实习生，向所有成员当面汇报你昨天完成了什么，并且向所有成员承诺你今天要完成什么，同时遇到不能解决的问题也可以提出，每个人回答完成后，要走到黑板前更新自己的 Sprint burn down（Sprint燃尽图）。同时，Team会做到每天至少一个Build，即一个成功编译，可以运行的版本。虽然这些东西在学校也学过，也有实践课程可以体验，但是感觉多少还是是纸上谈兵。如今在算是真正有机会在工作机会中体会Scrum。和其他外企一样，IBM工作语言是英语，虽然平时和同事交流可以用中文，但是邮件以及Message全部都是英文，而且每周一次国际会议也是用英文交流。
总结 在IBM工作，最重要的是团队合作，虽然平时工作的压力不大，实习生见见“世面”是可以的，但如果真要在技术上有所提升，建议IBM和其他类似的外企可以不用去了，可以去百度，阿里这样的互联网公司，对于锻炼自己的技术应该帮助更大。当然，也可以选择一些创业公司，现在正值互联网蓬勃发展的时候，去小型创业公司，自己可以独当一面，项目经验提升也是必然的。当然小型公司也有自己的缺陷，缺少自己的平台，过多利用现有的技术做产品，对于想从事底层操作系统的基础架构的同学就要重新考虑了。 不过总体来说，通过这次的实习经历，我学习到了不少的东西，不只是技术上，更多是关于工作方式以及团队意识。当然，也第一次去魔都体验了码农的生活，将自己在学校学习的知识利用到了实践当中，赚了自己的第一笔钱。</description>
    </item>
    
    <item>
      <title>Big Endian &amp; Little Endian</title>
      <link>https://morven.life/posts/the_story_of_big_and_little_endian/</link>
      <pubDate>Tue, 26 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/posts/the_story_of_big_and_little_endian/</guid>
      <description>字节序 谈到字节序，必然要牵扯到两大CPU派系。那就是Motorola的PowerPC系列CPU和Intel的x86系列CPU。PowerPC系列采用big endian方式存储数据，而x86系列则采用little endian方式存储数据。那么究竟什么是big endian，什么又是little endian呢？ 其实Big Endian是指低地址存放最高有效字节（MSB），而Little Endian则是低地址存放最低有效字节（LSB）。 文字说明比较抽象，下面举个例子用图像来说：
Big Endian
 低地址 高地址 -----------------------------------------&amp;gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | 12 | 34 | 56 | 78 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+  Little Endian
 低地址 高地址 -----------------------------------------&amp;gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | 78 | 56 | 34 | 12 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+  从上面两图可以看出，采用Big Endian方式存储数据是符合我们人类的思维习惯的，而Little Endian&amp;hellip; 为什么要注意字节序的问题呢？你可能这么问。当然，如果你写的程序只在单机环境下面运行，并且不和别人的程序打交道，那么你完全可以忽略字节序的存在。但是，如果你的程序要跟别人的程序产生交互呢？在这里我想说说两种语言。C/C++语言编写的程序里数据存储顺序是跟编译平台所在的CPU相关的，而JAVA编写的程序则唯一采用big endian方式来存储数据。试想，如果你用C/C++语言在x86平台下编写的程序跟别人的JAVA程序互通时会产生什么结果？就拿上面的0x12345678来说，你的程序传递给别人的一个数据，将指向0x12345678的指针传给了JAVA程序，由于JAVA采取big endian方式存储数据，很自然的它会将你的数据翻译为0x78563412。什么？竟然变成另外一个数字了？是的，就是这种后果。因此，在你的C程序传给JAVA程序之前有必要进行字节序的转换工作。 无独有偶，所有网络协议也都是采用big endian的方式来传输数据的。所以有时我们也会把big endian方式称之为网络字节序。当两台采用不同字节序的主机通信时，在发送数据之前都必须经过字节序的转换成为网络字节序后再进行传输。ANSI C中提供了下面四个转换字节序的宏。
Big Endian：最高字节在地址最低位，最低字节在地址最高位，依次排列。 Little Endian：最低字节在最低位，最高字节在最高位，反序排列。
Endian指的是当物理上的最小单元比逻辑上的最小单元小时，逻辑到物理的单元排布关系。咱们接触到的物理单元最小都是byte，在通信领域中，这里往往是bit，不过原理也是类似的。
An Example:
如果我们将0x1234abcd写入到以0x0000开始的内存中，则结果为 big-endian little-endian 0x0000 0x12 0xcd 0x0001 0x34 0xab 0x0002 0xab 0x34 0x0003 0xcd 0x12</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://morven.life/common/about/</link>
      <pubDate>Sat, 16 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>https://morven.life/common/about/</guid>
      <description>Hi, This is Morven, graduated from Nanjing University with Bechalor and Master degree in 2015, now working as a software engineer at IBM focusing on cloud technology. I generally use morvencao as my id on the Internet. I am a strong advocate and believer of Open Source Software and love reading, thinking, programming and everything challenging and interesting.
Reach me:
 Gmail Github Twitter Medium Instagram  </description>
    </item>
    
  </channel>
</rss>