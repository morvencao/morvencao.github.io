<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hugo 0.55.6"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="author" content="Morven&#39;s Life"><meta property="og:url" content="https://morven.life/notes/networking-1-linux-virtual-device/"><link rel="canonical" href="https://morven.life/notes/networking-1-linux-virtual-device/"><link rel="shortcut icon" href="https://morven.life/favicon.ico" type="image/x-png"><script type="application/ld+json">{
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/morven.life\/"
      },
      "articleSection" : "notes",
      "name" : "Linux网络数据包的接收与发送过程",
      "headline" : "Linux网络数据包的接收与发送过程",
      "description" : "早在农历新年之前，就构思着将近半年重拾的网络基础整理成成一个系列，正好赶上武汉疫情在春节假期爆发，闲来无事，于是开始这一系列的笔记。\n我的整体思路是：\n 第一篇笔记会简单介绍Linux网络数据包接收和发送过程，但不涉及TCP\/IP协议栈的细节知识，如果有需要了解这些基础知识的读者，我推荐去阅读阮一峰老师的互联网协议的系列文章。 接下来，我会总结常用的Linux虚拟设备，同时会结合Linux自带的新网络工具包iproute2来操作这些常用的网络设备。 有了前面的基础知识，我们再来了解几种常用的容器网络实现远离。 最后，我们深入探讨一下K8s平台中主流的网络实现。  其实，严格上来说，这种学习思路其实很“急功近利”，但是，对于不太了解网络基础知识和Linux内核原理的人来说，这反而是一种很有效的学习方式。但是，私以为学习过程不只应该由浅入深，更应该螺旋向前迭代，温故而知新，才能获益良多。\n数据包的接收过程 废话不多说，我们先开始第一篇笔记的内容。\n为了简化起见，我们以一个UDP数据包在物理网卡上处理流程来介绍Linux网络数据包的接收和发送过程，我会尽量忽略一些不相关的细节。\n从网卡到内存 我们知道，每个网络设备（网卡）需要有驱动才能工作，驱动需要在内核启动时加载到内核中才能工作。事实上，从逻辑上看，驱动是负责衔接网络设备和内核网络栈的中间模块，每当网络设备接收到新的数据包时，就会触发中断，而对应的中断处理程序正是加载到内核中的驱动程序。\n下面这张图详细的展示了数据包如何从网络设备进入内存，并被处于内核中的驱动程序和网络栈处理的：\n 数据包进入物理网卡。如果目的地址不是该网络设备，且该来网络设备没有开启混杂模式，该包会被网络设备丢弃。 物理网卡将数据包通过DMA的方式写入到指定的内存地址，该地址由网卡驱动分配并初始化。 物理网卡通过硬件中断（IRQ）通知CPU，有新的数据包到达物理网卡需要处理。 CPU根据中断表，调用已经注册的中断函数，这个中断函数会调到驱动程序（NIC Driver）中相应的函数 驱动先禁用网卡的中断，表示驱动程序已经知道内存中有数据了，告诉物理网卡下次再收到数据包直接写内存就可以了，不要再通知CPU了，这样可以提高效率，避免CPU不停的被中断。 启动软中断继续处理数据包。这样的原因是硬中断处理程序执行的过程中不能被中断，所以如果它执行时间过长，会导致CPU没法响应其它硬件的中断，于是内核引入软中断，这样可以将硬中断处理函数中耗时的部分移到软中断处理函数里面来慢慢处理。  内核处理数据包 上一步中网络设备驱动程序会通过软触发内核网络模块中的软中断处理函数，内核处理数据包的流程如下图所示：\n 对于第6步中驱动发出的软中断，内核中的ksoftirqd进程会调用网络模块的相应软中断所对应的处理函数，这里其实就是调用net_rx_action函数。 接下来net_rx_action调用网卡驱动里的poll函数来一个个地处理数据包。 而poll函数会让驱动会读取网卡写到内存中的数据包。事实上，内存中数据包的格式只有驱动知道。 驱动程序将内存中的数据包转换成内核网络模块能识别的skb(socket buffer)格式，然后调用napi_gro_receive函数 napi_gro_receive会处理GRO相关的内容，也就是将可以合并的数据包进行合并，这样就只需要调用一次协议栈。然后判断是否开启了RPS，如果开启了，将会调用enqueue_to_backlog。 enqueue_to_backlog函数会将数据包放入input_pkt_queue结构体中，然后返回。 \x26gt; Note: 如果input_pkt_queue满了的话，该数据包将会被丢弃，这个queue的大小可以通过net.core.netdev_max_backlog来配置 接下来CPU会在软中断上下文中处理自己input_pkt_queue里的网络数据（调用__netif_receive_skb_core函数） 如果没开启RPS，napi_gro_receive会直接调用__netif_receive_skb_core函数。 紧接着CPU会根据是不是有AF_PACKET类型的socket（原始套接字），如果有的话，拷贝一份数据给它(tcpdump抓包就是抓的这里的包)。 将数据包交给内核协议栈处理。 当内存中的所有数据包被处理完成后（poll函数执行完成），重新启用网卡的硬中断，这样下次网卡再收到数据的时候就会通知CPU。  内核协议栈 内核网络协议栈此时接收到的数据包其实是三层(IP网络层)数据包，因此，数据包首先会进入到IP网络层层，然后进入传输层处理。\nIP网络层  ip_rcv: ip_rcv函数是IP网络层处理模块的入口函数，该函数首先判断属否需要丢弃该数据包（目的mac地址不是当前网卡，并且网卡设置了混杂模式），如果需要进一步处理就然后调用注册在netfilter中的NF_INET_PRE_ROUTING这条链上的处理函数。 NF_INET_PRE_ROUTING: netfilter放在协议栈中的钩子函数，可以通过iptables来注入一些数据包处理函数，用来修改或者丢弃数据包，如果数据包没被丢弃，将继续往下走。 \x26gt; NF_INET_PRE_ROUTING等netfilter链上的处理逻辑可以通iptables来设置，详情请移步: https:\/\/morven.life\/notes\/the_knowledge_of_iptables\/ routing: 进行路由处理，如果是目的IP不是本地IP，且没有开启ip forward功能，那么数据包将被丢弃，如果开启了ip forward功能，那将进入ip_forward函数。 ip_forward: 该函数会先调用netfilter注册的NF_INET_FORWARD链上的相关函数，如果数据包没有被丢弃，那么将继续往后调用dst_output_sk函数。 dst_output_sk: 该函数会调用IP网络层的相应函数将该数据包发送出去，这一步将会在下一章节发送数据包中详细介绍。 ip_local_deliver: 如果上面路由处理发现发现目的IP是本地IP，那么将会调用ip_local_deliver函数，该函数先调用NF_INET_LOCAL_IN链上的相关函数，如果通过，数据包将会向下发送到UDP层。  传输层  udp_rcv: 该函数是UDP处理层模块的入口函数，它首先调用__udp4_lib_lookup_skb函数，根据目的IP和端口找对应的socket，如果没有找到相应的socket，那么该数据包将会被丢弃，否则继续。 sock_queue_rcv_skb: 该函数一是负责检查这个socket的receive buffer是不是满了，如果满了的话就丢弃该数据包；二是调用sk_filter看这个包是否是满足条件的包，如果当前socket上设置了filter，且该包不满足条件的话，这个数据包也将被丢弃。 __skb_queue_tail: 该函数将数据包放入socket接收队列的末尾。 sk_data_ready: 通知socket数据包已经准备好。 调用完sk_data_ready之后，一个数据包处理完成，等待应用层程序来读取。   Note: 上面所述的所有执行过程都在软中断的上下文中执行。",
      "inLanguage" : "en-US",
      "author" : "Morven\x27s Life",
      "creator" : "Morven\x27s Life",
      "publisher": "Morven\x27s Life",
      "accountablePerson" : "Morven\x27s Life",
      "copyrightHolder" : "Morven\x27s Life",
      "copyrightYear" : "2020",
      "datePublished": "2020-01-27 00:00:00 \x2b0000 UTC",
      "dateModified" : "2020-01-27 00:00:00 \x2b0000 UTC",
      "url" : "https:\/\/morven.life\/notes\/networking-1-linux-virtual-device\/",
      "keywords" : [  ]
  }</script><title>Linux网络数据包的接收与发送过程 - Morven&#39;s Life</title><meta property="og:title" content="Linux网络数据包的接收与发送过程 - Morven&#39;s Life"><meta property="og:type" content="article"><meta name="description" content="早在农历新年之前，就构思着将近半年重拾的网络基础整理成成一个系列，正好赶上武汉疫情在春节假期爆发，闲来无事，于是开始这一系列的笔记。
我的整体思路是：
 第一篇笔记会简单介绍Linux网络数据包接收和发送过程，但不涉及TCP/IP协议栈的细节知识，如果有需要了解这些基础知识的读者，我推荐去阅读阮一峰老师的互联网协议的系列文章。 接下来，我会总结常用的Linux虚拟设备，同时会结合Linux自带的新网络工具包iproute2来操作这些常用的网络设备。 有了前面的基础知识，我们再来了解几种常用的容器网络实现远离。 最后，我们深入探讨一下K8s平台中主流的网络实现。  其实，严格上来说，这种学习思路其实很“急功近利”，但是，对于不太了解网络基础知识和Linux内核原理的人来说，这反而是一种很有效的学习方式。但是，私以为学习过程不只应该由浅入深，更应该螺旋向前迭代，温故而知新，才能获益良多。
数据包的接收过程 废话不多说，我们先开始第一篇笔记的内容。
为了简化起见，我们以一个UDP数据包在物理网卡上处理流程来介绍Linux网络数据包的接收和发送过程，我会尽量忽略一些不相关的细节。
从网卡到内存 我们知道，每个网络设备（网卡）需要有驱动才能工作，驱动需要在内核启动时加载到内核中才能工作。事实上，从逻辑上看，驱动是负责衔接网络设备和内核网络栈的中间模块，每当网络设备接收到新的数据包时，就会触发中断，而对应的中断处理程序正是加载到内核中的驱动程序。
下面这张图详细的展示了数据包如何从网络设备进入内存，并被处于内核中的驱动程序和网络栈处理的：
 数据包进入物理网卡。如果目的地址不是该网络设备，且该来网络设备没有开启混杂模式，该包会被网络设备丢弃。 物理网卡将数据包通过DMA的方式写入到指定的内存地址，该地址由网卡驱动分配并初始化。 物理网卡通过硬件中断（IRQ）通知CPU，有新的数据包到达物理网卡需要处理。 CPU根据中断表，调用已经注册的中断函数，这个中断函数会调到驱动程序（NIC Driver）中相应的函数 驱动先禁用网卡的中断，表示驱动程序已经知道内存中有数据了，告诉物理网卡下次再收到数据包直接写内存就可以了，不要再通知CPU了，这样可以提高效率，避免CPU不停的被中断。 启动软中断继续处理数据包。这样的原因是硬中断处理程序执行的过程中不能被中断，所以如果它执行时间过长，会导致CPU没法响应其它硬件的中断，于是内核引入软中断，这样可以将硬中断处理函数中耗时的部分移到软中断处理函数里面来慢慢处理。  内核处理数据包 上一步中网络设备驱动程序会通过软触发内核网络模块中的软中断处理函数，内核处理数据包的流程如下图所示：
 对于第6步中驱动发出的软中断，内核中的ksoftirqd进程会调用网络模块的相应软中断所对应的处理函数，这里其实就是调用net_rx_action函数。 接下来net_rx_action调用网卡驱动里的poll函数来一个个地处理数据包。 而poll函数会让驱动会读取网卡写到内存中的数据包。事实上，内存中数据包的格式只有驱动知道。 驱动程序将内存中的数据包转换成内核网络模块能识别的skb(socket buffer)格式，然后调用napi_gro_receive函数 napi_gro_receive会处理GRO相关的内容，也就是将可以合并的数据包进行合并，这样就只需要调用一次协议栈。然后判断是否开启了RPS，如果开启了，将会调用enqueue_to_backlog。 enqueue_to_backlog函数会将数据包放入input_pkt_queue结构体中，然后返回。 &gt; Note: 如果input_pkt_queue满了的话，该数据包将会被丢弃，这个queue的大小可以通过net.core.netdev_max_backlog来配置 接下来CPU会在软中断上下文中处理自己input_pkt_queue里的网络数据（调用__netif_receive_skb_core函数） 如果没开启RPS，napi_gro_receive会直接调用__netif_receive_skb_core函数。 紧接着CPU会根据是不是有AF_PACKET类型的socket（原始套接字），如果有的话，拷贝一份数据给它(tcpdump抓包就是抓的这里的包)。 将数据包交给内核协议栈处理。 当内存中的所有数据包被处理完成后（poll函数执行完成），重新启用网卡的硬中断，这样下次网卡再收到数据的时候就会通知CPU。  内核协议栈 内核网络协议栈此时接收到的数据包其实是三层(IP网络层)数据包，因此，数据包首先会进入到IP网络层层，然后进入传输层处理。
IP网络层  ip_rcv: ip_rcv函数是IP网络层处理模块的入口函数，该函数首先判断属否需要丢弃该数据包（目的mac地址不是当前网卡，并且网卡设置了混杂模式），如果需要进一步处理就然后调用注册在netfilter中的NF_INET_PRE_ROUTING这条链上的处理函数。 NF_INET_PRE_ROUTING: netfilter放在协议栈中的钩子函数，可以通过iptables来注入一些数据包处理函数，用来修改或者丢弃数据包，如果数据包没被丢弃，将继续往下走。 &gt; NF_INET_PRE_ROUTING等netfilter链上的处理逻辑可以通iptables来设置，详情请移步: https://morven.life/notes/the_knowledge_of_iptables/ routing: 进行路由处理，如果是目的IP不是本地IP，且没有开启ip forward功能，那么数据包将被丢弃，如果开启了ip forward功能，那将进入ip_forward函数。 ip_forward: 该函数会先调用netfilter注册的NF_INET_FORWARD链上的相关函数，如果数据包没有被丢弃，那么将继续往后调用dst_output_sk函数。 dst_output_sk: 该函数会调用IP网络层的相应函数将该数据包发送出去，这一步将会在下一章节发送数据包中详细介绍。 ip_local_deliver: 如果上面路由处理发现发现目的IP是本地IP，那么将会调用ip_local_deliver函数，该函数先调用NF_INET_LOCAL_IN链上的相关函数，如果通过，数据包将会向下发送到UDP层。  传输层  udp_rcv: 该函数是UDP处理层模块的入口函数，它首先调用__udp4_lib_lookup_skb函数，根据目的IP和端口找对应的socket，如果没有找到相应的socket，那么该数据包将会被丢弃，否则继续。 sock_queue_rcv_skb: 该函数一是负责检查这个socket的receive buffer是不是满了，如果满了的话就丢弃该数据包；二是调用sk_filter看这个包是否是满足条件的包，如果当前socket上设置了filter，且该包不满足条件的话，这个数据包也将被丢弃。 __skb_queue_tail: 该函数将数据包放入socket接收队列的末尾。 sk_data_ready: 通知socket数据包已经准备好。 调用完sk_data_ready之后，一个数据包处理完成，等待应用层程序来读取。   Note: 上面所述的所有执行过程都在软中断的上下文中执行。"><link rel="stylesheet" href="https://unpkg.com/flexboxgrid@6.3.1/dist/flexboxgrid.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/tomorrow.min.css"><link rel="stylesheet" href="https://morven.life/css/index.css"><link href="https://morven.life/index.xml" rel="alternate" type="application/rss+xml" title="Morven&#39;s Life"><script>(function(undefined) {}).call('object' === typeof window && window || 'object' === typeof self && self || 'object' === typeof global && global || {});</script></head><body><article class="post Chinese" id="article"><div class="row"><div class="col-xs-12 col-md-8 col-md-offset-2 col-lg-6 col-lg-offset-3"><a href="https://morven.life/"><div class="head-line"></div></a><header class="post-header"><h1 class="post-title">Linux网络数据包的接收与发送过程</h1><div class="row"><div class="col-xs-6"><time class="post-date" datetime="2020-01-27 00:00:00 UTC">27 Jan 2020</time></div><div class="col-xs-6"><div class="post-author"><a target="_blank" href="https://morven.life/">@Morven&#39;s Life</a></div></div></div></header><div class="post-content markdown-body"><p>早在农历新年之前，就构思着将近半年重拾的网络基础整理成成一个系列，正好赶上武汉疫情在春节假期爆发，闲来无事，于是开始这一系列的笔记。</p><p>我的整体思路是：</p><ul><li>第一篇笔记会简单介绍Linux网络数据包接收和发送过程，但不涉及TCP/IP协议栈的细节知识，如果有需要了解这些基础知识的读者，我推荐去阅读<a href="http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html">阮一峰老师的互联网协议的系列文章</a>。</li><li>接下来，我会总结常用的Linux虚拟设备，同时会结合Linux自带的新网络工具包<a href="https://en.wikipedia.org/wiki/Iproute2">iproute2</a>来操作这些常用的网络设备。</li><li>有了前面的基础知识，我们再来了解几种常用的容器网络实现远离。</li><li>最后，我们深入探讨一下K8s平台中主流的网络实现。</li></ul><p>其实，严格上来说，这种学习思路其实很“急功近利”，但是，对于不太了解网络基础知识和Linux内核原理的人来说，这反而是一种很有效的学习方式。但是，私以为学习过程不只应该由浅入深，更应该螺旋向前迭代，温故而知新，才能获益良多。</p><hr><h2 id="数据包的接收过程">数据包的接收过程</h2><p>废话不多说，我们先开始第一篇笔记的内容。</p><p>为了简化起见，我们以一个UDP数据包在物理网卡上处理流程来介绍Linux网络数据包的接收和发送过程，我会尽量忽略一些不相关的细节。</p><h3 id="从网卡到内存">从网卡到内存</h3><p>我们知道，每个网络设备（网卡）需要有驱动才能工作，驱动需要在内核启动时加载到内核中才能工作。事实上，从逻辑上看，驱动是负责衔接网络设备和内核网络栈的中间模块，每当网络设备接收到新的数据包时，就会触发中断，而对应的中断处理程序正是加载到内核中的驱动程序。</p><p>下面这张图详细的展示了数据包如何从网络设备进入内存，并被处于内核中的驱动程序和网络栈处理的：</p><p><img src="https://i.loli.net/2020/01/27/8ZsXoQ1emVSzJlc.jpg" alt="network-receive-data-1.jpg"></p><ol><li>数据包进入物理网卡。如果目的地址不是该网络设备，且该来网络设备没有开启<a href="https://unix.stackexchange.com/questions/14056/what-is-kernel-ip-forwarding">混杂模式</a>，该包会被网络设备丢弃。</li><li>物理网卡将数据包通过DMA的方式写入到指定的内存地址，该地址由网卡驱动分配并初始化。</li><li>物理网卡通过硬件中断（IRQ）通知CPU，有新的数据包到达物理网卡需要处理。</li><li>CPU根据中断表，调用已经注册的中断函数，这个中断函数会调到驱动程序（NIC Driver）中相应的函数</li><li>驱动先禁用网卡的中断，表示驱动程序已经知道内存中有数据了，告诉物理网卡下次再收到数据包直接写内存就可以了，不要再通知CPU了，这样可以提高效率，避免CPU不停的被中断。</li><li>启动软中断继续处理数据包。这样的原因是硬中断处理程序执行的过程中不能被中断，所以如果它执行时间过长，会导致CPU没法响应其它硬件的中断，于是内核引入软中断，这样可以将硬中断处理函数中耗时的部分移到软中断处理函数里面来慢慢处理。</li></ol><h3 id="内核处理数据包">内核处理数据包</h3><p>上一步中网络设备驱动程序会通过软触发内核网络模块中的软中断处理函数，内核处理数据包的流程如下图所示：</p><p><img src="https://i.loli.net/2020/01/27/y2SZleoIwtbxDLs.jpg" alt="network-receive-data-2.jpg"></p><ol><li>对于第6步中驱动发出的软中断，内核中的<code>ksoftirqd</code>进程会调用网络模块的相应软中断所对应的处理函数，这里其实就是调用<code>net_rx_action</code>函数。</li><li>接下来<code>net_rx_action</code>调用网卡驱动里的<code>poll</code>函数来一个个地处理数据包。</li><li>而<code>poll</code>函数会让驱动会读取网卡写到内存中的数据包。事实上，内存中数据包的格式只有驱动知道。</li><li>驱动程序将内存中的数据包转换成内核网络模块能识别的<code>skb</code>(socket buffer)格式，然后调用<code>napi_gro_receive</code>函数</li><li><code>napi_gro_receive</code>会处理<a href="https://lwn.net/Articles/358910/">GRO</a>相关的内容，也就是将可以合并的数据包进行合并，这样就只需要调用一次协议栈。然后判断是否开启了RPS，如果开启了，将会调用<code>enqueue_to_backlog</code>。</li><li><code>enqueue_to_backlog</code>函数会将数据包放入<code>input_pkt_queue</code>结构体中，然后返回。 &gt; Note: 如果<code>input_pkt_queue</code>满了的话，该数据包将会被丢弃，这个queue的大小可以通过<code>net.core.netdev_max_backlog</code>来配置</li><li>接下来CPU会在软中断上下文中处理自己<code>input_pkt_queue</code>里的网络数据（调用<code>__netif_receive_skb_core</code>函数）</li><li>如果没开启<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L99-L222">RPS</a>，<code>napi_gro_receive</code>会直接调用<code>__netif_receive_skb_core</code>函数。</li><li>紧接着CPU会根据是不是有<code>AF_PACKET</code>类型的socket（原始套接字），如果有的话，拷贝一份数据给它(<code>tcpdump</code>抓包就是抓的这里的包)。</li><li>将数据包交给内核协议栈处理。</li><li>当内存中的所有数据包被处理完成后（<code>poll</code>函数执行完成），重新启用网卡的硬中断，这样下次网卡再收到数据的时候就会通知CPU。</li></ol><h3 id="内核协议栈">内核协议栈</h3><p>内核网络协议栈此时接收到的数据包其实是三层(IP网络层)数据包，因此，数据包首先会进入到IP网络层层，然后进入传输层处理。</p><h4 id="ip网络层">IP网络层</h4><p><img src="https://i.loli.net/2020/01/27/oxfqD6Upiw7Blbt.jpg" alt="network-receive-data-3.jpg"></p><ul><li>ip_rcv: <code>ip_rcv</code>函数是IP网络层处理模块的入口函数，该函数首先判断属否需要丢弃该数据包（目的mac地址不是当前网卡，并且网卡设置了混杂模式），如果需要进一步处理就然后调用注册在netfilter中的<code>NF_INET_PRE_ROUTING</code>这条链上的处理函数。</li><li>NF_INET_PRE_ROUTING: netfilter放在协议栈中的钩子函数，可以通过iptables来注入一些数据包处理函数，用来修改或者丢弃数据包，如果数据包没被丢弃，将继续往下走。 &gt; <code>NF_INET_PRE_ROUTING</code>等netfilter链上的处理逻辑可以通iptables来设置，详情请移步: <a href="https://morven.life/notes/the_knowledge_of_iptables/">https://morven.life/notes/the_knowledge_of_iptables/</a></li><li>routing: 进行路由处理，如果是目的IP不是本地IP，且没有开启<code>ip forward</code>功能，那么数据包将被丢弃，如果开启了<code>ip forward</code>功能，那将进入<code>ip_forward</code>函数。</li><li>ip_forward: 该函数会先调用<code>netfilter</code>注册的<code>NF_INET_FORWARD</code>链上的相关函数，如果数据包没有被丢弃，那么将继续往后调用<code>dst_output_sk</code>函数。</li><li>dst_output_sk: 该函数会调用IP网络层的相应函数将该数据包发送出去，这一步将会在下一章节发送数据包中详细介绍。</li><li>ip_local_deliver: 如果上面路由处理发现发现目的IP是本地IP，那么将会调用<code>ip_local_deliver</code>函数，该函数先调用<code>NF_INET_LOCAL_IN</code>链上的相关函数，如果通过，数据包将会向下发送到UDP层。</li></ul><h3 id="传输层">传输层</h3><p><img src="https://i.loli.net/2020/01/27/Hcyw6pFJDLVtZ8j.jpg" alt="network-receive-data-4.jpg"></p><ul><li>udp_rcv: 该函数是UDP处理层模块的入口函数，它首先调用<code>__udp4_lib_lookup_skb</code>函数，根据目的IP和端口找对应的<code>socket</code>，如果没有找到相应的<code>socket</code>，那么该数据包将会被丢弃，否则继续。</li><li>sock_queue_rcv_skb: 该函数一是负责检查这个socket的receive buffer是不是满了，如果满了的话就丢弃该数据包；二是调用<code>sk_filter</code>看这个包是否是满足条件的包，如果当前socket上设置了filter，且该包不满足条件的话，这个数据包也将被丢弃。</li><li>__skb_queue_tail: 该函数将数据包放入socket接收队列的末尾。</li><li>sk_data_ready: 通知socket数据包已经准备好。</li><li>调用完sk_data_ready之后，一个数据包处理完成，等待应用层程序来读取。</li></ul><blockquote><p>Note: 上面所述的所有执行过程都在软中断的上下文中执行。</p></blockquote><h2 id="总结">总结</h2><p>理解了Linux网络数据包的接收和发送流程，我们就可以知道在哪些地方监控和修改数据包，哪些情况下数据包可能被丢弃，特别是了解了netfilter中相应钩子函数的位置，对于了解iptables的用法有一定的帮助，同时也会帮助我们更好的理解Linux下的网络虚拟设备。</p></div></div></div></article><script src="https://morven.life/js/highlight.pack.js"></script><script src="https://unpkg.com/quicklink@0.1.1/dist/quicklink.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@10.19.0/dist/lazyload.min.js"></script><script>var lazyImage = new LazyLoad({
    container: document.getElementById('article')
  });</script><script>hljs.initHighlightingOnLoad();
  
  var posts = document.getElementById('posts-list');
  posts && quicklink({
    el: posts,
    priority: true,
  });</script></body></html>