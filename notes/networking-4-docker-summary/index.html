<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hugo 0.55.6"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="author" content="Morven&#39;s Life"><meta property="og:url" content="https://morven.life/notes/networking-4-docker-summary/"><link rel="canonical" href="https://morven.life/notes/networking-4-docker-summary/"><link rel="shortcut icon" href="https://morven.life/favicon.ico" type="image/x-png"><script type="application/ld+json">{
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/morven.life\/"
      },
      "articleSection" : "notes",
      "name" : "容器网络(一)",
      "headline" : "容器网络(一)",
      "description" : "所谓容器网络需要结局的两大核心问题是：\n 容器IP地址的管理 容器之间的相互通信  其中，容器IP地址的管理包括容器IP地址的分配与回收，而容器之间的相互通信包括同一主机容器之间和跨主机容器之间通信两种场景。这两个问题也不能完全分开来看，因为不同的解决方案往往要同时考虑以上两点。容器网络的发展已经相对成熟，这篇笔记先对主流容器网络模型做一些概述，然后将进一步对典型的容器网络模型展开实践。\nCNM vs CNI 关于容器网络，Docker与Kubernetes分别提出了不同的规范标准：\n Docker采用的CNM(Container Network Model) Kunernetes支持的CNI模型(Container Network Interface)  CNM基于libnetwork，是Docker内置的模型规范，它的总体架构如下图所示：\n可以看到，CNM规范主要定义了以下三个组件：\n Sandbox: 每个Sandbox包一个容器网络栈(network stack)的配置：容器的网口、路由表和DNS设置等，Sanbox可以通过Linux网络命名空间netns来实现 Endpoint: 每个Sandbox通过Endpoint加入到一个Network里，Endpoint可以通过Linux虚拟网络设备veth对来实现 Network: 一组能相互直接通信的Endpoint，Network可以通过Linux网桥设备bridge，VLAN等实现  可以看到，底层实现原理还是我们之前介绍过的Linux虚拟网络设备，网络命名空间等。但是，为什么Kubernetes没有采用CNM规范标准；而是选择CNI，感兴趣的话可以去看看Kubernetes的文章Why Kubernetes doesn’t use libnetwork，总的来说，不使用CNM最关键的一点是，是因为Kubernetes考虑到CNM在一定程度上和container runtime耦合度太高，因此以Kubernetes为领导的其他一些组织开始制定新的CNI规范。CNI并不是Docker原生支持的，它是为容器技术设计的通用型网络接口，因此CNI接口可以很容易地从高层向底层调用，但从底层到高层却不是很方便，所以一些常见的CNI插件很难在Docker层面激活。但是这两个模型全都支持插件化，也就是说我们每个人都可以按照这两套网络规范来编写自己的具体网络实现。\n我们省去这两套规范的具体介绍，直接从要解决的网络问题出发，来看看主流的容器网络实现原理。\n技术术语 在开始之前，我们总结一些在容器网络的介绍文章里面看到各种技术术语：\n IPAM: IP Address Management，即IP地址管理。IPAM并不是容器时代特有的词汇，传统的标准网络协议比如DHCP其实也是一种IPAM，负责从MAC地址分发IP地址；但是到了容器时代我们提到IPAM，我们特指为每一个容器实例分配和回收IP地址，保证一个集群里面的所有容器都分配全局唯一的IP地址；主流的做法包括：基于CIDR的IP地址段分配地或精确为每一个容器分配IP。 Overlay：在容器时代，就是在主机现有二层（数据链路层）或三层（IP网络层）基础之上再构建起来一个独立的网络，这个overlay网络通常会有自己独立的IP地址空间、交换或者路由的实现。 IPIP: 一种基于Linux网络设备TUN实现的隧道协议，允许将三层（IP）网络包封装在另外一个三层网络包之发送和接收，详情请看之前IPIP隧道的介绍笔记。 IPSec: 跟IPIP隧道协议类似，是一个点对点的一个加密通信协议，一般会用到Overlay网络的数据隧道里。 VXLAN：最主要是解决VLAN支持虚拟网络数量（4096）过少的问题而由VMware、Cisco、RedHat等联合提出的解决方案。VXLAN可以支持在一个VPC(Virtual Private Cloud)划分多达1600万个虚拟网络。 BGP: 主干网自治网络的路由协议，当代的互联网由很多小的AS自治网络(Autonomous system)构成，自治网络之间的三层路由是由BGP实现的，简单来说，通过BGP协议AS告诉其他AS自己子网里都包括哪些IP地址段，自己的AS编号以及一些其他的信息。 SDN: Software-Defined Networking，一种广义的概念，通过软件方式快速配置网络，往往包括一个中央控制层来集中配置底层基础网络设施。  host网络 不管是对外暴露容器内的服务还是容器之间的跨主机通信，我们能想到的最直观的解决方案就是直接使用宿主机host网络，这时，容器完全复用复用宿主机的网络设备以及协议栈，容器的IP就是主机的IP，这样，只要宿主机主机能通信，容器也就自然能通信。但是这样，为了暴露容器服务，每个容器需要占用宿主机上的一个端口，通过这个端口和外界通信。所以，就需要手动维护端口的分配，不要使不同的容器服务运行在一个端口上，正因为如此，这种容器网络模型很难被推广到生产环境。\ndocker原生支持的网络模式可以通过docker network ls来看：\n# docker network ls NETWORK ID NAME DRIVER SCOPE f559b082c95f bridge bridge local 5f11ccbbf488 host host local 97aedfe8792d none null local  可以看到三种网络模型，在创建容器的时候可以通过--network来指定要使用的模型。其中bridge是默认的网络模型，我们后面将会模拟实现bridge模型；nono不创建任何网络，host网络模型是我们即将要验证的，它不会创建新的netns网络命名空间。",
      "inLanguage" : "en-US",
      "author" : "Morven\x27s Life",
      "creator" : "Morven\x27s Life",
      "publisher": "Morven\x27s Life",
      "accountablePerson" : "Morven\x27s Life",
      "copyrightHolder" : "Morven\x27s Life",
      "copyrightYear" : "2020",
      "datePublished": "2020-01-30 00:00:00 \x2b0000 UTC",
      "dateModified" : "2020-01-30 00:00:00 \x2b0000 UTC",
      "url" : "https:\/\/morven.life\/notes\/networking-4-docker-summary\/",
      "keywords" : [  ]
  }</script><title>容器网络(一) - Morven&#39;s Life</title><meta property="og:title" content="容器网络(一) - Morven&#39;s Life"><meta property="og:type" content="article"><meta name="description" content="所谓容器网络需要结局的两大核心问题是：
 容器IP地址的管理 容器之间的相互通信  其中，容器IP地址的管理包括容器IP地址的分配与回收，而容器之间的相互通信包括同一主机容器之间和跨主机容器之间通信两种场景。这两个问题也不能完全分开来看，因为不同的解决方案往往要同时考虑以上两点。容器网络的发展已经相对成熟，这篇笔记先对主流容器网络模型做一些概述，然后将进一步对典型的容器网络模型展开实践。
CNM vs CNI 关于容器网络，Docker与Kubernetes分别提出了不同的规范标准：
 Docker采用的CNM(Container Network Model) Kunernetes支持的CNI模型(Container Network Interface)  CNM基于libnetwork，是Docker内置的模型规范，它的总体架构如下图所示：
可以看到，CNM规范主要定义了以下三个组件：
 Sandbox: 每个Sandbox包一个容器网络栈(network stack)的配置：容器的网口、路由表和DNS设置等，Sanbox可以通过Linux网络命名空间netns来实现 Endpoint: 每个Sandbox通过Endpoint加入到一个Network里，Endpoint可以通过Linux虚拟网络设备veth对来实现 Network: 一组能相互直接通信的Endpoint，Network可以通过Linux网桥设备bridge，VLAN等实现  可以看到，底层实现原理还是我们之前介绍过的Linux虚拟网络设备，网络命名空间等。但是，为什么Kubernetes没有采用CNM规范标准；而是选择CNI，感兴趣的话可以去看看Kubernetes的文章Why Kubernetes doesn’t use libnetwork，总的来说，不使用CNM最关键的一点是，是因为Kubernetes考虑到CNM在一定程度上和container runtime耦合度太高，因此以Kubernetes为领导的其他一些组织开始制定新的CNI规范。CNI并不是Docker原生支持的，它是为容器技术设计的通用型网络接口，因此CNI接口可以很容易地从高层向底层调用，但从底层到高层却不是很方便，所以一些常见的CNI插件很难在Docker层面激活。但是这两个模型全都支持插件化，也就是说我们每个人都可以按照这两套网络规范来编写自己的具体网络实现。
我们省去这两套规范的具体介绍，直接从要解决的网络问题出发，来看看主流的容器网络实现原理。
技术术语 在开始之前，我们总结一些在容器网络的介绍文章里面看到各种技术术语：
 IPAM: IP Address Management，即IP地址管理。IPAM并不是容器时代特有的词汇，传统的标准网络协议比如DHCP其实也是一种IPAM，负责从MAC地址分发IP地址；但是到了容器时代我们提到IPAM，我们特指为每一个容器实例分配和回收IP地址，保证一个集群里面的所有容器都分配全局唯一的IP地址；主流的做法包括：基于CIDR的IP地址段分配地或精确为每一个容器分配IP。 Overlay：在容器时代，就是在主机现有二层（数据链路层）或三层（IP网络层）基础之上再构建起来一个独立的网络，这个overlay网络通常会有自己独立的IP地址空间、交换或者路由的实现。 IPIP: 一种基于Linux网络设备TUN实现的隧道协议，允许将三层（IP）网络包封装在另外一个三层网络包之发送和接收，详情请看之前IPIP隧道的介绍笔记。 IPSec: 跟IPIP隧道协议类似，是一个点对点的一个加密通信协议，一般会用到Overlay网络的数据隧道里。 VXLAN：最主要是解决VLAN支持虚拟网络数量（4096）过少的问题而由VMware、Cisco、RedHat等联合提出的解决方案。VXLAN可以支持在一个VPC(Virtual Private Cloud)划分多达1600万个虚拟网络。 BGP: 主干网自治网络的路由协议，当代的互联网由很多小的AS自治网络(Autonomous system)构成，自治网络之间的三层路由是由BGP实现的，简单来说，通过BGP协议AS告诉其他AS自己子网里都包括哪些IP地址段，自己的AS编号以及一些其他的信息。 SDN: Software-Defined Networking，一种广义的概念，通过软件方式快速配置网络，往往包括一个中央控制层来集中配置底层基础网络设施。  host网络 不管是对外暴露容器内的服务还是容器之间的跨主机通信，我们能想到的最直观的解决方案就是直接使用宿主机host网络，这时，容器完全复用复用宿主机的网络设备以及协议栈，容器的IP就是主机的IP，这样，只要宿主机主机能通信，容器也就自然能通信。但是这样，为了暴露容器服务，每个容器需要占用宿主机上的一个端口，通过这个端口和外界通信。所以，就需要手动维护端口的分配，不要使不同的容器服务运行在一个端口上，正因为如此，这种容器网络模型很难被推广到生产环境。
docker原生支持的网络模式可以通过docker network ls来看：
# docker network ls NETWORK ID NAME DRIVER SCOPE f559b082c95f bridge bridge local 5f11ccbbf488 host host local 97aedfe8792d none null local  可以看到三种网络模型，在创建容器的时候可以通过--network来指定要使用的模型。其中bridge是默认的网络模型，我们后面将会模拟实现bridge模型；nono不创建任何网络，host网络模型是我们即将要验证的，它不会创建新的netns网络命名空间。"><link rel="stylesheet" href="https://unpkg.com/flexboxgrid@6.3.1/dist/flexboxgrid.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/tomorrow.min.css"><link rel="stylesheet" href="https://morven.life/css/index.css"><link href="https://morven.life/index.xml" rel="alternate" type="application/rss+xml" title="Morven&#39;s Life"><script>(function(undefined) {}).call('object' === typeof window && window || 'object' === typeof self && self || 'object' === typeof global && global || {});</script></head><body><article class="post Chinese" id="article"><div class="row"><div class="col-xs-12 col-md-8 col-md-offset-2 col-lg-6 col-lg-offset-3"><a href="https://morven.life/"><div class="head-line"></div></a><header class="post-header"><h1 class="post-title">容器网络(一)</h1><div class="row"><div class="col-xs-6"><time class="post-date" datetime="2020-01-30 00:00:00 UTC">30 Jan 2020</time></div><div class="col-xs-6"><div class="post-author"><a target="_blank" href="https://morven.life/">@Morven&#39;s Life</a></div></div></div></header><div class="post-content markdown-body"><p>所谓容器网络需要结局的两大核心问题是：</p><ol><li>容器IP地址的管理</li><li>容器之间的相互通信</li></ol><p>其中，容器IP地址的管理包括容器IP地址的分配与回收，而容器之间的相互通信包括同一主机容器之间和跨主机容器之间通信两种场景。这两个问题也不能完全分开来看，因为不同的解决方案往往要同时考虑以上两点。容器网络的发展已经相对成熟，这篇笔记先对主流容器网络模型做一些概述，然后将进一步对典型的容器网络模型展开实践。</p><h2 id="cnm-vs-cni">CNM vs CNI</h2><p>关于容器网络，Docker与Kubernetes分别提出了不同的规范标准：</p><ul><li>Docker采用的<a href="https://github.com/docker/libnetwork/blob/master/docs/design.md">CNM</a>(Container Network Model)</li><li>Kunernetes支持的<a href="https://github.com/containernetworking/cni">CNI</a>模型(Container Network Interface)</li></ul><p>CNM基于<a href="https://github.com/docker/libnetwork">libnetwork</a>，是Docker内置的模型规范，它的总体架构如下图所示：</p><p><img src="https://i.loli.net/2020/01/31/lWUKNw5Tbp3cArC.jpg" alt="cnm-model.jpg"></p><p>可以看到，CNM规范主要定义了以下三个组件：</p><ul><li>Sandbox: 每个Sandbox包一个容器网络栈(network stack)的配置：容器的网口、路由表和DNS设置等，Sanbox可以通过Linux网络命名空间netns来实现</li><li>Endpoint: 每个Sandbox通过Endpoint加入到一个Network里，Endpoint可以通过Linux虚拟网络设备veth对来实现</li><li>Network: 一组能相互直接通信的Endpoint，Network可以通过Linux网桥设备bridge，VLAN等实现</li></ul><p>可以看到，底层实现原理还是我们之前介绍过的Linux虚拟网络设备，网络命名空间等。但是，为什么Kubernetes没有采用CNM规范标准；而是选择CNI，感兴趣的话可以去看看Kubernetes的文章<a href="https://kubernetes.io/blog/2016/01/why-kubernetes-doesnt-use-libnetwork/">Why Kubernetes doesn’t use libnetwork</a>，总的来说，不使用CNM最关键的一点是，是因为Kubernetes考虑到CNM在一定程度上和container runtime耦合度太高，因此以Kubernetes为领导的其他一些组织开始制定新的CNI规范。CNI并不是Docker原生支持的，它是为容器技术设计的通用型网络接口，因此CNI接口可以很容易地从高层向底层调用，但从底层到高层却不是很方便，所以一些常见的CNI插件很难在Docker层面激活。但是这两个模型全都支持插件化，也就是说我们每个人都可以按照这两套网络规范来编写自己的具体网络实现。</p><p>我们省去这两套规范的具体介绍，直接从要解决的网络问题出发，来看看主流的容器网络实现原理。</p><h2 id="技术术语">技术术语</h2><p>在开始之前，我们总结一些在容器网络的介绍文章里面看到各种技术术语：</p><ul><li>IPAM: IP Address Management，即IP地址管理。IPAM并不是容器时代特有的词汇，传统的标准网络协议比如DHCP其实也是一种IPAM，负责从MAC地址分发IP地址；但是到了容器时代我们提到IPAM，我们特指为每一个容器实例分配和回收IP地址，保证一个集群里面的所有容器都分配全局唯一的IP地址；主流的做法包括：基于CIDR的IP地址段分配地或精确为每一个容器分配IP。</li><li>Overlay：在容器时代，就是在主机现有二层（数据链路层）或三层（IP网络层）基础之上再构建起来一个独立的网络，这个overlay网络通常会有自己独立的IP地址空间、交换或者路由的实现。</li><li>IPIP: 一种基于Linux网络设备TUN实现的隧道协议，允许将三层（IP）网络包封装在另外一个三层网络包之发送和接收，详情请看之前IPIP隧道的<a href="https://morven.life/notes/networking-3-ipip/">介绍笔记</a>。</li><li>IPSec: 跟IPIP隧道协议类似，是一个点对点的一个加密通信协议，一般会用到Overlay网络的数据隧道里。</li><li>VXLAN：最主要是解决VLAN支持虚拟网络数量（4096）过少的问题而由VMware、Cisco、RedHat等联合提出的解决方案。VXLAN可以支持在一个VPC(Virtual Private Cloud)划分多达1600万个虚拟网络。</li><li>BGP: 主干网自治网络的路由协议，当代的互联网由很多小的AS自治网络(Autonomous system)构成，自治网络之间的三层路由是由BGP实现的，简单来说，通过BGP协议AS告诉其他AS自己子网里都包括哪些IP地址段，自己的AS编号以及一些其他的信息。</li><li>SDN: Software-Defined Networking，一种广义的概念，通过软件方式快速配置网络，往往包括一个中央控制层来集中配置底层基础网络设施。</li></ul><h2 id="host网络">host网络</h2><p>不管是对外暴露容器内的服务还是容器之间的跨主机通信，我们能想到的最直观的解决方案就是直接使用宿主机host网络，这时，容器完全复用复用宿主机的网络设备以及协议栈，容器的IP就是主机的IP，这样，只要宿主机主机能通信，容器也就自然能通信。但是这样，为了暴露容器服务，每个容器需要占用宿主机上的一个端口，通过这个端口和外界通信。所以，就需要手动维护端口的分配，不要使不同的容器服务运行在一个端口上，正因为如此，这种容器网络模型很难被推广到生产环境。</p><p>docker原生支持的网络模式可以通过<code>docker network ls</code>来看：</p><pre><code># docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
f559b082c95f        bridge              bridge              local
5f11ccbbf488        host                host                local
97aedfe8792d        none                null                local
</code></pre><p>可以看到三种网络模型，在创建容器的时候可以通过<code>--network</code>来指定要使用的模型。其中bridge是默认的网络模型，我们后面将会模拟实现bridge模型；nono不创建任何网络，host网络模型是我们即将要验证的，它不会创建新的netns网络命名空间。</p><p>在开始实验之前，我们需要确定宿主机上没有其他进程在监听<code>80</code>端口：</p><pre><code># lsof -i tcp:80
</code></pre><p>然后我们创建容器运行一个nginx实例，并通过<code>--network</code>来指定要使用的host模型：</p><pre><code># docker run -d --name mynginx --network host nginx:latest
87f1e766410c4bd2644a007115628462751f779fc95252fffb3d7183bb59747c
# lsof -i tcp:80
COMMAND   PID            USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
nginx   29804            root    6u  IPv4 109710      0t0  TCP *:http (LISTEN)
nginx   29839 systemd-resolve    6u  IPv4 109710      0t0  TCP *:http (LISTEN)
# ls /var/run/docker/netns/
default
</code></pre><p>可以看到，宿主机的<code>80</code>端口在容器启动之后就打开了，并确定宿主机网络中没有新的netns网络命名空间创建出来。</p><blockquote><p>Note: 使用<code>ip netns show</code>并不能看到当前所有的netns网络命名空间，因为docker会将netns的设备文件从linux默认的<code>/var/run/netns</code>迁移到<code>/var/run/docker/netns/</code>，我们可以创建映射到<code>/var/run/docker/netns/</code>的符号链接<code>/var/run/netns</code>：</p></blockquote><pre><code># ln -s /var/run/docker/netns /var/run/netns
# ip netns show
default
</code></pre><h2 id="bridge网络">bridge网络</h2><p>bridge桥接网络是docker默认的网络模型，如果我们在创建容器的时候不指定网络模型，则默认使用<code>bridge</code>。bridge网络模型可以解决单宿主机上的容器之间的通信以及容器服务对外的暴露，实现原理也很简单：</p><p><img src="https://i.loli.net/2020/01/30/RjzDdbcK7uJ546Q.jpg" alt="network-docker-bridge-1.jpg"></p><p>可以看到，bridge网络模型主要依赖于大名鼎鼎的docker0网桥以及veth虚拟网络设备对实现，通过之前笔记对于linux虚拟网络设备的了解，我们知道veth设备对对于从一个设备发出的数据包，会直接出现在另一个网络设备上，即使不在一个netns网络命名空间中，所以将veth设备对实际上是连接不同netns网络命名空间的”网线”，docker0网桥设备充当不同容器网络的网关。事实上，我们一旦而当以bridge网络模式创建容器时，会自动创建相应的veth设备对，其中一端连接到docker0网桥，另外一端连接到容器网络的eth0虚拟网卡。</p><p>首先我们在安装了docker的宿主机上查看网桥设备docker0和路由规则：</p><pre><code># ip link show docker0
4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:59:c8:67:c0 brd ff:ff:ff:ff:ff:ff
# ip route ls
...
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1
</code></pre><p>然后使用默认的bridge网络模式创建一个容器，并查看宿主机端的veth设备对：</p><pre><code># docker run -d --name mynginx  nginx:latest
# ip link show type veth
11: veth42772d8@if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
    link/ether e2:a3:89:76:14:f3 brd ff:ff:ff:ff:ff:ff link-netnsid 0
# bridge link
11: veth42772d8 state UP @if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 master docker0 state forwarding priority 32 cost 2
</code></pre><p>可以看到新的veth设备对的一端<code>veth42772d8</code>已经连接到<code>docker0</code>网桥，那么另外一端呢？</p><pre><code># ls /var/run/docker/netns/
62fd67d9ef3e  default
# nsenter --net=/var/run/docker/netns/62fd67d9ef3e ip link show type veth
10: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
# nsenter --net=/var/run/docker/netns/62fd67d9ef3e ip addr show  type veth
10: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
</code></pre><p>正如我们设想的那样，veth设备的另外一端处于新的netns网络命名空间<code>62fd67d9ef3e</code>中，并且IP地址为<code>172.17.0.2/16</code>，与<code>docker0</code>处于同一子网中。</p><blockquote><p>Note: 如果我们创建了映射到<code>/var/run/docker/netns/</code>的符号链接<code>/var/run/netns</code>，就不用使用<code>nsenter</code>命令或者进入容器内部查看veth设备对的另外一端，直接使用如下<code>iproute2</code>工具包</p></blockquote><pre><code># ip netns show
62fd67d9ef3e (id: 0)
default
# ip netns exec 62fd67d9ef3e ip link show type veth
10: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
# ip netns exec 62fd67d9ef3e ip addr show type veth
10: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
</code></pre><h2 id="bridge网络模拟">bridge网络模拟</h2><p>我们加下来就模拟一下bridge网络模型的实现，基本的网络拓扑图如下所示：</p><p><img src="https://i.loli.net/2020/01/30/ighFPJUxGuNRTZE.jpg" alt="network-docker-bridge-2.jpg"></p><ol><li><p>首先创建两个netns网络命名空间：</p><pre><code># ip netns add netns_A
# ip netns add netns_B
# ip netns
netns_B
netns_A
default
</code></pre></li><li><p>在default网络命名空间中创建网桥设备mybr0，并分配IP地址<code>172.18.0.1/16</code>使其成为对应子网的网关：</p><pre><code># ip link add name mybr0 type bridge
# ip addr add 172.18.0.1/16 dev mybr0
# ip link show mybr0
12: mybr0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
link/ether ae:93:35:ab:59:2a brd ff:ff:ff:ff:ff:ff
# ip route
...
172.18.0.0/16 dev mybr0 proto kernel scope link src 172.18.0.1
</code></pre></li><li><p>接下来，创建veth设备对并连接在第一步创建的两个netns网络命名空间：</p><pre><code># ip link add vethA type veth peer name vethpA
# ip link show vethA
14: vethA@vethpA: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
link/ether da:f1:fd:19:6b:4a brd ff:ff:ff:ff:ff:ff
# ip link show vethpA
13: vethpA@vethA: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
link/ether 86:d6:16:43:54:9e brd ff:ff:ff:ff:ff:ff
</code></pre></li><li><p>将上一步创建的veth设备对的一端<code>vethA</code>连接到<code>mybr0</code>网桥并启动：</p><pre><code># ip link set dev vethA master mybr0
# ip link set vethA up
# bridge link
14: vethA state LOWERLAYERDOWN @vethpA: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 master mybr0 state disabled priority 32 cost 2
</code></pre></li><li><p>将veth设备对的另一端<code>vethpA</code>放到netns网络命名空间<code>netns_A</code>中并配置IP启动：</p><pre><code># ip link set vethpA netns netns_A
# ip netns exec netns_A ip link set vethpA name eth0
# ip netns exec netns_A ip addr add 172.18.0.2/16 dev eth0
# ip netns exec netns_A ip link set eth0 up
# ip netns exec netns_A ip addr show type veth
13: eth0@if14: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
link/ether 86:d6:16:43:54:9e brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 172.18.0.2/16 scope global eth0
   valid_lft forever preferred_lft forever
</code></pre></li><li><p>现在就可以验证从<code>netns_A</code>网络命名空间中访问<code>mybr0</code>网关：</p><pre><code># ip netns exec netns_A ping -c 2 172.18.0.1
PING 172.18.0.1 (172.18.0.1) 56(84) bytes of data.
64 bytes from 172.18.0.1: icmp_seq=1 ttl=64 time=0.096 ms
64 bytes from 172.18.0.1: icmp_seq=2 ttl=64 time=0.069 ms

--- 172.18.0.1 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1004ms
rtt min/avg/max/mdev = 0.069/0.082/0.096/0.016 m
</code></pre></li><li><p>若想要从从<code>netns_A</code>网络命名空间中非<code>172.18.0.0/16</code>的地址，就需要增加一条默认默认路由：</p><pre><code># ip netns exec netns_A ip route add default via 172.18.0.1
# ip netns exec netns_A ip route
default via 172.18.0.1 dev eth0
172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.2
</code></pre></li></ol><blockquote><p>Note: 如果你此时尝试去ping其他的公网地址，eg. <code>google.com</code>，是ping不通的，是因为ping的出去的数据包（ICMP包）的源地址没有做源地址转换(snat)，导致ICMP包有去无回；Docker是通过设置<code>iptables</code>实现源地址转换的。</p></blockquote><ol><li><p>接下来，按照上述步骤创建连接<code>default</code>和<code>netns_B</code>网络命名空间veth设备对：</p><pre><code># ip link add vethB type veth peer name vethpB
# ip link set dev vethB master mybr0
# ip link set vethB up
# ip link set vethpB netns netns_B
# ip netns exec netns_B ip link set vethpB name eth0
# ip netns exec netns_B ip addr add 172.18.0.3/16 dev eth0
# ip netns exec netns_B ip link set eth0 up
# ip netns exec netns_B ip route add default via 172.18.0.1
# ip netns exec netns_B ip add show eth0
15: eth0@if16: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
link/ether 0e:2f:c6:de:fe:24 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 172.18.0.3/16 scope global eth0
   valid_lft forever preferred_lft forever
# ip netns exec netns_B ip route show
default via 172.18.0.1 dev eth0
172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.3
</code></pre></li><li><p>默认情况下把Linux会把网桥设备bridge的<code>FORWORD</code>功能禁用，所以在<code>netns_A</code>里面是ping不通<code>netns_B</code>的，需要额外增加一条iptables规则：</p><pre><code># iptables -A FORWARD -i mybr0 -j ACCEPT
</code></pre></li><li><p>现在就可以验证两个netns网络命名空间之间可以互通：</p><pre><code># ip netns exec netns_A ping -c 2 172.18.0.3
PING 172.18.0.3 (172.18.0.3) 56(84) bytes of data.
64 bytes from 172.18.0.3: icmp_seq=1 ttl=64 time=0.091 ms
64 bytes from 172.18.0.3: icmp_seq=2 ttl=64 time=0.093 ms

--- 172.18.0.3 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1027ms
rtt min/avg/max/mdev = 0.091/0.092/0.093/0.001 ms
# ip netns exec netns_B ping -c 2 172.18.0.2
PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data.
64 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=0.259 ms
64 bytes from 172.18.0.2: icmp_seq=2 ttl=64 time=0.078 ms

--- 172.18.0.2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1030ms
rtt min/avg/max/mdev = 0.078/0.168/0.259/0.091 ms
</code></pre></li></ol><p>实际上，此时两个netns网络命名空间处于同一个子网中，所以网桥设备<code>mybr0</code>还是在二层（数据链路层）起到的作用，只需要对方的MAC地址就可以访问。</p><p>但是如果需要从两个netns网络命名空间访问其他网段的地址，这个时候就需要设置默认网桥设备<code>mybr0</code>充当的默认网关地址就发挥作用了：来自于两个netns网络命名空间的数据包发现目标IP地址并不是本子网地址，于是发给网关<code>mybr0</code>，此时网桥设备<code>mybr0</code>其实工作在三层（IP网络层），它收到数据包之后，查看本地路由与目标IP地址，寻找下一跳的地址。</p><p>当然，如果需要从两个netns网络命名空间访问其他公网地址.eg. <code>google.com</code>，需要这是iptables来做源地址转换，这里就不细细展开来说。</p><h2 id="容器之间跨主机通信">容器之间跨主机通信</h2><p>上面我们介绍的bridge网络模型主要用于解决统一主机间的容器相互访问以及容器对外暴露服务的问题，并没有涉及到怎么解决跨主机的容器之间怎么互相访问的问题。</p><p>解决跨主机通信的可行方案主要是让容器配置与宿主机不一样的IP地址，往往是在现有二层或三层网络之上再构建起来一个独立的网络，这个网络通常会有自己独立的IP地址空间、交换或者路由的实现。</p><p>但是由于容器有自己独立配置的IP地址，underlay平面的底层网络设备如交换机、路由器等完全不感知这些IP的存在，也就导致容器的IP不能直接路由出去实现跨主机通信。</p><p>为了解决容器独立IP地址间的访问问题，主要有以下两个思路：</p><ol><li>修改底层网络设备配置，加入容器网络IP地址的管理，修改路由器网关等，该方式主要和SDN(Software define networking)结合。</li><li><p>完全不修改底层网络设备配置，复用原有的underlay平面网络，解决容器跨主机通信，主要有如下两种方式:</p><ul><li>隧道传输(Overlay)： 将容器的数据包封装到原主机网络的三层或者四层数据包中，然后使用主机网络的IP或者TCP/UDP传输到目标主机，目标主机拆包后再转发给目标容器。Overlay隧道传输常见方案包括Vxlan、ipip等，目前使用Overlay隧道传输技术的主流容器网络有Flannel、Weave等。</li><li>修改主机路由：把容器网络加到主机路由表中，把主机网络设备当作容器网关，通过路由规则转发到指定的主机，实现容器的三层互通。目前通过路由技术实现容器跨主机通信的网络如Flannel host-gw、Calico等。</li></ul></li></ol><p>本小节我们就先来看看这些主流解决方案的核心思想，下一篇笔记kubernetes的网络模型我们将会仔细介绍这些主流解决方案的实现原理。</p></div></div></div></article><script src="https://morven.life/js/highlight.pack.js"></script><script src="https://unpkg.com/quicklink@0.1.1/dist/quicklink.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@10.19.0/dist/lazyload.min.js"></script><script>var lazyImage = new LazyLoad({
    container: document.getElementById('article')
  });</script><script>hljs.initHighlightingOnLoad();
  
  var posts = document.getElementById('posts-list');
  posts && quicklink({
    el: posts,
    priority: true,
  });</script></body></html>