<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>note on Morven's Life</title><link>https://morven.life/categories/note/</link><description>Recent content in note on Morven's Life</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 10 Apr 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://morven.life/categories/note/index.xml" rel="self" type="application/rss+xml"/><item><title>浅聊 Kubernetes 网络模型</title><link>https://morven.life/posts/networking-6-k8s-summary/</link><pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate><guid>https://morven.life/posts/networking-6-k8s-summary/</guid><description>通过前面的一些列笔记，我们对各种容器网络模型的实现原理已经有了基本的认识，然而真正将容器技术发扬光大的是Kubernetes容器编排平台。Kubernetes通过整合规模庞大的容器实例形成集群，这些容器实例可能运行在异构的底层网络环境中，如何保证这些容器间的互通是实际生产环境中首要考虑的问题之一。
Kubernetes网络基本要求 Kubernetes对容器技术做了更多的抽象，其中最重要的一点是提出pod的概念，pod是Kubernetes资源调度的基本单元，我们可以简单地认为pod是容器的一种延伸扩展，从网络的角度来看，pod必须满足以下条件：
每一个Pod都有一个独特的IP地址，所有pod都在一个可以直接连通的、扁平的网络空间中 同一个pod内的所有容器共享同一个netns网络命名空间 基于这样的基本要求，我们可以知道：
同一个pod内的所有容器之间共享端口，可直接通过localhost+端口来访问 由于每个pod有单独的IP，所以不需要考虑容器端口与主机端口映射以及端口冲突问题 事实上，Kubernetes进一步确定了对一个合格集群网络的基本要求：
任意两个pod之间其实是可以直接通信的，无需显式地使用NAT进行地址的转换； 任意集群节点node与任意pod之间是可以直接通信的，无需使用明显的地址转换，反之亦然； 任意pod看到自己的IP跟别人看见它所用的IP是一样的，中间不能经过地址转换； 也就是说，必须同时满足以上三点的网络模型才能适用于kubernetes，事实上，在早期的Kubernetes中，并没有什么网络标准，只是提出了以上基本要求，只有满足这些要求的网络才可以部署Kubernetes，基于这样的底层网络假设，Kubernetes设计了pod-deployment-service的经典三层服务访问机制。直到1.1发布，Kubernetes才开始采用全新的CNI(Container Network Interface)网络标准。
CNI 其实，我们在前面介绍容器网络的时候，就提到了CNI网络规范，CNI相对于CNM(Container Network Model)对开发者的约束更少，更开放，不依赖于Docker。事实上，CNI规范确实非常简单，详见：https://github.com/containernetworking/cni/blob/master/SPEC.md
实现一个CNI网络插件只需要一个配置文件和一个可执行文件：
配置文件描述插件的版本、名称、描述等基本信息 可执行文件会被上层的容器管理平台调用，一个CNI可执行文件自需要实现将容器加入到网络的ADD操作以及将容器从网络中删除的DEL操作（以及一个可选的VERSION查看版本操作） Kubernetes使用CNI网络插件的基本工作流程：
kubelet先创建pause容器生成对应的netns网络命名空间 根据配置调用具体的CNI插件，可以配置成CNI插件链来进行链式调用 当CNI插件被调用时，它根据环境变量以及命令行参数来获得网络命名空间netns、容器的网络设备等必要信息，然后执行ADD操作 CNI插件给pause容器配置正确的网络，pod中其他的容器都是用pause容器的网络 如果不清楚什么是pause容器，它在pod中处于什么样的位置，请查看之前的笔记：https://morven.life/notes/from-container-to-pod/
pod网络模型 要了解kubernetes网络模型的实现原理，我们就要从单个pod入手，事实上，一旦熟悉了单个pod的网络模型，就会发现kubernetes网络模型基本遵循和容器网络模型一样的原理。
通过前面的笔记从docker容器到pod，我们知道pod启动的时候先创建pause容器生成对应的netns网络命名空间，然后其他容器共享pause容器创建的网络命名空间。而对于单个容器的网络模型我们之前也介绍过，主要就是通过docker0网桥设备与veth设备对连接不同的容器网络命名空间，由此，我们可以得到如下图所示的单个pod网络模型的创建过程：
可以看到，同一个pod里面的其他容器共享pause容器创建的网络命名空间，也就是说，所有的容器共享相同的网络设备，路由表设置，服务端口等信息，仿佛是在同一台机器上运行的不同进程，所以这些容器之间可以直接通过localhost与对应的端口通信；对于集群外部的请求，则通过docker0网桥设备充当的网关，同时通过iptables做地址转换。到这里我们就会发现，这其实就是对单个容器的bridge网络模型的扩展。
主流kubernetes网络方案 上一小节我们知道单个pod的网络模型是容器网络模型的扩展，但是pod与pod之间的是怎么相互通信的呢？这其实与容器之间相互通信非常类似，也分为同一个主机上的pod之间与跨主机的pod之间两种。
如容器网络模型一样，对于统一主机上的pod之间，通过docker0网桥设备直接二层（数据链路层）网络上通过MAC地址直接通信：
而跨主机的pod之间的相互通信也主要有以下两个思路：
修改底层网络设备配置，加入容器网络IP地址的管理，修改路由器网关等，该方式主要和SDN(Software define networking)结合。
完全不修改底层网络设备配置，复用原有的underlay平面网络，解决容器跨主机通信，主要有如下两种方式:
隧道传输(Overlay)： 将容器的数据包封装到原主机网络的三层或者四层数据包中，然后使用主机网络的IP或者TCP/UDP传输到目标主机，目标主机拆包后再转发给目标容器。Overlay隧道传输常见方案包括Vxlan、ipip等，目前使用Overlay隧道传输技术的主流容器网络有Flannel等； 修改主机路由：把容器网络加到主机路由表中，把主机网络设备当作容器网关，通过路由规则转发到指定的主机，实现容器的三层互通。目前通过路由技术实现容器跨主机通信的网络如Flannel host-gw、Calico等； 下面简单介绍几种主流的方案：
Flannel是目前使用最为普遍的方案，提供了多种网络backend，它支持多种数据路径，也适合于overlay/underlay等多种场景。对于overlay的数据包封装，可以使用用户态的UDP，内核态的Vxlan(性能相对较好)，甚至在集群规模不大，且处于同一个二层域时可以采用host-gw的方式修改主机路由表； Weave工作模式与Flannel很相似的，它最早只提供了UDP（称为sleeve模式）的网络方式，后来又加上了fastpass方式（基于VxLAN），不过Weave消除了Flannel中用来存储网络地址的额外组件，自己集成了高可用的数据存储功能； Calico主要是采用了修改主机路由，节点之间采用BGP的协议去进行路由的同步。但是现实中的网络并不总是支持BGP路由的，因此Calico也支持内核中的IPIP模式，使用overlay的方式来传输数据； 下表是几种主流Kubernetes网络方案的对比：</description></item><item><title>容器网络(二)</title><link>https://morven.life/posts/networking-5-docker-multi-hosts/</link><pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate><guid>https://morven.life/posts/networking-5-docker-multi-hosts/</guid><description>上一篇我们介绍的bridge网络模型主要用于解决同一主机间的容器相互访问以及容器对外暴露服务的问题，并没有涉及到怎么解决跨主机容器之间互相访问的问题。
对于跨主机容器间的相互访问问题，我们能想到的最直观的解决方案就是直接使用宿主机host网络，这时，容器完全复用复用宿主机的网络设备以及协议栈，容器的IP就是主机的IP，这样，只要宿主机主机能通信，容器也就自然能通信。但是这样，为了暴露容器服务，每个容器需要占用宿主机上的一个端口，通过这个端口和外界通信。所以，就需要手动维护端口的分配，而且不能使不同的容器服务运行在一个端口上，正因为如此，这种容器网络模型很难被推广到生产环境。
因此解决跨主机通信的可行方案主要是让容器配置与宿主机不一样的IP地址，往往是在现有二层或三层网络之上再构建起来一个独立的overlay网络，这个网络通常会有自己独立的IP地址空间、交换或者路由的实现。但是由于容器有自己独立配置的IP地址，underlay平面的底层网络设备如交换机、路由器等完全不感知这些IP的存在，也就导致容器的IP不能直接路由出去实现跨主机通信。
为了解决容器独立IP地址间的访问问题，主要有以下两个思路：
修改底层网络设备配置，加入容器网络IP地址的管理，修改路由器网关等，该方式主要和SDN(Software define networking)结合。
完全不修改底层网络设备配置，复用原有的underlay平面网络，解决容器跨主机通信，主要有如下两种方式:
隧道传输(Overlay)： 将容器的数据包封装到宿主机网络的三层或者四层数据包中，然后使用宿主机的IP或者TCP/UDP传输到目标主机，目标主机拆包后再转发给目标容器。Overlay隧道传输常见方案包括VxLAN、ipip等，目前使用Overlay隧道传输技术的主流容器网络有Flannel等。 修改主机路由：把容器网络加到宿主机网络的路由表中，把宿主机网络设备当作容器网关，通过路由规则转发到指定的主机，实现容器的三层互通。目前通过路由技术实现容器跨主机通信的网络如Flannel host-gw、Calico等。 技术术语 在开始之前，我们总结一些在容器网络的介绍文章里面看到各种技术术语：
IPAM: IP Address Management，即IP地址管理。IPAM并不是容器时代特有的词汇，传统的标准网络协议比如DHCP其实也是一种IPAM，负责从MAC地址分发IP地址；但是到了容器时代我们提到IPAM，我们特指为每一个容器实例分配和回收IP地址，保证一个集群里面的所有容器都分配全局唯一的IP地址；主流的做法包括：基于CIDR的IP地址段分配地或精确为每一个容器分配IP。 Overlay：在容器时代，就是在主机现有二层（数据链路层）或三层（IP网络层）基础之上再构建起来一个独立的网络，这个overlay网络通常会有自己独立的IP地址空间、交换或者路由的实现。 IPIP: 一种基于Linux网络设备TUN实现的隧道协议，允许将三层（IP）网络包封装在另外一个三层网络包之上发送和接收，详情请看之前IPIP隧道的介绍笔记。 IPSec: 跟IPIP隧道协议类似，是一个点对点的一个加密通信协议，一般会用到Overlay网络的数据隧道里。 VxLAN：最主要是解决VLAN支持虚拟网络数量（4096）过少的问题而由VMware、Cisco、RedHat等联合提出的解决方案。VxLAN可以支持在一个VPC(Virtual Private Cloud)划分多达1600万个虚拟网络。 BGP: 主干网自治网络的路由协议，当代的互联网由很多小的AS自治网络(Autonomous system)构成，自治网络之间的三层路由是由BGP实现的，简单来说，通过BGP协议AS告诉其他AS自己子网里都包括哪些IP地址段，自己的AS编号以及一些其他的信息。 SDN: Software-Defined Networking，一种广义的概念，通过软件方式快速配置网络，往往包括一个中央控制层来集中配置底层基础网络设施。 Docker原生overlay Docker原生支持overlay网络来解决容器间的跨主机通信问题，事实上，对于Docker原生支持的overlay网络，Laurent Bernaille在DockerCon 2017上详细剖析了它的实现原理，甚至还有从头开始一步步实现Docker的overlay网络的实践教程，这三篇文章为：
Deep dive into docker overlay networks part 1 Deep dive into docker overlay networks part 2 Deep dive into docker overlay networks part 3 所以在这里我就只是大致介绍一下Docker原生支持的overlay网络模型的大致原理：</description></item><item><title>容器网络(一)</title><link>https://morven.life/posts/networking-4-docker-sigle-host/</link><pubDate>Sun, 08 Mar 2020 00:00:00 +0000</pubDate><guid>https://morven.life/posts/networking-4-docker-sigle-host/</guid><description>所谓容器网络需要结局的两大核心问题是：
容器IP地址的管理 容器之间的相互通信 其中，容器IP地址的管理包括容器IP地址的分配与回收，而容器之间的相互通信包括同一主机容器之间和跨主机容器之间通信两种场景。这两个问题也不能完全分开来看，因为不同的解决方案往往要同时考虑以上两点。容器网络的发展已经相对成熟，这篇笔记先对主流容器网络模型做一些概述，然后将进一步对典型的容器网络模型展开实践。
CNM vs CNI 关于容器网络，Docker与Kubernetes分别提出了不同的规范标准：
Docker采用的CNM(Container Network Model) Kunernetes支持的CNI模型(Container Network Interface) CNM基于libnetwork，是Docker内置的模型规范，它的总体架构如下图所示：
可以看到，CNM规范主要定义了以下三个组件：
Sandbox: 每个Sandbox包一个容器网络栈(network stack)的配置：容器的网口、路由表和DNS设置等，Sanbox可以通过Linux网络命名空间netns来实现 Endpoint: 每个Sandbox通过Endpoint加入到一个Network里，Endpoint可以通过Linux虚拟网络设备veth对来实现 Network: 一组能相互直接通信的Endpoint，Network可以通过Linux网桥设备bridge，VLAN等实现 可以看到，底层实现原理还是我们之前介绍过的Linux虚拟网络设备，网络命名空间等。CNM规范的典型场景是这样的：用户可以创建一个或多个Network，一个容器Sandbox可以通过Endpoint加入到一个或多个Network，同一个Network中容器Sanbox可以通信，不同Network中的容器Sandbox隔离。这样就可以实现从容器与网络的解耦，也就是锁，在创建容器之前，可以先创建网络，然后决定让容器加入哪个网络。
但是，为什么Kubernetes没有采用CNM规范标准，而是选择CNI，感兴趣的话可以去看看Kubernetes的官方博客文章Why Kubernetes doesn’t use libnetwork，总的来说，不使用CNM最关键的一点是，是因为Kubernetes考虑到CNM在一定程度上和container runtime耦合度太高，因此以Kubernetes为领导的其他一些组织开始制定新的CNI规范。CNI并不是Docker原生支持的，它是为容器技术设计的通用型网络接口，因此CNI接口可以很容易地从高层向底层调用，但从底层到高层却不是很方便，所以一些常见的CNI插件很难在Docker层面激活。但是这两个模型全都支持插件化，也就是说我们每个人都可以按照这两套网络规范来编写自己的具体网络实现。
docker通过libnetwork原生支持的网络模式可以通过docker network ls来看：
# docker network ls NETWORK ID NAME DRIVER SCOPE f559b082c95f bridge bridge local 5f11ccbbf488 host host local 97aedfe8792d none null local 可以看到三种网络模型，在创建容器的时候可以通过--network来指定要使用的模型。其中bridge是默认的网络模型，我们接下来将会介绍并模拟实现bridge模型；none网络模型不创建任何网络，host网络模型即使用主机网络，它不会创建新的netns网络命名空间。
Note: 如果打开了docker swarm，那么你还会看到overlay网络模型，后面我们会详细介绍docker原生overlay网络模型的实现原理。
bridge网络 bridge桥接网络是docker默认的网络模型，如果我们在创建容器的时候不指定网络模型，则默认使用bridge。bridge网络模型可以解决单宿主机上的容器之间的通信以及容器服务对外的暴露，实现原理也很简单：
可以看到，bridge网络模型主要依赖于大名鼎鼎的docker0网桥以及veth虚拟网络设备对实现，通过之前笔记对于linux虚拟网络设备的了解，我们知道veth设备对对于从一个veth设备发出的数据包，会直接发送到另一端的veth设备上，即使不在一个netns网络命名空间中，所以将veth设备对实际上是连接不同netns网络命名空间的”网线”，docker0网桥设备充当不同容器网络的网关。事实上，我们一旦而当以bridge网络模式创建容器时，会自动创建相应的veth设备对，其中一端连接到docker0网桥，另外一端连接到容器网络的eth0虚拟网卡。
首先我们在安装了docker的宿主机上查看网桥设备docker0和路由规则：</description></item><item><title>揭秘 IPIP 隧道</title><link>https://morven.life/posts/networking-3-ipip/</link><pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate><guid>https://morven.life/posts/networking-3-ipip/</guid><description>上一篇笔记中，我们在介绍网络设备的时候了解了一种典型的通过TUN/TAP设备来实现VPN的原理，但是并没有实践TUN/TAP虚拟网络设备具体在linux中怎么发挥实际功能的。这篇笔记我们就来看看在云计算领域中一种非常典型的IPIP隧道如何基于TUN设备来实现。
IPIP隧道 上一篇笔记中我们也提到了，TUN网络设备能将三层（IP）网络包封装在另外一个三层网络包之中，看起来通过TUN设备发送出来的数据包会像会这样：
MAC: xx:xx:xx:xx:xx:xx IP Header: &amp;lt;new destination IP&amp;gt; IP Body: IP: &amp;lt;original destination IP&amp;gt; TCP: stuff HTTP: stuff 这就是典型的IPIP数据包的结构。Linux原生支持好几种不同的IPIP隧道类型，但都依赖于TUN网络设备，我们可以通过命令ip tunnel help来查看IPIP隧道的相关类型以及操作：
# ip tunnel help Usage: ip tunnel { add | change | del | show | prl | 6rd } [ NAME ] [ mode { ipip | gre | sit | isatap | vti } ] [ remote ADDR ] [ local ADDR ] [ [i|o]seq ] [ [i|o]key KEY ] [ [i|o]csum ] [ prl-default ADDR ] [ prl-nodefault ADDR ] [ prl-delete ADDR ] [ 6rd-prefix ADDR ] [ 6rd-relay_prefix ADDR ] [ 6rd-reset ] [ ttl TTL ] [ tos TOS ] [ [no]pmtudisc ] [ dev PHYS_DEV ] Where: NAME := STRING ADDR := { IP_ADDRESS | any } TOS := { STRING | 00.</description></item><item><title>从 container 到 pod</title><link>https://morven.life/posts/from-container-to-pod/</link><pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate><guid>https://morven.life/posts/from-container-to-pod/</guid><description>很多人应该像我一样，第一次接触 docker 的概念，都会见到或者听过下面这句话：
docker 技术比虚拟机技术更为轻便、快捷，docker 容器本质上是进程
甚至我们或多或少都在潜移默化中接受了 container 实现是基于 linux 内核中 namespace 和 cgroup 这两个非常重要的特性。那么，namespace 和 cgroup 到底是怎么来隔离 docker 容器进程的呢？今天我们就来一探究竟。
container 在了解 docker 容器进程怎么隔离之前，我们先来看看 linux 内核中的 namespace 和 cgroup 到底是什么。
namespace 如果让我们自己实现一种类似于 vm 一样的虚拟技术，我们首先会想到的是怎么解决每个 vm 的进程与宿主机进程的隔离问题，防止进程权限“逃逸”。2008年发布的 linux 内核v2.6.24带来的命名空间（namespace）特性使得我们可以对 linux 做各种各样的隔离。
熟悉 chroot 命令的同学都应该大体能猜到 linux 内核中的 namespace 是如何发挥作用的，在 linux 系统中，系统默认的目录结构都是以根目录 / 开始的，chroot 命令用来以指定的位置作为根目录来运行指令。与此类似，了解 linux 启动流程的同学都知道在 linux 启动的时候又一个 pid 为1的 init 进程，其他所有的进程都由此进程衍生而来，init 和其他衍生而来的进程形成以 init 进程为根节点树状结构，在同一个进程树里的进程只要有足够的权限就可以审查甚至终止其他进程。这样，显然会带来安全性问题。而 pid namespace（linux namespace 的一种）允许我们创建单独的进程树，新进程树里面有自己的 pid 为1的进程，该进程一般为创建新进程树的时候指定。pid namespace 使得不同进程树里面的进程不能相互直接访问，提供了进程间的隔离，甚至可以创建嵌套的进程树：</description></item><item><title>Linux 虚拟网络设备</title><link>https://morven.life/posts/networking-2-virtual-devices/</link><pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate><guid>https://morven.life/posts/networking-2-virtual-devices/</guid><description>随着容器逐步取代虚拟机，成为现在云基础架构的标准，这些容器的网络管理模块都离不开 Linux 虚拟网络设备。事实上，了解常用的 Linux 虚拟网络设备对于我们理解容器网络以及其他依赖于容器的基础网络架构实现都大有裨益。现在开始，我们就来看看常见的 Linux 虚拟网络设备有哪些以及它们的典型使用场景。
虚拟网络设备 通过上一篇笔记我们也知道，网络设备的驱动程序并不直接与内核中的协议栈交互，而是通过内核的网络设备管理模块作为中间桥梁。这样做的好处是，驱动程序不需要了解网络协议栈的细节，协议栈也不需要针对特定驱动处理数据包。
对于内核网络设备管理模块来说，虚拟设备和物理设备没有区别，都是网络设备，都能配置 IP 地址，甚至从逻辑上来看，虚拟网络设备和物理网络设备都类似于管道，从任意一端接收到的数据将从另外一端发送出去。比如物理网卡的两端分别是协议栈与外面的物理网络，从外面物理网络接收到的数据包会转发给协议栈，相反，应用程序通过协议栈发送过来的数据包会通过物理网卡发送到外面的物理网络。但是对于具体将数据包发送到哪里，怎么发送，不同的网络设备有不同的驱动实现，与内核设备管理模块以及协议栈没关系。
总的来说，虚拟网络设备与物理网络设备没有什么区别，它们的一端连接着内核协议栈，而另一端的行为是什么取决于不同网络设备的驱动实现。
TUN/TAP TUN/TAP 虚拟网络设备一端连着协议栈，另外一端不是物理网络，而是另外一个处于用户空间的应用程序。也就是说，协议栈发给 TUN/TAP 的数据包能被这个应用程序读取到，当然应用程序能直接向 TUN/TAP 发送数据包。
一个典型的使用 TUN/TAP 网络设备的例子如下图所示：
上图中我们配置了一个物理网卡，IP 为18.12.0.92，而 tun0 为一个 TUN/TAP 设备，IP 配置为10.0.0.12。数据包的流向为：
应用程序 A 通过 socket A 发送了一个数据包，假设这个数据包的目的 IP 地址是 10.0.0.22 socket A 将这个数据包丢给网络协议栈 协议栈根据本地路由规则和数据包的目的 IP，将数据包由给 tun0 设备发送出去 tun0 收到数据包之后，将数据包转发给了用户空间的应用程序 B 应用程序 B 收到数据包之后构造一个新的数据包，将原来的数据包嵌入在新的数据包（IPIP 包）中，最后通过 socket B 将数据包转发出去 Note: 新数据包的源地址变成了 tun0 的地址，而目的 IP 地址则变成了另外一个地址 18.13.0.91.
socket B 将数据包发给协议栈 协议栈根据本地路由规则和数据包的目的 IP，决定将这个数据包要通过设备 eth0 发送出去，于是将数据包转发给设备 eth0 设备 eth0 通过物理网络将数据包发送出去 我们看到发送给10.</description></item><item><title>Linux 数据包的接收与发送过程</title><link>https://morven.life/posts/networking-1-pkg-snd-rcv/</link><pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate><guid>https://morven.life/posts/networking-1-pkg-snd-rcv/</guid><description>早在农历新年之前，就构思着将近半年重拾的网络基础知识整理成成一个系列，正好赶上新冠疫情在春节假期爆发，闲来无事，于是开始这一系列的笔记。
我的整体思路是：
第一篇笔记会简单介绍 Linux 网络数据包接收和发送过程，但不涉及 TCP/IP 协议栈的细节知识； 接下来总结一下常用的 Linux 虚拟网络设备，同时会结合使用 Linux 网络工具包 iproute2 来操作这些常用的网络设备； 有了前面的基础知识，再来看看几种常用的容器网络实现原理； 最后，我们将探讨一下 k8s 平台中主流的网络模型实现； 严格来说，这种学习思路其实很“急功近利”，但是，对于不太了解网络基础知识和 Linux 内核原理的同学来说，这反而是一种很有效的方式。但是，私以为学习过程不只应该由浅入深，更应该螺旋向前迭代，温故而知新，才能获益良多。
废话不多说，我们先开始第一篇笔记的内容。
数据包的接收过程 为了简化起见，我们以一个 UDP 数据包在物理网卡上处理流程来介绍 Linux 网络数据包的接收和发送过程，我会尽量忽略一些不相关的细节。
从网卡到内存 我们知道，每个网络设备（网卡）有驱动才能工作，驱动在内核启动时需要加载到内核中。事实上，从逻辑上看，驱动是负责衔接网络设备和内核网络栈的中间模块，每当网络设备接收到新的数据包时，就会触发中断，而对应的中断处理程序正是加载到内核中的驱动程序。
下面这张图详细地展示了数据包如何从网络设备进入内存，并被处于内核中的驱动程序和网络栈处理的：
数据包进入物理网卡，如果目的地址不是该网络设备，且该网络设备没有开启混杂模式，该包会被该网络设备丢弃； 物理网卡将数据包通过 DMA 的方式写入到指定的内存地址，该地址由网卡驱动分配并初始化； 物理网卡通过硬件中断（IRQ）通知 CPU，有新的数据包到达物理网卡需要处理； 接下来 CPU 根据中断表，调用已经注册了的中断函数，这个中断函数会调到驱动程序（NIC Driver）中相应的函数； 驱动先禁用网卡的中断，表示驱动程序已经知道内存中有数据了，告诉物理网卡下次再收到数据包直接写内存就可以了，不要再通知 CPU 了，这样可以提高效率，避免 CPU 不停地被中断； 启动软中断继续处理数据包。这样做的原因是硬中断处理程序执行的过程中不能被中断，所以如果它执行时间过长，会导致 CPU 没法响应其它硬件的中断，于是内核引入软中断，这样可以将硬中断处理函数中耗时的部分移到软中断处理函数里面来慢慢处理； 内核处理数据包 上一步中网络设备驱动程序会通过触发内核网络模块中的软中断处理函数，内核处理数据包的流程如下图所示：
对上一步中驱动发出的软中断，内核中的 ksoftirqd 进程会调用网络模块的相应软中断所对应的处理函数，确切地说，这里其实是调用 net_rx_action 函数； 接下来 net_rx_action 调用网卡驱动里的 poll 函数来一个个地处理数据包； 而 poll 函数会让驱动程序读取网卡写到内存中的数据包，事实上，内存中数据包的格式只有驱动知道； 驱动程序将内存中的数据包转换成内核网络模块能识别的 skb(socket buffer) 格式，然后调用 napi_gro_receive 函数； napi_gro_receive 函数会处理 GRO 相关的内容，也就是将可以合并的数据包进行合并，这样就只需要调用一次协议栈，然后判断是否开启了 RPS；如果开启了，将会调用 enqueue_to_backlog 函数； enqueue_to_backlog 函数会将数据包放入 input_pkt_queue 结构体中，然后返回； Note: 如果 input_pkt_queue 满了的话，该数据包将会被丢弃，这个队列的大小可以通过 net.</description></item><item><title>Istio 的前世今生</title><link>https://morven.life/posts/the_history_of_istio/</link><pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_history_of_istio/</guid><description>其实要彻底了解 Istio 以及服务网格出现的背景，就得从计算机发展的早期说起。
下面这张图展示的的通信模型变种其实从计算机刚出现不久的上世纪50年代开始就得到广泛应用，那个时候，计算机很稀有，也很昂贵，人们手动管理计算机之间的连接，图中绿色的网络栈底层只负责传输电子信号和字节码：
随着计算机变得越来越普及，计算机的价格也没那么贵了，计算机之间的连接数量和相互之间通信的数据量出现了疯狂式的增长，人们越来越依赖网络系统，工程师们必须确保他们开发的服务能够满足用户的要求。于是，如何提升系统质量成为人们关注的焦点。计算机需要知道如何找到其他节点，处理同一个通道上的并发连接，与非直接连接的计算机发生通信，通过网络路由数据包、加密流量等等，除此之外，还需要流量控制机制，流量控制可以防止下游服务器给上游服务器发送过多的数据包。
于是，在一段时期内，开发人员需要在自己的代码里处理上述问题。在下面这张图的示例中，为了确保下游服务器不给其他上游服务造成过载，应用程序需要自己处理流量控制逻辑，于是网络中的流量控制逻辑和业务逻辑就混杂在一起：
幸运的是，到了上世纪60年代末，TCP/IP 协议栈的横空出世解决了可靠传输和流量控制等问题，此后尽管网络逻辑代码依然存在，但已经从应用程序里抽离出来，成为操作系统网络栈的一部分，工程师只需要按照操作系统的调用接口进行编程就可以解决基础的网络传输问题：
进入21世纪，计算机越来越普及、也越来越便宜，相互连接的计算机节点越来越多，业界出现了各种网络系统，如分布式代理和面向服务架构：
分布式为我们带来了更高层次的能力和好处，但却也带来了新的挑战。这时候工程师的重心开始转移到应用程序的网络功能上面，这时候的服务之间的对话以“消息”为传输单元，当工程师们通过网络进行调用服务时，必须能为应用程序消息执行超时、重试、确认等操作。
于是，有工程师是开始尝试使用消息主干网（messaging backbone）集中式地来提供控制应用程序网络功能的模块，如服务发现、负载均衡、重试等等，甚至可以完成诸如协议调解、消息转换、消息路由、编排等功能，因为他们觉得如果可以将这些看似同一层面的内容加入到基础设施中，应用程序或许会更轻量、更精简、更敏捷。这些需求绝对是真实的，ESB(Enterprise Service Bus) 演变并满足了这些需要。ESB 在是2005年被提出的，它的概念特别类似于计算机硬件概念里的 USB, USB 作为电脑中的标准扩展接口，可以连接各种外部设备；而 ESB 则就把路由管理、协议转换、策略控制等通用应用程序网络功能加到现有的集中式消息总线里。
这看似行得通！
可是，在实施 SOA 架构的时候，工程师们发现这种架构有点儿用力过度、矫枉过正了。集中式的消息总线往往会成为架构的瓶颈，用它来进行流量控制、路由、策略执行等并不像我们想象那么容易，加上组织结构过于复杂，强制使用专有的格式，需要业务逻辑实现路由转换和编排等功能，各个服务之间耦合度很高，在敏捷运动的时代背景下，ESB 架构已经无法跟上时代的节奏了。
在接下来的几年内，REST 革命和 API 优先的思潮孕育了微服务架构，而以 docker 为代表的容器技术和以 k8s 为代表的容器编排技术的出现促进了微服务架构的落地。事实上，微服务时代可以以 k8s 的出现节点划分为“前微服务时代”和“后微服务时代”：
“前微服务时代”基本上是微服务作为用例推动容器技术的发展，而到“后微服务时代”，特别是成为标准的 k8s 其实在驱动和重新定义微服务的最佳实践，容器和 k8s 为微服务架构的落地提供了绝佳的客观条件。
微服务架构有很多好处，比如：
快速分配计算资源 快速部署升级迭代 易于分配的存储 易于访问的边界等等 但是作为较复杂的分布式系统，微服务架构给运维带来了新的挑战。当工程师开始接尝试微服务架构，必须考虑如何进行微服务治理。狭义的“微服务治理”，关注的是微服务组件之间的连接与通讯，例如服务注册发现、东西向路由流控、负载均衡、熔断降级、遥测追踪等。
历史总是惊人的相似，面对类似的问题，第一批采用微服务架构的企业遵循着与第一代网络计算机系统类似的策略，也就是说，解决网络通信问题的任务又落在了业务工程师的肩上。
这个时候出现了看到诸如 Netflix OSS 堆栈、Twitter Finagle 以及赫赫有名的 Spring Cloud 这样的框架和类库帮助业务工程师快速开发应用程序级别的网络功能，只需要写少量代码，就可以把服务发现、负载均衡、路由管理、遥测收集、监控告警等这些功能实现：
但是如果仔细想一下的话，就会发现这样编写微服务程序的问题也很明显。
这些类库或者框架是特定语言编写的，并且混合在业务逻辑中（或在整个基础设施上层分散的业务逻辑中）。姑且不说类库和框架的学习成本和门槛，我们知道微服务架构问世的一个承诺就是不同的微服务可以采用不同的编程语言来编写，可是当你开始编写代码的时候会发现有些语言还没有提供对应的类库。这是一个尴尬的局面！这个问题非常尖锐，为了解决这个问题，大公司通常选择就是统一编程语言来编写微服务代码。另外的问题是，怎么去做框架升级？框架不可能一开始就完美无缺，所有功能都齐备，没有任何 BUG。升级一般都是逐个版本递进升级，一旦出现客户端和服务器端版本不一致，就要小心维护兼容性。实际上，每做出一次变更都需要进行深度的集成和测试，还要重新部署所有的服务，尽管服务本身并没有发生变化。
与网络协议栈一样，工程师们急切地希望能够将分布式服务所需要的一些特性放到底层的平台中。这就像工程师基于 HTTP 协议开发非常复杂的应用，无需关心底层 TCP 如何传输控制数据包。在开发微服务时也是类似的，业务工程师们聚焦在业务逻辑上，不需要浪费时间去编写服务基础设施代码或管理系统用到的软件库和框架。把这种想法囊括到之前架构中，就是下边这幅图所描述的样子：</description></item><item><title>一份好吃的 Istio 入门餐</title><link>https://morven.life/posts/getting_started_istio/</link><pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate><guid>https://morven.life/posts/getting_started_istio/</guid><description>前两天 Istio 正式发布 1.2 版本，至此，Istio 社区再次回归每3个月一个大版本加每月一个小版本更新的传统。从 Istio 1.0发布以来，社区在对改进 Istio 一些非功能性的需求（例如性能与扩展性）方面的努力是大家有目共睹的。我觉得是时候写一篇通俗易懂的 Istio 入门文章，让更多的人去体验一下 Istio 在云原生时代所带来的各种益处。
为什么需要 Istio 说到 Istio，就不得不提到另外一个名词：Service Mesh，中文名为“服务网格”。
相信很多人对于传统的单体应用以及它所面临的问题很熟悉，一种显而易见的方案是将其分解为多个微服务。这确实能简化微服务的开发，但是不得不说，也带来非常多的挑战，比如对于成百上千的微服务的通信、监控以及安全性的管理并不是一件简单的事。目前为止，针对这些问题确实有一些解决方案（比如 Spring Cloud），但基本都是通过类似于类库或者定制化脚本的方式将为服务串联在一起，附带很高的维护升级成本。
Service Mesh 的出现正是为了解决这一问题。它是位于底层网络与上层服务之间的一层基础设施，提供了一种透明的、与编程语言无关的方式，使网络配置、安全配置以及遥测等操作能够灵活而简便地实现自动化。从本质上说，它解耦了服务的开发与运维工作。如果你是一名开发者，那么部署升级服务的时候不需要担心这些操作会对整个分布式系统带来哪些运维层面的影响；与之对应，如果你是运维人员，那么也可以放心大胆地进行服务之间的运维结构进行变更，而不需要修改服务的源代码。
而 Istio 真正地将 Service Mesh 的概念发扬光大。它创新性地将控制平面（Control Plane）与数据平面（Data Plane）解耦，通过独立的控制平面可以对 Mesh 获得全局性的可观测性（Observability）和可控制性（Controllability），从而让 Service Mesh 有机会上升到平台的高度，这对于对于希望提供面向微服务架构基础设施的厂商，或是企业内部需要赋能业务团队的平台部门都具有相当大的吸引力。
为什么会需要这样的设计呢？
我们先来看一个单独的一个微服务，正常情况下不管单个微服务部署在一个物理机上，亦或一个容器里，甚至一个 k8s 的 pod 里面，它的基本数据流向不外乎所有的 Inbound 流量作为实际业务逻辑的输入，经过微服务处理的输出作为 Outbound 流量：
随着应用的衍进与扩展，微服务的数量快速增长，整个分布式系统变得“失控”，没有一种通用可行的方案来监控、跟踪这些独立、自治、松耦合的微服务组件之间的通信；对于服务的发现和负载均衡，虽说像 k8s 这样的平台通过提供 apiserver 与 kube-proxy 下发 iptables 规则来实现了基本的部署、升级和有限的运行流量管理能力，但是本质上 k8s 提供的滚动升级都是依赖于 replicas 实例数的伸缩来调整，没办法实现按照版本的灰度发布或者金丝雀发布，不能满足高并发应用下高级的服务应用层特性；至于更加复杂的熔断、限流、降级等需求。
看起来不通过应用侵入式编程几乎不可能实现。
但是，真的是这样吗？我们来换一个思路，想象一下，如果我们在部署微服务的同时部署一个 sidecar，这个 sidecar 充当代理的功能，它会拦截所有流向微服务业务逻辑的流量，做一些预处理（解包然后抽取、添加或者修改特定字段再封包）之后在将流量转发给微服务，微服务处理的输出也要经过 sidecar 拦截做一些后处理（解包然后抽取、删除或者修改特定字段再封包），最后将流量分发给外部：
随着微服务数量的增长，整个分布式应用看起来类似于这样，可以看到所有的 sidecar 接管了微服务之间的通信流量：</description></item><item><title>MacBookPro 开启 HiDPI</title><link>https://morven.life/posts/enable_hidpi_for_external_monitor/</link><pubDate>Fri, 12 Apr 2019 00:00:00 +0000</pubDate><guid>https://morven.life/posts/enable_hidpi_for_external_monitor/</guid><description>上周旧笔记本（2015-Mid MacBookPro）由于自己的疏忽导致背包内水杯漏水而浸液，导致开机黑屏，基本无法使用。因为是主力机，所以无奈只能硬着头皮换了最新款的 MacBookPro（2018-Mid）。不换不知道，原本以为很快的数据和配置迁移消耗了我一整晚的时间。可能是因为近些年来苹果品控的下降，对于最新 MacBookPro 没多少好感，拿到新本后令人发狂的蝶式键盘加上鸡肋的 TouchBar，让新旧本之间的过渡期再次延长，于是决定还是继续使用外接键盘和鼠标，并将自己之前的显示器作为主显示器。
将外接显示器连接上之后，很快就会发现整体显示模糊，即使4K显示屏也不能达到期望的 Retina 显示效果。网上搜索一番之后才发现原因是新本没有开启 HiDPI。
何为 HiDPI 我们知道，高分辨率意味着更小的字体和图标，而 HiDPI 可以用软件的方式实现单位面积内的高密度像素。通过开启 HiDPI 渲染，可以在保证分辨率不变的情况下，使得字体和图标变大。一句话概括就是就是：
高 PPI(硬件) + HiDPI 渲染(软件) = 更细腻的显示效果(Retina) 如何开启 HiDPI 关于如何开启 HiDPI，Google 搜索之后会有很多方案，但是因为系统的不断升级，有的不够全面，有的过于繁琐。在此针对我目前的笔记本（MacBookPro 2018-Mid）给出一个相对简洁的方案。主要包含三个步骤：
关闭 SIP 终端命令 开启 SIP 备份 实际的操作的过程会更改部分系统文件，因此在操作前要确保对文件进行备份：
打开终端并进入到 /System/Library/Displays/Contents/Resources 拷贝 Overrides 文件夹到其他目录一份 关闭 SIP Note: 关闭 SIP 有风险，确保所有操作完整之后再次打开 SIP，否则对系统文件的保护将不存在
打开终端并输入 csrutil status，如果结果为 enabled 则表明 SIP 为开启状态 关机之后再按电源键后长按 command + R 直至出现苹果 LOGO 在 Utils-&amp;gt;Terminal 打开终端，并输入 csrutil disable，之后关掉终端 重启并正常开机 打开终端并输入 csrutil status，确保结果是 disable 运行设置脚本 Note: 为了简化配置过程，我使用了 Github 上开源的自动脚本来开启 HiDPI，如果想手动来配置，请访问：https://comsysto.</description></item><item><title>Go 模块化编程</title><link>https://morven.life/posts/the_go_module/</link><pubDate>Tue, 26 Feb 2019 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_go_module/</guid><description>Go 语言在去年八月份发布的1.11版本中增加了对模块化编程的支持以及内置的基于模块的依赖管理工具。Go 语言的模块（module）是文件树中的包（package）的集合，其中模块根目录包含的 go.mod 文件定义了模块的导入路径（import path）、Go 语言的版本以及模块的其他依赖性要求。每个模块的依赖性要求被列为单独的一个模块路径并指定相应的模块版本，只有满足了所有依赖性要求的模块才能被成功构建。
使用 Go 语言自带的模块化编程能力，就不需要将 Go 语言的代码放入到 $GOPATH/src 中，也就是过去的 GOPATH 模式，实际上，我们可以在 $GOPATH/src 外的任何目录下使用 go mod init 创建 Go 项目并初始化 Go 模块。
Note: 为了兼容性，在 Go 语言1.11与1.12中，Go 命令仍然可以在旧的 GOPATH 模式下运行，从 Go 语言1.13开始，模块模式(GO111MODULE=on)将成为默认模式。
GOPATH 的前世今生 而 Go 语言支持模块化编程之前，一般我们的 Go 项目使用需要使用 GOPATH 模式，也就是说需要将 Go 语言的代码放入到 $GOPATH/src 中。典型的 GOPATH 目录结构包含必须包含以下三个子文件夹：
GOPATH ├── bin // binaries ├── pkg // cache |── src // go source code ├── github.com ├── rsc.</description></item><item><title>创建最小 Docker 镜像</title><link>https://morven.life/posts/build_the_smallest_possible_docker_image/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://morven.life/posts/build_the_smallest_possible_docker_image/</guid><description>如果你熟悉 docker，你可能知道 docker 镜像存储使用 Union FS 的分层存储技术。在构建一个 docker 镜像时，会一层一层构建，前一层是后一层的基础，每一层构建完成之后就不会再改变。正是因为这一点，我们在构建 docker 镜像的时候，要特别小心，每一层尽量只包含需要的东西，构建应用额外的东西尽量在构建结束的时候删除。举例来说，比如你在构建一个 Go 语言编写的简单应用程序的时候，原则上只需要一个 Go 编译出来的二进制文件，没有必要保留构建的工具以及环境。
docker 官方提供了一个特殊的空镜像 scratch，使用这个镜像意味着我们不需要任何的已有镜像为基础，直接将我们自定义的指令作为镜像的第一层。
FROMscratch...实际上，我们可以创建自己的 scratch 镜像：
$ tar cv --files-from /dev/null | docker import - scratch 那么，问题来了，我们可以使用 scratch 镜像为基础制作哪些镜像呢？答案是所有不需要任何依赖库的可执行文件都可以以 scratch 为基础镜像来制作。具体来说，对于 linux 下静态编译的程序来说，并不需要操作系统提供的运行时支持，所有需要的一切都已经在可执行文件中包含了，比如使用 Go 语言开发的很多应用会使用直接 FROM scratch 的方式制作镜像，这样最终的镜像体积非常小。
下面是一个简单的 Go 语言开发的 web 程序代码：
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;net/http&amp;#34; ) func main() { http.HandleFunc(&amp;#34;/&amp;#34;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, &amp;#34;Hello, you&amp;#39;ve requested: %s\n&amp;#34;, r.URL.Path) }) http.</description></item><item><title>Docker 知识点拾遗</title><link>https://morven.life/posts/the_knowledge_of_docker/</link><pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_knowledge_of_docker/</guid><description>Docker 是一个划时代的产品，它彻底地释放了计算机虚拟化的威力，极大的提高了应用的部署、测试和分发的效率。虽然我们几乎每天都在使用 docker，但还是有一些很容易被忽略得的 docker 知识点，今天，我们就集中起来看看。
容器与传统虚拟机 经常有人说“ docker 是一种性能非常好的虚拟机”，这种说法是错误的。docker 相比于传统虚拟机的技术来说更为轻便，具体表现在 docker 不是在宿主机上虚拟出一套硬件并运行完整的操作系统，然后再在其上运行所需的应用进程，而是直接在 docker 容器里面的进程直接运行在宿主的内核中，docker 会做文件系统、网络以及进程隔离等，容器不用进行硬件和内核的虚拟。这样一来 docker 会相对于传统虚拟机来说“体积更轻、跑的更快，相同宿主机下可创建的数量更多”。
docker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 systemd 去启动后台服务，容器内没有后台服务的概念。举个例子，常有人在 dockerfile 里面这样写：
CMD service nginx start然后发现容器执行后就立即退出了，甚至在容器内去使用 systemctl 命令来管理服务，结果却发现根本执行不了。这就是没有区分容器和虚拟机的差异，依旧以传统虚拟机的角度去理解容器。对于 docker 容器而言，其启动程序就是容器的应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，其它辅助进程不是它需要关心的东西。而使用 CMD 指令 service nginx start 则是以后台守护进程形式启动 nginx 服务，事实上，service nginx start 最终会被 docker 引擎转化为 [ &amp;quot;sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;service nginx start&amp;quot;] 命令，因此主进程实际上是 sh。那么当 service nginx start 命令结束后， sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。
正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行：
CMD [&amp;#34;nginx&amp;#34;, &amp;#34;-g&amp;#34;, &amp;#34;daemon off;&amp;#34;]分层存储 我们知道完整的操作系统由内核空间和用户空间组成。从存储的角度来看，内核空间主要是指需要引导程序加载和启动的内核程序，用户空间的核心则是 rootfs 。而 docker 容器启动并不会虚拟出新内核，而是共享宿主机的内核，所以对于 docker 镜像与容器而言，我们主要关注的存储结构是 rootfs 。不同 Linux 发行版的主要区别也是 rootfs 。比如以前的 Ubuntu 使用 UpStart 系统管理服务，apt 管理软件包；而 CentOS 使用 systemd 和 yum ，但是这些都是在用户空间上的区别，Linux 内核的差别不大。</description></item><item><title>Go 反射机制</title><link>https://morven.life/posts/the_go_reflection/</link><pubDate>Wed, 12 Sep 2018 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_go_reflection/</guid><description>很多语言都支持反射，Go 语言也不例外。那么什么是反射呢？总的来说，反射就是计算机程序语言可以在运行时动态访问、检查和修改任意对象本身状态和行为的能力。不同语言的反射特性有着不同的工作方式，有些语言还不支持反射特性。我们今天主要来看看在 Go 语言中，反射是怎么工作的。
在开始之前，先安利一篇 Go 官方出品的介绍反射机制的博客：The Laws of Reflection
这篇笔记是我个人对于 Go 官方博客关于反射机制的总结与补充，强烈食用本文之前去看看官方博客。
反射的适用场景 需要说明的是，Go 语言是一门静态类型的编译型语言，也就是在编译过程中，编译器就能发现一些类型错误，但是它并不能发现反射相关代码的错误，这种错误往往需要在运行时才能被发现，所以一旦反射代码出错，将直接导致程序 panic 而退出。此外，反射相关代码的可读性往往较差，执行效率也比正常 Go 语言代码低不少。综上，除非以下特殊情况必须使用反射外，我们应经量避免使用 Go 语言的反射特性。
一般来说，适用于反射的场景主要包括：
某段函数或者方法需要处理的数据类型不确定，会包含一些列的可能类型，这个时候我们就需要使用反射动态获取要处理的数据类型，基于数据类型的不同调用不同的处理程序。一个非常典型的例子是我们通过反序列化得到一个类型为 map[string]interface{} 的数据结构，然后通过反射机制递归地获取内部每个字段的类型。
在某些场景下我们需要根据特定的条件决定调用哪个函数，也就是说需要在运行期间获取函数以及函数运行所需的参数来动态地调用函数。典型的例子是现在主流的 RPC 框架的实现机制，RPC 服务器端维护函数名到函数反射值的映射，RPC 客户端通过网络传递函数名、参数列表给 RPC 服务器端后，RPC 服务器解析为反射值，调用执行函数，最后再将函数的返回值打包通过网络返回给 RPC 客户端。
Note: 当然，可能还有其他适用于反射的应用场景，这里只是罗列常用的使用场景。
反射的实现基础 首先需要明确一点，Go 反射的实现基础是类型。在 Go 语言中，每个变量都有一个静态类型，也就是在编译阶段就需要检查的类型，比如 int、string、map、struct 等。需要注意的是，这个静态类型是指变量声明的时候指定的类型，并不一定不是底层真正存储的数据类型，例如：
type MyInt int var i int var j MyInt 上面的代码中，虽然变量 i 和 j 的真正存储的数据类型都是 int，但是对于编译器来说，它们是不同的类型，要进行显示的类型转换才能相互赋值。
有的变量可能除了静态类型之外，还会有动态类型。所谓动态类型就是指变量值实际存储的类型信息，举例来说：
var r io.</description></item><item><title>Go 模板渲染</title><link>https://morven.life/posts/the_go_template/</link><pubDate>Fri, 24 Aug 2018 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_go_template/</guid><description>随着近几年 Restful 架构的盛行，前后端分离大行其道，模板渲染也由后端转移到了前端，后端只需要提供资源数据即可，这样导致类似于 JSP、PHP 等传统服务端模板脚本语言几乎问人问津了。但是在 Go 语言中，模板渲染技术不只局限于服务端标记语言（如 HTML）的渲染，GO 语言经常使用模版语言来处理譬如插入特定数据的文本转化等，虽然没有正则表达式那么灵活，但是渲染效率远优于正则表达式，而且使用起来也更简单。对于某些云计算的场景十分友好。今天，我们就来详细聊一聊 Go 语言模板渲染的技术细节。
运行机制 模板的渲染技术本质上都是一样的，一句话来说明就是字符串模板和结构化数据的结合，再详细地讲就是将定义好的模板应用于结构化的数据，使用注解语法引用数据结构中的元素（例如 Struct 中的特定字段，Map 中的键值）并显示它们的值。模板在执行过程中遍历数据结构并且设置当前光标（. 往往表示当前的作用域）标识当前位置的元素。
类似于 Python 语言的 jinja与 NodeJS 语言中的 jade等模版引擎，Go 语言模板引擎的运行机制也是类似：
创建模板对象 解析模板字串 加载数据渲染模板 Go 模板渲染核心包 Go语言提供了两个标准库用来处理模板渲染 text/template 和 html/template，它们的接口几乎一摸一样，但处理的模板数据不同。其中 text/template 用来处理普通文本的模板渲染，而 html/template 专门用来渲染格式化 HTML 字符串。
下面的例子我们使用 text/template 来处理普通文本模板的渲染：
package main import ( &amp;#34;os&amp;#34; &amp;#34;text/template&amp;#34; ) type Student struct { ID uint Name string } func main() { stu := Student{0, &amp;#34;jason&amp;#34;} tmpl, err := template.</description></item><item><title>Go 上下文</title><link>https://morven.life/posts/the_go_context/</link><pubDate>Tue, 12 Jun 2018 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_go_context/</guid><description>从 Go 语言从1.7开始，正式将 context 即“上下文”包引入官方标准库。事实上，我们在 Go 语言编程中经常遇到“上下文”，不论是在一般的服务器代码还是复杂的并发处理程序中，它都起到很重要的作用。今天，我们就来深入探讨一下它的实现以及最佳实践。
官方文档对于 context 包的解释是：
Package context defines the Context type, which carries deadlines, cancelation signals, and other request-scoped values across API boundaries and between processes.
简单来说，“上下文”可以理解为程序单元的一个运行状态、现场、快照。其中上下是指存在上下层的传递，上层会把内容传递给下层，而程序单元则指的是 Go 协程。每个 Go 协程在执行之前，都要先知道程序当前的执行状态，通常将这些执行状态封装在一个“上下文”变量中，传递给要执行的 Go 协程中。而 context 包是专门用来简化处理针对单个请求的多个 Go 协程与请求截止时间、取消信号以及请求域数据等相关操作的。一个常遇到的例子是在 Go 语言实现的服务器程序中，每个网络请求一般需要创建单独的Go 协程进行处理，这些 Go 协程有可能涉及到多个 API 的调用，进而可能会开启其他的 Go 协程；由于这些 Go 协程都是在处理同一个网络请求，所以它们往往需要访问一些共享的资源，比如用户认证令牌环、请求截止时间等；而且如果请求超时或者被取消后，所有的 Go 协程都应该马上退出并且释放相关的资源。使用上下文可以让 Go 语言开发者方便地实现这些 Go 协程之间的交互操作，跟踪并控制这些 Go 协程，并传递请求相关的数据、取消 Go 协程的信号或截止日期等。
上下文的数据结构 context 包中核心数据结构是一种嵌套的结构或者说是单向继承的结构。基于最初的上下文（也叫做“根上下文”），开发者可以根据使用场景的不同定义自己的方法和数据来继承“根上下文”；正是上下文这种分层的组织结构，允许开发者在每一层上下文中定义一些不同的特性，这种层级式的组织也使得上下文易于扩展、职责清晰。
context 包中最基础的数据结构是 Context 接口：</description></item><item><title>Go 协程与管道</title><link>https://morven.life/posts/the_go_channel/</link><pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_go_channel/</guid><description>说到 Go 语言，不得不提 Go 语言的并发编程。Go 从语言层面增加了对并发编程的良好支持，不像 Python、Java 等其他语言使用 Thread 库来新建线程，同时使用线程安全队列库来共享数据。Go 语言对于并发编程的支持依赖于 Go 语言的两个基础概念：Go 协程（Routine）和Go 管道（Channel）。
Note: 也许我们还对并发(Concurrency)和并行(Parallelism)傻傻分不清楚，在这里再次强调两者的不同点：
Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once.
也就是说，并发是在同一时间处理多件事情，往往是通过编程的手段，目的是将 CPU 的利用率提到最高；而并行是在同一时间做多件事情，需要多核 CPU 的支持。
Go 协程 Go 协程是 Go 语言并行编程的核心概念之一，是比 Thread 更轻量级的并发单元，完全处于用户态并由 Go 语言运行时管理，最小 Go 协程只需极少的栈内存(大约是4~5KB)，这样十几个Go 协程的规模可能体现在底层就是五六个线程的大小，最高同时运行成千上万个并发任务；同时，Go 语言内部实现了 Go 协程之间的内存共享使得它比 Thread 更高效，更易用，我们不必再使用类似于晦涩难用的线程安全队列库来同步数据。
创建 Go 协程 要创建一个Go 协程，我们只需要在函数调⽤语句前添加 go 关键字，Go 语言的调度器会自动将其安排到合适的系统线程上执行。
go f(x, y, z) 会启动一个新的 Go 协程并执行 f(x, y, z)。</description></item><item><title>Go 接口与组合</title><link>https://morven.life/posts/the_go_interface_and_composition/</link><pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_go_interface_and_composition/</guid><description>Go 接口与鸭子类型 什么是“鸭子类型”？
If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck.
以上引用自维基百科的解释描述了什么是“鸭子类型”，所谓鸭子类型，是动态编程语言的一种对象推断策略，它更关注对象如何被使用，而不是对象的类型本身。
传统的静态语言，如 Java、C++ 必须显示地声明类型实现了某个接口，之后，才能将类型用在任何需要这个接口的地方，否则编译都不会通过，这也是静态语言比动态语言更安全的原因。但是 Go 语言作为一门“现代”静态语言，使用的是动态编程语言的对象推断策略，它更关注对象能如何被使用，而不是对象的类型本身。也就是说，Go 语言引入了动态语言的便利，同时又会进行静态语言的类型检查，因此，它采用了折中的做法：不要求类型显示地声明实现了某个接口，只要实现了相关的方法即可，编译器就能检测到。
举个例子，下面的代码片段先定义一个接口，和使用此接口作为函数的参数：
type IGreeting interface { greeting() } func sayHello(i IGreeting) { i.greeting() } 接下来再定义两个结构体：
type A struct {} func (a A) greeting() { fmt.Println(&amp;#34;Hi, I am A!&amp;#34;) } type B struct {} func (b B) greeting() { fmt.</description></item><item><title>从零开始认识 iptables</title><link>https://morven.life/posts/the_knowledge_of_iptables/</link><pubDate>Sat, 20 May 2017 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_knowledge_of_iptables/</guid><description>在使用 Linux 的过程中，很多人和我一样经常接触 iptables，但却只知道它是用来设置 Linux 防火墙的工具，不知道它具体是怎么工作的。今天，我们就从零开始认识一下 Linux 系统下 iptables 的具体工作原理。
iptables 是 Linux 上常用的防火墙软件 netfilter 项目的一部分，所以要讲清楚 iptables，我们先理一理什么是防火墙？
什么是防火墙 简单来说，防火墙是一种网络隔离工具，部署于主机或者网络的边缘，目标是对于进出主机或者本地网络的网络报文根据事先定义好的规则做匹配检测，规则匹配成功则对相应的网络报文做定义好的处理（允许，拒绝，转发，丢弃等）。防火墙根据其管理的范围来分可以将其划分为主机防火墙和网络防火墙；根据其工作机制来区分又可分为包过滤型防火墙（netfilter）和代理服务器（Proxy）。我们接下来在这篇笔记中主要说说包过滤型防火墙（netfilter）。
Note: 也有人将 TCP Wrappers 也划分为防火墙的一种，它是根据服务程序软件的名称来处理网络数据包的工具。
包过滤型防火墙 包过滤型防火墙主要依赖于 Linux 内核软件 netfilter，它是一个 Linux 内核“安全框架”，而 iptables 是内核软件 netfilter 的配置工具，工作于用户空间。iptables/netfilter 组合就是 Linux 平台下的过滤型防火墙，并且这个防火墙软件是免费的，可以用来替代商业防火墙软件，来完成网络数据包的过滤、修改、重定向以及网络地址转换（nat）等功能。
Note: 在有些 Linux 发行版上，我们可以使用 systemctl start iptables 来启动 iptables 服务，但需要指出的是，iptables 并不是也不依赖于守护进程，它只是利用 Linux 内核提供的功能。
Linux 网络管理员通过配置 iptables 规则以及对应的网路数据包处理逻辑，当网络数据包符合这样的规则时，就做执行预先定义好的相应处理逻辑。可以简单的总结为：
IF network_pkg match rule; THEN handler FI 其中规则可以包括匹配数据报文的源地址、目的地址、传输层协议（TCP/UDP/ICMP/..）以及应用层协议（HTTP/FTP/SMTP/..）等，处理逻辑就是根据规则所定义的方法来处理这些数据包，如放行（accept），拒绝（reject），丢弃（drop）等。
而 netfilter 是工作于内核空间当中的一系列网络协议栈的钩子（hook），为内核模块在网络协议栈中的不同位置注册回调函数。也就是说，在数据包经过网络协议栈的不同位置时做相应的由 iptables 配置好的处理逻辑。 netfilter 中的五个钩子（这里也称为五个关卡）分别为：PRE_ROUTING/INPUT/FORWARD/OUTPUT/POST_ROUTING，网络数据包的流向图如下图所示：</description></item><item><title>编写健壮的 Shell 脚本</title><link>https://morven.life/posts/how-to-write-robust-shell-script/</link><pubDate>Mon, 06 Feb 2017 00:00:00 +0000</pubDate><guid>https://morven.life/posts/how-to-write-robust-shell-script/</guid><description>编写 Shell 脚本应该是程序员必须掌握的技能。因为 Shell 脚本简单易上手的特性，在日常工作中，我们经常使用它来自动化应用的测试部署、环境的搭建清理等。其实在编写运行 Shell 脚本的时候也会遇到各种坑，稍不注意就会导致 Shell 脚本因为各种原因不能正常执行。实际上，编写健壮可靠的 Shell 脚本也是有很多技巧的，今天我们就来探讨一下。
设置 Shell 的默认执行环境参数 在执行 Shell 脚本的时候，通常都会创建一个新的 Shell ，比如，当我们执行：
bash script.sh 我们指明使用 bash 会创建一个新的 Shell 来执行 script.sh ，同时给定了这个执行环境默认的各种参数。 set 命令可以用来修改 Shell 环境的运行参数，不带任何参数的 set 命令，会显示所有的环境变量和 Shell 函数。对于所有可以定制的运行参数，请查看官方手册，我们重点介绍其中最常用的四个。
跟踪命令的执行 默认情况下， Shell 脚本执行后只显示运行结果，不会展示结果是哪一行代码的输出，如果多个命令连续执行，它们的运行结果就会连续输出，导致很难分清一串结果是什么命令产生的。 set -x 用来在运行结果之前，先输出执行的那一行命令，行首以 + 表示是命令而非命令输出，同时，每个命令的参数也会展开，这样我们可以清晰地看到每个命令的运行实参，这对于 Shell 脚本的 debug 来说非常友好。
#!/bin/bash set -x v=5 echo $v echo &amp;#34;hello&amp;#34; # output: # + v=5 # + echo 5 # 5 # + echo hello # hello set -x 还有另一种写法： set -o xtrace 。</description></item><item><title>Webpack 使用小结</title><link>https://morven.life/posts/the_webpack_summary/</link><pubDate>Sun, 20 Nov 2016 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_webpack_summary/</guid><description>分而治之是软件工程领域的重要思想，对于复杂度日益增加的前端也同样适用。一般前端团队选择合适的框架之后就要开始考虑开发维护的效率问题。而模块化是目前前端领域比较流行的分而治之手段。
javascript 模块化已经有很多规范和工具，例如 CommonJS/AMD/requireJS/CMD/ES6 Module ，在上篇文章中有详细的介绍。 CSS 模块化基本要依靠 Less , Sass 以及 Stylus 等于处理器的 import/minxin 特性实现。而 html 以及 html 模版和其他资源比如图片的模块化怎么去处理呢？
这也正是 webpack 要解决的问题之一。严格来说， webpack 是一个模块打包工具，它既不像 requireJS 和 seaJS 这样的模块加载器，也不像 grunt 和 gulp 这样优化前端开发流程的构建工具，像是两类工具的集合却又远不止如此。
总的来说， Webpack 是一个模块打包工具，它将 js 、 css 、 html 以及图片等都视为模块资源，这些模块资源必然存在某种依赖关系， webpack 就是通过静态分析各种模块文件之间的依赖关系，通过不同种类的 Loader 将所有模块打包成起来。
webpack VS gulp 严格来说， gulp 与 webpack 并没有可比性。 gulp 应该和 grunt 属于同一类工具，能够优化前端工作流程，比如压缩合并 js 、css ，预编译 typescript 、 sass 等。也就是说，我们可以根据需要配置插件，就可以将之前需要手动完成的任务自动化。 webpack 作为模块打包工具，可以和 browserify 相提并论，两者都是预编译模块化解决方案。相比 requireJS 、 seaJS 这类“在线”模块化方案更加智能。因为是“预编译”，所以不需要在浏览器中加载解释器。另外，你可以直接在本地编写 js ，不管是 AMD/CMD/ES6 风格的模块化，都编译成浏览器认识的 js 。</description></item><item><title>Javascript 模块化开发</title><link>https://morven.life/posts/developing-modular-javascript/</link><pubDate>Sun, 16 Oct 2016 00:00:00 +0000</pubDate><guid>https://morven.life/posts/developing-modular-javascript/</guid><description>随着互联网时代的到来，前端技术更新速度越来越快。起初只要在 script 标签中嵌入几行代码就能实现一些基本的用户交互，到现在随着 Ajax、jQuery、MVC 以及 MVVM 的发展，Javascript 代码量变得日益庞大复杂。
网页越来越像桌面程序，需要一个团队分工协作、进度管理、单元测试等等。开发者不得不使用软件工程的方法，管理网页的业务逻辑。Javascript 模块化开发，已经成为一个迫切的需求。理想情况下，开发者只需要实现核心的业务逻辑，其他都可以加载别人已经写好的模块。
但是，Javascript 不是一种模块化编程语言，它不支持“类”（class），更别提“模块”（module）了。直到前不久 ES6 正式定稿，Javascript 才开始正式支持“类”和“模块”，但还需要很长时间才能完全投入实用。
什么是模块化 模块是任何大型应用程序架构中不可缺少的一部分，一个模块就是实现特定功能的代码区块或者文件。模块可以使我们清晰地分离和组织项目中的代码单元。在项目开发中，通过移除依赖、松耦合可以使应用程序的可维护性更强。有了模块，开发者就可以更方便地使用别人的代码，想要什么功能，就加载什么模块。模块开发需要遵循一定的规范，否则就会混乱不堪。
Javascript 社区做了很多努力，在现有的运行环境中，实现&amp;quot;模块&amp;quot;的效果。本文总结了当前 Javascript 模块化编程的最佳实践，说明如何投入实用。
Javascript 模块化基本写法 在第一部分，将讨论基于传统 Javascript 语法的模块化写法。
原始写法 模块就是实现特定功能的一组方法。只要把不同的函数（以及记录状态的变量）简单地放在一起，就算是一个模块：
function func1(){ //... } function func2(){ //... } 上面的函数 func1() 和 func2()，组成一个模块。使用的时候，直接调用就行了。这种做法的缺点很明显：&amp;ldquo;污染&amp;quot;了全局变量，无法保证不与其他模块发生变量名冲突，而且模块成员之间看不出直接关系。
对象写法 为了解决上面的缺点，可以把模块写成一个对象，所有的模块成员都放到这个对象里面：
var moduleA = new Object({ _count : 0, func1 : function (){ //... }, func2 : function (){ //... } }); 上面的函数 func1() 和func2()，都封装在 moduleA 对象里，使用的时候，就是调用这个对象的属性。</description></item><item><title>JSON Web Token</title><link>https://morven.life/posts/the_jwt_quick_start/</link><pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_jwt_quick_start/</guid><description>近几年，前后端分离大行其道，在典型的前后端分离的应用架构中，后端主要作为 Model 层，为前端提供数据访问的 API，前后端之间的通信需要在不可信（Zero Trust）的异构网络之间进行，为了保证数据安全可靠地在客户端与服务端之间传输，实现客户端认证就显得非常重要。而 HTTP 协议本身是无状态的，实现服务端的客户端认证的基础是记录客户端和服务端的对话状态。
我们最熟悉的认证客户端的方式就是基于 session/cookie 的状态记录方式，基本流程是：
客户端向服务器端发送用户名和密码 服务器验证通过后，创建新的对话（session）并保存保存相关数据，比如用户角色、登录时间等 服务器向客户端返回一个 SESSIONID，写入客户端的 Cookie 客户端随后的每一次请求，都会通过 Cookie，将 SESSIONID 传回服务器 服务器收到 SESSIONID，取出相应 session 并与保存的 session 信息进行对比，由此得知用户的身份 这种认证方式最大的问题是，没有分布式架构，无法支持横向扩展。如果使用一个服务器，该模式完全没有问题。但是，如果它是服务器群集或面向服务的跨域体系结构，则需要一个统一的 session 数据库库来保存会话数据实现共享，这样负载均衡下的每个服务器才可以正确的验证用户身份。
举例来说，某企业同时有两个不同的网站 A 和网站 B 提供服务，如何做到用户只需要登录其中一个网站，然后它就会自动登录到另一个网站？
一种解决方案是使用持久化 session 的基础设施，例如 Redis，写入 session 数据到持久层。收到新的客户端请求后，服务端从持久层查找对应的 session 信息。这种方案的优点在于架构清晰，而缺点是架构修改比较费劲，整个服务的验证逻辑层都需要重写，工作量相对较大。而且由于依赖于持久层的数据库或者类似系统，会有单点故障风险，如果持久层失败，整个认证体系都会挂掉。
有没有别的方案呢？这就是我们即将要学习的 JWT(JSON Web Token) 认证方式。
什么是 JWT JWT 另辟蹊径，基于令牌（token）认证客户端，也就是说只需要在每次客户端请求的HTTP头部附上对应的 token，服务器端负责去检查 token 的签名来确保 token 没有被篡改，这样通过客户端保存数据，而不是服务器保存会话数据，每个请求都被发送回服务器端认证。
根据官方的定义，JWT 是一套开放的标准（RFC 7519），它定义了一套简洁且安全的方案，可以在客户端和服务器之间传输 JSON 格式的 token 信息。
JWT 的工作原理 JWT 服务端认证的基本原理是在服务器身份验证之后，将生成一个 JSON 对象并将其发送回客户端，如下所示：</description></item><item><title>大话「一致性哈希」</title><link>https://morven.life/posts/consistent_hash/</link><pubDate>Mon, 24 Aug 2015 00:00:00 +0000</pubDate><guid>https://morven.life/posts/consistent_hash/</guid><description>伴随着系统规模的扩增，出现了很多分布式应用。比如在分布式系统中为了保证 Redis 的高可用需要搭建 Redis 对数据进行分槽处理，再比如 MySql 数据库存储的数据达到一定规模的时候需要对数据库分库操作。除了这些典型的分布式应用之外，比如我们要开发一款分布式作业调度系统，实际的执行作业的节点拥有不同的配置（CPU、内存以及 GPU 等），同时也分布在不同的地域，我们如何保证的作业尽可能调度到正确的节点，同时在新增节点或者某节点宕机的情况下保证尽可能减少对作业调度的影响呢？
说白了，上面所述的问题就是如何把数据对象按照一定的规则映射到不同的机器上，我们能想到的最直观的方式就是哈希函数，借助于哈希函数，我们能够准确地知道数据对象需要映射到哪个节点。但是普通的哈希函数可以解决我们在分布式系统中遇到的数据对象映射的问题吗？
简单哈希 我们还是以上面所说的分布式作业调度系统为例，联想到 Java 中哈希表 hashmap 的实现原理，通过哈希函数对作业数据对象的键来计算哈希值，最简单的哈希函数就是通过作业数据对象对作业执行节点总数执行取模运算，得到的哈希值可以看作是“哈希桶”，也就是作业应该调度到的节点。
这看起来很简单地就解决了数据对象的映射问题，但是如果随着系统吞吐量的增加，我们需要重新增加一些新的计算节点到集群中，此时我们再次计算数据对象的哈希值的时候，取模运算的底数已经发生了变化，这会导致数据对象映射的结果也发生变化，也就是说作业调度的节点发生了变化，这对于一些对配置有特殊需要的作业来说，影响是巨大的。所以，我们需要想办法让这种情况不发生，这种情况发生的根本是哈希算法本身的特性导致的，直接使用取模的话这是无法避免的。
最简单的哈希算法通过求模运算来计算关键字的哈希值，然后将关键字映射到有限的地址空间中，以便于快速检索关键字。由于简单哈希算法只是采用了求模运算，使得它存在很多缺陷:
增删节点更新效率低：如果整个系统中的存储节点数量发生变化，哈希算法的哈希函数也需要变化。例如，增加或删除一个节点是，哈希函数将变化为 hash(key)%(N±1)，这种变化使得所有关键字映射的地址空间都发生变化，也就是说，整个系统所有对象的映射地址都需要重新计算，这样系统就无法对外界请求做出正常的响应，将导致系统处于瘫痪状态； 平衡性差：采用简单哈希算法的系统没有考虑不同节点间性能的差异，导致系统平衡性差。如果新添加的节点由于硬件性能的提升具有更强的计算能力，该算法不能有效地利用节点性能差异提高系统的负载； 单调性不足：新的节点加入到系统中时，哈希函数的结果应能够保证之前的已分配映射地址的数据对象可以被映射到原地址空间。简单哈希算法不能满足单调性，一旦节点的数量发生变化，那么所有的数据对象映射的地址都会发生变化； 一致性哈希 一致性哈希算法对简单哈希算法的缺陷进行了修正，保证系统在删除或者增加一个节点时，尽量不改变已有的数据对象到地址空间的映射关系，也就是尽量满足哈希算法的单调性。
环形哈希空间 一致性哈希算法将所有数据对象的键值映射到一个32位长的地址空间值中，即 0~2^32-1 的数值空间，该地址空间首尾相连形成一个环状，即一个首(0)尾(2^32-1)相接的闭合圆环，如下面所示:
把数值对象映射到哈希空间 把所有的数值对象映射到环形哈希空间。考虑这样的4个数值对象：object1~object4，通过哈希函数计算出的哈希值作为环形地址空间中的地址，这样4个数值对象在环上的分布如图所示：
hash(object1) = key1; ...... hash(object4) = key4; 把机器节点映射到哈希空间：对于机器节点也使用相同的哈希函数映射到环形地址空间。一致性哈希对于数值对象和机器节点使用相同的哈希函数映射到同一个环形哈希空间中。然后沿着环形地址空间按照顺时针方向将所有的数据对象存储到最近的节点上： 目前存在 node A, node B, node C 共3台机器节点，通过哈希函数的计算，它们在环形地址空间上的映射结果将如图所示，可以看出这些机器节点在环形哈希空间中，基于对应的哈希值排序。
hash(node A) = key A; ...... hash(node C) = key C; Note: 对于机器节点哈希值的计算，哈希函数的输入可以选择机器的 IP 地址或者机器名的字符串签名。
从图中可以看出，所有的数值对象与机器节点处于同一环形哈希空间中，从数值对象映射到的位置开始顺时针查找对应的存储节点，并将数值对象保存到找到的第一个机器节点上。这样 object1 存储到了 node A 中，object3 存储到了 node B中，object2 和 object4 都存储到了 node C 中。处于这样的集群环境中，环形地址空间不会变更，通过计算每个数值对象的哈希地址空间值，就可以定位到对应的机器节点，也就是实际存储数值对象的机器。</description></item><item><title>SSH 协议以及端口转发</title><link>https://morven.life/posts/the_knowledge_of_ssh/</link><pubDate>Thu, 19 Mar 2015 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_knowledge_of_ssh/</guid><description>SSH 估计是每台 Linux 机器的标配了。日程工作中程序员写完代码之后，很少在本机上直接部署测试，经常需要通过 SSH 登录到远程 Linux 主机上进行验证。实际上 SSH 具备多种功能，不仅仅是远程登录这么简单，今天我们就详细探讨一下 SSH 协议以及它的高级功能。
SSH 原理 SSH 是一种网络协议，用于计算机之间的加密登录，也就是说这种登陆是安全的。SSH 协议之所以安全，是因为它基于非对称加密，基本的过程可以描述为：
客户端通过 SSH user@remote-host 发起登录远程主机的请求 远程主机收到请求之后，将自己的公钥发给客户端 客户端使用这个公钥加密登录密码之后发给远程主机 远程主机使用自己的私钥解密登陆请求，获得登录密码，如果正确，则建立 SSH 通道，之后所有客户端和远程主机的数据传输都要使用加密进行 看似很完美是吗？其实有个问题，如果有人中途拦截了登录请求，将自己伪造的公钥发送给客户端，客户端其实并不能识别这个公钥的可靠性，因为 SSH 协议并不像 HTTPS 协议那样，公钥是包含在证书中心颁发的证书之中。所以有攻击者（中间人攻击）拦截客户端到远程主机的登陆请求，伪造公钥来获取远程主机的登录密码，SSH 协议的安全机制就荡然无存了。
说实话，SSH 协议本身确实无法阻止这种攻击形式，最终还是依靠终端用户自身来识别并规避风险。
比如，我们在第一次登录某一台远程主机的时候，会得到如下提示：
$ ssh user@remote-host The authenticity of host &amp;#39;10.30.110.230 (10.30.110.230)&amp;#39; can&amp;#39;t be established. ECDSA key fingerprint is SHA256:RIXlybA1rgf4mbnWvLuOMAxGRQQFgfnM2+YbYLT7aQA. Are you sure you want to continue connecting (yes/no)? 这个提示的意思是说无法确定远程主机的真实性，只能得到它的公钥指纹 (fingerprint) 信息，需要你确认是否信任这个返回的公钥。这里所说的“指纹”是非对称加密公钥的 MD5 哈希结果。我们知道为了保证非对称加密私钥的安全性，一般非对称加密公钥设置基本都不小于1024位，很难直接让终端用户去确认完整的非对称加密公钥，于是通过 MD5 哈希函数将之转化为128位的指纹，就很容易识别了。实际上，有很多网络应用程序都是用非对称加密公钥指纹来让终端用户识别公钥的可靠性。</description></item><item><title> Linux 的启动流程</title><link>https://morven.life/posts/the_start_process_of_linux/</link><pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_start_process_of_linux/</guid><description>和 Window 等其他操作系统一样，Linux 的启动也分为两个阶段：引导（boot）和启动（startup）。引导阶段开始于打开电源开关，接下来板载程序 BIOS 开始于上电自检过程，结束于内核初始化完成。启动阶段接管剩余的工作，直到操作系统初始化完成进入可操作状态，并能够执行用户的功能性任务。
我们不花过多篇幅讲解引导阶段硬件板载程序加载运行的过程。事实上，由于在板载程序上很多行为基本上固定的，程序员很难介入，所以接下来主要讲讲主板的引导程序如何加载内核以及控制权交给 Linux 操作系统之后各个服务的启动流程。
GRUB 引导加载程序 所谓引导程序，一个用于计算机寻找操作系统内核并加载其到内存的智能程序，通常位于硬盘的第一个扇区，并由 BIOS 载入内存执行。为什么需要引导程序，而不是直接由 BIOS 加载操作系统？原因是 BOIS 只会自动加载硬盘的第一个扇区512字节的内容，而操作系统的大小远远大于这个值，所以才会先加载引导程序，然后通过引导加载程序加载操作系统到内存中。
目前，各个 Linux 发行版主流的引导程序是 GRUB(GRand Unified BootLoader)。GRUB 主要有以下几个作用：
拥有一个可以让用户选择到底启动哪个系统的的菜单界面 可以调用其他的启动引导程序，来实现多系统引导 加载操作系统的内核程序 GRUB1 现在已经逐步被弃用，在大多数现代发行版上它已经被 GRUB2 所替换。GRUB2 通过 /boot/grub2/grub.cfg 进行配置，最终 GRUB 定位和加载 Linux 内核程序到内存中，并转移控制权到内核程序。
内核程序 内核程序的相关文件位于 /boot 目录下，这些内核文件均带有前缀 vmlinuz 。内核文件都是以一种自解压的压缩格式存储以节省空间，选定的内核程序自解压完成并加载到内存中开始执行。
# ll /boot/ total 152404 drwxr-xr-x 4 root root 4096 Nov 29 15:34 ./ drwxr-xr-x 22 root root 335 Jan 16 12:22 ../ -rw-r--r-- 1 root root 190587 Aug 10 2018 config-3.</description></item><item><title>创建私有 CA 以及颁发数字证书</title><link>https://morven.life/posts/how_to_create_pki_with_openssl/</link><pubDate>Mon, 24 Nov 2014 00:00:00 +0000</pubDate><guid>https://morven.life/posts/how_to_create_pki_with_openssl/</guid><description>在上一篇文章深入理解PKI系统的与数字证书中介绍了 PKI 系统的基本组成以及 CA 认证中心的主要作用，以及 X.509 证书基本标准。今天，我们继续应用已经学习的理论知识构建一套自己的 PKI/CA 数字证书信任体系。
数字证书生成工具 有以下两种常见的工具来生成 RSA 公私密钥对:
Note: 有些情形只需要公私密钥对就够了，不需要数字证书，比如私有的 SSH 服务，但是对于一些要求身份认证的情形，则需要对公钥进行数字签名形成数字证书。
ssh-keygen openssl genrsa 实际上 ssh-keygen 底层也是使用 openssl 提供的库来生成密钥对。
ssh-keygen 举例来说，要使用 ssh-keygen 生成2048位 RSA 密钥对，只需要执行以下命令：
$ ssh-keygen -b 2048 -t rsa -f foo_rsa Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in foo_rsa. Your public key has been saved in foo_rsa.</description></item><item><title>PKI 系统的与 CA 中心</title><link>https://morven.life/posts/the_pki_and_digital_certificate/</link><pubDate>Wed, 12 Nov 2014 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_pki_and_digital_certificate/</guid><description>在上一篇文章数字签名与数字证书中介绍了数字签名、数字证书的一些基础知识，但是并没有提到数字证书如何进行管理，比如数字证书的文件格式、数字证书的申请以及轮换等知识，这篇文章将介绍数字证书的管理。
说到数字证书的管理，不得不提到一个专有名词： PKI(Publick Key Infrastructure) ，翻译过来就是公钥基础设施，是一种遵循既定标准的密钥管理平台，它能够为所有网络应用提供加密和数字签名等密码服务及所必需的密钥和证书管理体系。简单来说，可以理解为利用之前提到过的公私钥非对称加密技术为应用提供加密和数字签名等密码服务以及与之相关的密钥和证书管理体系。
PKI 既不是一个协议，也不是一个软件，它是一个标准，在这个标准之下发展出的为了实现安全基础服务目的的技术统称为 PKI 。
PKI的组成 PKI 作为一个实施标准，包含一系列的组件：
CA(Certificate Authority) 中心：是 PKI 的”核心”，即数字证书的申请及签发机关， CA 必须具备权威性的特征，它负责管理 PKI 结构下的所有用户(包括各种应用程序)的证书的签发，把用户的公钥和用户的其他信息捆绑在一起，在网上验证用户的身份， CA 还要负责用户证书的黑名单登记和黑名单发布；
X.509 目录服务器：X.500 目录服务器用于&amp;quot;发布&amp;quot;用户的证书和黑名单信息，用户可通过标准的 LDAP 协议查询自己或其他人的证书和下载黑名单信息；
基于 SSL(Secure Socket Layer) 的安全 web 服务器：Secure Socket Layer(SSL) 协议最初由 Netscape 企业发展，现已成为网络用来鉴别网站和网页浏览者身份，以及在浏览器使用者及网页服务器之间进行加密通讯的全球化标准；
Web 安全通信平台：Web 有 Web 客户端和 Web 服务端两部分，通过具有高强度密码算法的 SSL 协议保证客户端和服务器端数据的机密性、完整性以及身份验证；
自开发安全应用系统：自开发安全应用系统是指各行业自开发的各种具体应用系统，例如银行、证券的应用系统等。完整的 PKI 包括:
认证政策的制定 认证规则 运作制度的制定 所涉及的各方法律关系内容 技术的实现等 证书中心 认证中心也就是 CA ，是一个负责发放和管理数字证书的权威机构，它作为电子商务交易中受信任的第三方，承担公钥体系中公钥的合法性检验的责任。 CA 为每个使用公开密钥的用户发放一个数字证书，以实现公钥的分发并证明其合法性。作为 PKI 的核心部分， CA 实现了 PKI 中一些很重要的功能:</description></item><item><title>数字签名与数字证书</title><link>https://morven.life/posts/the_digital_signature_and_digital_certificate/</link><pubDate>Mon, 06 Oct 2014 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_digital_signature_and_digital_certificate/</guid><description>在之前的密码学笔记中主要是了解了密码学的基础知识，包括两种加密算法的原理，上篇笔记结束的时候引入了在非对称加密算法中数字证书（Digital Certificate）的概念。这篇笔记将继续探讨什么是数字证书，不过在了解它之前，先得知道什么是数字签名（Digital Signature）。
关于数字签名和数字证书的概念，有一篇非常经典的文章做了详细的介绍，本文的大部分内容来自于那篇文章。
数字证书与数字签名 Bob 生成了自己的公钥、私钥对，将私钥自己保存，公钥分发给了他的朋友们： Pat Susan 与 Daug
Susan 要给 Bob 写一封保密的信件，写完后用 Bob 的公钥加密，就可以达到保密的效果。 Bob 收到信件之后用自己的私钥来解密，就可以看到信件的内容。这里假设 Bob 的私钥没有泄露（私钥是十分敏感的信息，一定要注意保管），即使信件被别人截获，信件内容也无法解密，也就是说这封信的内容不会有第三个人知道。
Bob 给 Susan 回信，决定采用&amp;quot;数字签名&amp;quot;：他写完信之后后先用哈希函数，生成信件的摘要（Digest），然后，再使用自己的私钥，对这个摘要进行加密，生成&amp;quot;数字签名&amp;quot;（Digital Signature）。
Bob 将这个数字签名，附在信件里面，一起发给 Susan：
Susan 收到信件之后，对信件本身使用相同的哈希函数，得到当前信件内容的摘要，同时，取下数字签名，用 Bob 的公钥解密，得到原始信件的摘要，如果两者相同就说明信件的内容没有被修改过。由此证明，这封信确实是 Bob 发出的。
但是，更复杂的情况出现了。 Daug 想欺骗 Susan ，他伪装成 Bob 制作了一对公钥、私钥对，并将公钥分发给 Susan ， Susan 此时实际保存的是 Daug 的公钥，但是还以为这是 Bob 的公钥。因此， Daug 就可以冒充 Bob ，用自己的私钥做成&amp;quot;数字签名&amp;quot;写信给 Susan ，而 Susan 用假的 Bob 公钥进行解密。一切看起来完美无缺？
Susan 觉得有些不对劲，因为她并不确定这个公钥是不是真正属于 Bob 的。于是她想到了一个办法，要求 Bob 去找&amp;quot;证书中心&amp;quot;（Certificate Authority），为公钥做认证。证书中心用证书中心的私钥，对 Bob 的公钥和一些相关信息一起加密，生成&amp;quot;数字证书&amp;quot;（Digital Certificate）。</description></item><item><title>密码学基础</title><link>https://morven.life/posts/the_basic_of_cryptology/</link><pubDate>Tue, 16 Sep 2014 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_basic_of_cryptology/</guid><description>密码学 密码学（Cryptography）是研究编制密码和破译密码的技术科学。研究密码变化的客观规律，应用于编制密码以保守通信秘密的，称为编码学；应用于破译密码以获取通信情报的，称为破译学，总称密码学。
密码是通信双方按约定的法则进行信息特殊变换的一种重要保密手段。依照这些法则，变明文为密文，称为加密变换；变密文为明文，称为脱密变换。密码在早期仅对文字或数码进行加、脱密变换，随着通信技术的发展，对语音、图像、数据等都可实施加、脱密变换。
密码算法 什么是密码算法（Cryptography Algorithm），通常是指加、解密过程所使用的信息变换规则，是用于信息加密和解密的数学函数。对明文进行加密时所采用的规则称作加密算法，而对密文进行解密时所采用的规则称作解密算法。加密算法和解密算法的操作通常都是在一组密钥的控制下进行的。
什么是密钥？密钥（Secret Key）是密码算法中的一个可变参数，通常是一组满足一定条件的随机序列。用于加密算法的叫做加密密钥，用于解密算法的叫做解密密钥，加密密钥和解密密钥可能相同，也可能不相同。
加密算法根据密钥的不同分为两类，对称加密算法(Symmetric-key Encryption Algorithm)和非对称加密算法(Asymmetric-key Encryption Algorithm)。
目前，通用的单钥加密算法为 DES(Data Encryption Standard) ，通用的双钥加密算法为 RSA(Rivest-Shamir-Adleman) ，都产生于上个世纪70年代。
对称加密 首先，让我们先从一个情景开始讲起。
比如张三学习比李四好，李四就想在考试的时候让张三“帮助”一下自己，当然，他们俩不可能像我们平常对话一样说，第一题选 A ，第二题选 B 等等，为什么？因为监考老师明白他俩在谈论什么，也就是说这种沟通交流方式属于“明文”，所以李四就想：“需要发明一种只有我和张三明白的交流方式”，那李四做了什么呢？李四去找张三说：“当我连续咳嗽三声的时候你看我，然后如果我摸了下左耳朵，说明你可以开始给我传答案了，如果没反应，那说明我真的是在咳嗽&amp;hellip;”， 然后，怎么传答案呢？很简单，“你摸左耳朵代表 A ， 摸右耳朵代表 B ，左手放下代表 C ，右手放下代表 D ，”好了，这就是他们的“加密算法”，将信息的一种形式（A, B, C, D），这里我们称为“明文”，转换成了另一种形式（摸左耳朵，摸右耳朵，放左手，放右手），这里称为“密文”，经过这种转换，很显然监考老师不会明白这些“密文”，这样，张三和李四就通过“密文”形式实现了信息的交换。
对称加密算法也叫单钥加密，加密和解密过程都用同一套密钥。历史上，人类传统的加密方法都是前一种，比如二战期间德军用的 Enigma 电报密码，莫尔斯电码都可以看作是一种单钥加密算法。
结合前面的例子对应一下，密钥就是“将（A, B, C, D）转换成（摸左耳朵，摸右耳朵，放左手，放右手）”这么一个规则。
事实上，单钥加密的这组密钥成为在两个或多个成员间的共同秘密，以便维持专属的通讯联系。
这句话很好理解了吧，密钥是张三和李四间共同的秘密！只有他俩事先知道。
所以，为什么叫对称加密呢，你可以这么理解，一方通过密钥将信息加密后，把密文传给另一方，另一方通过这个相同的密钥将密文解密，转换成可以理解的明文。他们之间的关系如下：
目前常见的对称加密算法有：
DES, 3DES, AES, Blowfish, IDEA, RC5, RC6... 非对称加密 非对称加密算法也称为双钥加密，加密和解密过程用的是两套密钥。非对称加密是一种比对称加密更加优秀的加密算法。对称加密的密钥只有一把，所以密钥的保存变得很重要。一旦密钥泄漏，密码也就被破解。而在非对称加密的情况下，密钥有两把，一把是公开的公钥，还有一把是不公开的私钥。
非对称加密的原理如下：
公钥和私钥是一一对应的关系，有一把公钥就必然有一把与之对应的、独一无二的私钥，反之亦成立； 所有的（公钥, 私钥）对都是不同的； 用公钥可以解开私钥加密的信息，反之亦成立； 同时生成公钥和私钥应该相对比较容易，但是从公钥推算出私钥，应该是很困难或者是不可能的； 在对称加密体系中，公钥用来加密信息，私钥用来数字签名。</description></item><item><title>字符编码的前世今生</title><link>https://morven.life/posts/the_character_encoding/</link><pubDate>Sun, 12 May 2013 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_character_encoding/</guid><description>字符编码问题看似无关紧要，常常被忽略，但是如果对字符编码知识没有一个系统完整的认识，在实际编码过程中我们就会遇到各种“坑”。今天，我们就来详细看看字符编码。
一切的起源 字符编码主要是解决如何使用计算机的方式来表达特定的字符，但是有计算机基础理论知识的人都知道，计算机内部所有的数据都是基于二进制，每个二进制位（bit）有0和1两种状态，我们可以组合多个二进位来表达更大的数值，例如八个二进制位就可以组合出256种状态，这被称为一个字节（byte）。这就是说，我们可以用一个字节来映射表示256种不同的状态，如果每一个状态对应一个符号，就是256个符号，从 00000000 到 11111111，这样就建立了最初的计算机数值到自然语言字符最基本的映射关系。上个世纪60年代，美国国家标准协会 ANSI 制定了一个标准，规定了常用字符的集合以及每个字符对应的编号，这就是字符编码最初的形态 ASCII 字符集，也称为 ASCII 码。ASCII 码规定了英语字符与二进制位之间的对应关系。
ASCII码一共规定了128个字符的编码（包括32个不能打印出来的控制符号），比如空格 SPACE 的ASCII 码是32（二进制表示为 00100000），大写的字母 A 的ASCII 码是65（二进制表示为 01000001）。这128个符号只需要占用了一个字节的后面7位，最前面的一位统一规定为0。
按照 ASCII 字符集编码和解码就是简单的查表过程。例如将字符序列编码为二进制流写入存储设备，只需要在 ASCII 字符集中依次找到字符对应的字节，然后直接将该字节写入存储设备即可，解码二进制流就是相反的过程。
各种 OEM 编码的衍生 当计算机发展起来的时候，人们逐渐发现，ASCII 字符集里的128个字符不能满足他们的需求。在英语国家，128个字符编码足矣，但是对于非英语国家，人们无法在 ASCII 字符集中找到他们的基本字符。比如，在法语中，字母上方有注音符号，它就无法用 ASCII 码表示。于是有些人就在想，ASCII 字符只是使用了一个字节的前128个变换，后面的128位完全可以利用起来，于是一些欧洲国家就决定，利用字节中闲置的最高位编入新的符号。比如，法语中的 é 的编码为130（二进制 10000010）。这样一来，这些欧洲国家使用的编码体系，可以表示最多256个符号。
但是，这里又出现了新的问题。不同的国家有不同的字母，因此，哪怕它们都使用256个符号的编码方式，代表的字母却不一样。比如，130在法语编码中代表了 é，在希伯来语编码中却代表了字母 Gimel (ג)，在俄语编码中又会代表另一个符号。不同的 OEM 字符集导致人们无法跨机器传播交流各种信息。例如甲发了一封简历 résumés 给乙，结果乙看到的却是 rגsumגs，因为 é 字符在甲机器上的 OEM 字符集中对应的字节是 0×82，而在乙的机器上，由于使用的 OEM 字符集不同，对 0×82 字节解码后得到的字符却是 ג。
但是尽管出现了不同的 OEM 编码，所有这些编码方式中，0到127表示的符号是一样的，不一样的只是128到255这一段代表的字符。
至于亚洲国家的文字，使用的符号就更多了，汉字就多达10万左右。一个字节只能表示256种符号，肯定是不够的，就必须使用多个字节表达一个符号。比如，简体中文常见的编码方式是 GB2312，使用两个字节表示一个汉字，所以理论上最多可以表示 256x256，也就是65536个汉字。
Note: 中文编码的问题很复杂，这篇笔记不作深入讨论，但需要指出的是虽然都是用多个字节表示一个符号，但是 GB 类的汉字编码与后文的 unicode 和 UTF-8 编码方案是没有关系的。</description></item><item><title>Big Endian &amp; Little Endian</title><link>https://morven.life/posts/the_story_of_big_and_little_endian/</link><pubDate>Tue, 26 Mar 2013 00:00:00 +0000</pubDate><guid>https://morven.life/posts/the_story_of_big_and_little_endian/</guid><description>字节序 谈到字节序，必然要牵扯到两大 CPU 派系。那就是 Motorola 的 PowerPC 系列 CPU 和 Intel 的 x86 系列 CPU 。 PowerPC 系列采用大端 (Big Endian) 方式存储数据，而 x86 系列则采用小端 (Little Endian) 方式存储数据。那么究竟什么是 Big Endian ，什么又是 Little Endian 呢？
其实 Big Endian 是指低地址存放最高有效字节，而 Little Endian 则是低地址存放最低有效字节。
文字说明比较抽象，下面举个例子用图像来说：
Big Endian
低地址 高地址 -----------------------------------------&amp;gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | 12 | 34 | 56 | 78 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Little Endian
低地址 高地址 -----------------------------------------&amp;gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | 78 | 56 | 34 | 12 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 从上面两图可以看出，采用 Big Endian 方式存储数据是符合我们人类的思维习惯的，而 Little Endian&amp;hellip;</description></item></channel></rss>